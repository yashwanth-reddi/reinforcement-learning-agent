{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install gym pyvirtualdisplay\n",
        "!sudo apt-get install -y xvfb python-opengl ffmpeg\n",
        "\n",
        "!pip3 install --upgrade setuptools --user\n",
        "!pip3 install ez_setup \n",
        "\n",
        "!pip3 install gym[atari] \n",
        "\n",
        "!pip install -q autorom[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6O1LIue-9Sl",
        "outputId": "48291ab3-b0a5-481e-9b2a-b2d6c5f711b2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libfontenc1 libpython2-stdlib libxfont2 libxkbfile1 python2\n",
            "  python2-minimal x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "Suggested packages:\n",
            "  python-tk python-numpy libgle3 python2-doc\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libfontenc1 libpython2-stdlib libxfont2 libxkbfile1 python-opengl\n",
            "  python2 python2-minimal x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 14 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 8,318 kB of archives.\n",
            "After this operation, 18.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7,072 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libfontenc1 amd64 1:1.1.4-0ubuntu1 [14.0 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libxfont2 amd64 1:2.0.3-1 [91.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbfile1 amd64 1:1.1.0-1 [65.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-opengl all 3.1.0+dfsg-2build1 [486 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-xkb-utils amd64 7.7+5 [158 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu1 [573 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-utils amd64 1:7.7+6 [91.5 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xserver-common all 2:1.20.13-1ubuntu1~20.04.8 [27.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.8 [780 kB]\n",
            "Fetched 8,318 kB in 6s (1,392 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 14.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python2-minimal.\n",
            "(Reading database ... 122352 files and directories currently installed.)\n",
            "Preparing to unpack .../python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libpython2-stdlib:amd64.\n",
            "Preparing to unpack .../libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python2.\n",
            "(Reading database ... 122381 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python2_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "Preparing to unpack .../01-freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../02-libfontenc1_1%3a1.1.4-0ubuntu1_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../03-libxfont2_1%3a2.0.3-1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../04-libxkbfile1_1%3a1.1.0-1_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../05-python-opengl_3.1.0+dfsg-2build1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-2build1) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../06-x11-xkb-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../07-xfonts-encodings_1%3a1.0.5-0ubuntu1_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../08-xfonts-utils_1%3a7.7+6_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../09-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../10-xserver-common_2%3a1.20.13-1ubuntu1~20.04.8_all.deb ...\n",
            "Unpacking xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../11-xvfb_2%3a1.20.13-1ubuntu1~20.04.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2 (2.7.17-2ubuntu4) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-2build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5) ...\n",
            "Setting up xfonts-utils (1:7.7+6) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.6.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ez_setup\n",
            "  Downloading ez_setup-0.9.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ez_setup\n",
            "  Building wheel for ez_setup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ez_setup: filename=ez_setup-0.9-py3-none-any.whl size=11012 sha256=560b7801140915a2cf258b01b3cd3d79e08a00d09af5c768c34e1af54961b4ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/39/49/e7ce9ce92f074adc6f755a0cc05992407730e14d94ce3f9554\n",
            "Successfully built ez_setup\n",
            "Installing collected packages: ez_setup\n",
            "Successfully installed ez_setup-0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (6.4.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.15.0)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir save_graph\n",
        "!mkdir save_model\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gym\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "\n",
        "# Hyperparameters for DQN agent, memory and training\n",
        "EPISODES = 3500\n",
        "HEIGHT = 84\n",
        "WIDTH = 84\n",
        "HISTORY_SIZE = 4\n",
        "learning_rate = 0.0001\n",
        "evaluation_reward_length = 100\n",
        "Memory_capacity = 1000000\n",
        "train_frame = 100000\n",
        "batch_size = 32\n",
        "scheduler_gamma = 0.4\n",
        "scheduler_step_size = 100000\n",
        "\n",
        "# Hyperparameters for Double DQN agent\n",
        "update_target_network_frequency = 1000\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.fc = nn.Linear(3136, 512)\n",
        "        self.head = nn.Linear(512, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.fc(x.view(x.size(0), -1)))\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "def find_max_lives(env):\n",
        "    env.reset()\n",
        "    _, _, _, info = env.step(0)\n",
        "    return info['lives']\n",
        "\n",
        "def check_live(life, cur_life):\n",
        "    if life > cur_life:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def get_frame(X):\n",
        "    x = np.uint8(resize(rgb2gray(X), (HEIGHT, WIDTH), mode='reflect') * 255)\n",
        "    return x\n",
        "\n",
        "def get_init_state(history, s):\n",
        "    for i in range(HISTORY_SIZE):\n",
        "        history[i, :, :] = get_frame(s)\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self):\n",
        "        self.memory = deque(maxlen=Memory_capacity)\n",
        "    \n",
        "    def push(self, history, action, reward, done):\n",
        "        if torch.is_tensor(action):\n",
        "          action = action.cpu()\n",
        "          \n",
        "        self.memory.append((history, action, reward, done))\n",
        "\n",
        "    def sample_mini_batch(self, frame):\n",
        "        mini_batch = []\n",
        "        if frame >= Memory_capacity:\n",
        "            sample_range = Memory_capacity\n",
        "        else:\n",
        "            sample_range = frame\n",
        "\n",
        "        sample_range -= (HISTORY_SIZE + 1)\n",
        "\n",
        "        idx_sample = random.sample(range(sample_range), batch_size)\n",
        "        for i in idx_sample:\n",
        "            sample = []\n",
        "            for j in range(HISTORY_SIZE + 1):\n",
        "                sample.append(self.memory[i + j])\n",
        "\n",
        "            sample = np.array(sample)\n",
        "            mini_batch.append((np.stack(sample[:, 0], axis=0), sample[3, 1], sample[3, 2], sample[3, 3]))\n",
        "\n",
        "        return mini_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "ZgNM1cYh51U9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "state = env.reset()\n",
        "\n",
        "\n",
        "number_lives = find_max_lives(env)\n",
        "state_size = env.observation_space.shape\n",
        "action_size = 3 #fire, left, and right\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "F-kjHKsR54pe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bc2bb1-5622-4905-e994-fbe63a4a37a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, action_size):\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # These are hyper parameters for the DQN\n",
        "        self.discount_factor = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.explore_step = 500000\n",
        "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
        "        self.train_start = 100000\n",
        "        self.update_target = 1000\n",
        "\n",
        "        # Generate the memory\n",
        "        self.memory = ReplayMemory()\n",
        "\n",
        "        # Create the policy net and the target net\n",
        "        self.policy_net = DQN(action_size)\n",
        "        self.policy_net.to(device)\n",
        "        self.target_net = DQN(action_size)\n",
        "        self.target_net.to(device)\n",
        "        \n",
        "        self.optimizer = optim.Adam(params=self.policy_net.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
        "\n",
        "        # Initialize a target network and initialize the target network to the policy net\n",
        "        self.update_target_net()\n",
        "\n",
        "    def load_policy_net(self, path):\n",
        "        self.policy_net = torch.load(path)           \n",
        "\n",
        "    # after some time interval update the target net to be same with policy net\n",
        "    def update_target_net(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    \"\"\"Get action using policy net using epsilon-greedy policy\"\"\"\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            temp = []\n",
        "            random_number = random.randrange(self.action_size)\n",
        "            temp.append([random_number])\n",
        "            return torch.tensor(temp, device=device, dtype=torch.long)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.tensor([state], device=device)\n",
        "                q_values = self.policy_net(state_tensor)\n",
        "                action = q_values.argmax().item()\n",
        "            return torch.tensor([[action]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "    def train_policy_net(self, frame):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "\n",
        "        mini_batch = self.memory.sample_mini_batch(frame)\n",
        "        mini_batch = np.array(mini_batch).transpose()\n",
        "\n",
        "        history = np.stack(mini_batch[0], axis=0)\n",
        "        states = np.float32(history[:, :4, :, :]) / 255.\n",
        "        states = torch.from_numpy(states).cuda()\n",
        "        actions = list(mini_batch[1])\n",
        "        actions = torch.LongTensor(actions).cuda()\n",
        "        rewards = list(mini_batch[2])\n",
        "        rewards = torch.FloatTensor(rewards).cuda()\n",
        "        next_states = np.float32(history[:, 1:, :, :]) / 255.\n",
        "        next_states = torch.tensor(next_states).cuda()\n",
        "\n",
        "        dones = mini_batch[3] # checks if the game is over\n",
        "        musk = torch.tensor(list(map(int, dones==False)),dtype=torch.bool)\n",
        "        \n",
        "        # Compute Q(s_t, a), the Q-value of the current state\n",
        "        state_action_values = self.policy_net(states)[range(batch_size), actions.view(batch_size).long()]\n",
        "\n",
        "        # Compute Q function of next state\n",
        "        next_state_values = torch.zeros(batch_size, device=device)\n",
        "        temp = [s is not None for s in next_states]\n",
        "        non_final_mask = torch.tensor(temp, device=device, dtype=torch.bool)\n",
        "        temp2 = [s for s in next_states if s is not None]\n",
        "        non_final_ns = torch.stack(temp2).to(device)\n",
        "\n",
        "        # Find maximum Q-value of action at next state from policy net\n",
        "        next_state_values[non_final_mask] = self.target_net(non_final_ns).max(1)[0].detach()\n",
        "\n",
        "\n",
        "        next_mul_discount = next_state_values * self.discount_factor\n",
        "        expected_value =  next_mul_discount + rewards\n",
        "        reshaped_state_action_value = state_action_values.view(32)\n",
        "\n",
        "        # Compute the Huber Loss\n",
        "        loss = F.smooth_l1_loss(reshaped_state_action_value, expected_value)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimize the model, .step() both the optimizer and the scheduler!\n",
        "        for param in self.policy_net.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "qTCSj2f6N3cl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnZ8Uvo7X_bZ"
      },
      "source": [
        "## Creating a DQN Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFmF0Dk7X_bZ"
      },
      "source": [
        "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
        "\n",
        "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
        "\n",
        "__Frame__ : Number of frames processed in total.\n",
        "\n",
        "__Memory Size__ : The current size of the replay memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QUoo_FvrX_bZ"
      },
      "outputs": [],
      "source": [
        "double_dqn = True \n",
        "\n",
        "agent = Agent(action_size)\n",
        "\n",
        "\n",
        "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
        "frame = 0\n",
        "memory_size = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIOw783KX_ba"
      },
      "source": [
        "### Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rco5YolyX_ba"
      },
      "source": [
        "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rV91mDdpX_ba",
        "outputId": "f00ca687-ec4d-45c8-f2b3-73360b1e5e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0   score: 1.0   memory length: 152   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 1   score: 0.0   memory length: 276   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 0.5\n",
            "episode: 2   score: 2.0   memory length: 495   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 3   score: 0.0   memory length: 619   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 0.75\n",
            "episode: 4   score: 2.0   memory length: 817   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.0\n",
            "episode: 5   score: 2.0   memory length: 1019   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
            "episode: 6   score: 2.0   memory length: 1218   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
            "episode: 7   score: 0.0   memory length: 1342   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.125\n",
            "episode: 8   score: 1.0   memory length: 1513   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.1111111111111112\n",
            "episode: 9   score: 1.0   memory length: 1684   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.1\n",
            "episode: 10   score: 1.0   memory length: 1836   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.0909090909090908\n",
            "episode: 11   score: 1.0   memory length: 1988   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.0833333333333333\n",
            "episode: 12   score: 2.0   memory length: 2210   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.1538461538461537\n",
            "episode: 13   score: 3.0   memory length: 2438   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
            "episode: 14   score: 0.0   memory length: 2562   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 15   score: 1.0   memory length: 2713   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.1875\n",
            "episode: 16   score: 1.0   memory length: 2883   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.1764705882352942\n",
            "episode: 17   score: 0.0   memory length: 3007   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.1111111111111112\n",
            "episode: 18   score: 2.0   memory length: 3227   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.1578947368421053\n",
            "episode: 19   score: 2.0   memory length: 3444   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 20   score: 1.0   memory length: 3615   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.1904761904761905\n",
            "episode: 21   score: 1.0   memory length: 3767   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.1818181818181819\n",
            "episode: 22   score: 0.0   memory length: 3891   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.1304347826086956\n",
            "episode: 23   score: 1.0   memory length: 4061   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.125\n",
            "episode: 24   score: 1.0   memory length: 4231   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.12\n",
            "episode: 25   score: 3.0   memory length: 4479   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.1923076923076923\n",
            "episode: 26   score: 4.0   memory length: 4746   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.2962962962962963\n",
            "episode: 27   score: 2.0   memory length: 4945   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.3214285714285714\n",
            "episode: 28   score: 0.0   memory length: 5068   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2758620689655173\n",
            "episode: 29   score: 0.0   memory length: 5192   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2333333333333334\n",
            "episode: 30   score: 1.0   memory length: 5344   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.2258064516129032\n",
            "episode: 31   score: 2.0   memory length: 5564   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 32   score: 2.0   memory length: 5745   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.2727272727272727\n",
            "episode: 33   score: 0.0   memory length: 5869   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2352941176470589\n",
            "episode: 34   score: 0.0   memory length: 5993   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 35   score: 4.0   memory length: 6289   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.2777777777777777\n",
            "episode: 36   score: 2.0   memory length: 6488   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.2972972972972974\n",
            "episode: 37   score: 1.0   memory length: 6658   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.2894736842105263\n",
            "episode: 38   score: 2.0   memory length: 6859   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.3076923076923077\n",
            "episode: 39   score: 2.0   memory length: 7078   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.325\n",
            "episode: 40   score: 1.0   memory length: 7230   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.3170731707317074\n",
            "episode: 41   score: 0.0   memory length: 7354   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
            "episode: 42   score: 2.0   memory length: 7553   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.302325581395349\n",
            "episode: 43   score: 0.0   memory length: 7677   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2727272727272727\n",
            "episode: 44   score: 1.0   memory length: 7849   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.2666666666666666\n",
            "episode: 45   score: 2.0   memory length: 8030   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.2826086956521738\n",
            "episode: 46   score: 0.0   memory length: 8154   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2553191489361701\n",
            "episode: 47   score: 0.0   memory length: 8278   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2291666666666667\n",
            "episode: 48   score: 1.0   memory length: 8430   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.2244897959183674\n",
            "episode: 49   score: 0.0   memory length: 8553   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 50   score: 1.0   memory length: 8704   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.196078431372549\n",
            "episode: 51   score: 3.0   memory length: 8916   epsilon: 1.0    steps: 212    lr: 0.0001     evaluation reward: 1.2307692307692308\n",
            "episode: 52   score: 4.0   memory length: 9209   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.2830188679245282\n",
            "episode: 53   score: 2.0   memory length: 9432   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.2962962962962963\n",
            "episode: 54   score: 1.0   memory length: 9601   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.290909090909091\n",
            "episode: 55   score: 1.0   memory length: 9773   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
            "episode: 56   score: 0.0   memory length: 9896   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.263157894736842\n",
            "episode: 57   score: 5.0   memory length: 10241   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.3275862068965518\n",
            "episode: 58   score: 5.0   memory length: 10583   epsilon: 1.0    steps: 342    lr: 0.0001     evaluation reward: 1.3898305084745763\n",
            "episode: 59   score: 5.0   memory length: 10907   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 60   score: 0.0   memory length: 11030   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4262295081967213\n",
            "episode: 61   score: 0.0   memory length: 11153   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.403225806451613\n",
            "episode: 62   score: 2.0   memory length: 11334   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.4126984126984128\n",
            "episode: 63   score: 4.0   memory length: 11594   epsilon: 1.0    steps: 260    lr: 0.0001     evaluation reward: 1.453125\n",
            "episode: 64   score: 0.0   memory length: 11718   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4307692307692308\n",
            "episode: 65   score: 0.0   memory length: 11842   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4090909090909092\n",
            "episode: 66   score: 0.0   memory length: 11966   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3880597014925373\n",
            "episode: 67   score: 2.0   memory length: 12186   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.3970588235294117\n",
            "episode: 68   score: 0.0   memory length: 12310   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3768115942028984\n",
            "episode: 69   score: 1.0   memory length: 12480   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3714285714285714\n",
            "episode: 70   score: 3.0   memory length: 12727   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3943661971830985\n",
            "episode: 71   score: 0.0   memory length: 12851   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.375\n",
            "episode: 72   score: 0.0   memory length: 12974   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.356164383561644\n",
            "episode: 73   score: 0.0   memory length: 13098   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.337837837837838\n",
            "episode: 74   score: 2.0   memory length: 13297   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.3466666666666667\n",
            "episode: 75   score: 2.0   memory length: 13478   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.355263157894737\n",
            "episode: 76   score: 3.0   memory length: 13725   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3766233766233766\n",
            "episode: 77   score: 5.0   memory length: 14070   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.4230769230769231\n",
            "episode: 78   score: 1.0   memory length: 14239   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4177215189873418\n",
            "episode: 79   score: 3.0   memory length: 14466   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.4375\n",
            "episode: 80   score: 1.0   memory length: 14636   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4320987654320987\n",
            "episode: 81   score: 0.0   memory length: 14760   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4146341463414633\n",
            "episode: 82   score: 3.0   memory length: 15023   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.4337349397590362\n",
            "episode: 83   score: 1.0   memory length: 15192   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
            "episode: 84   score: 2.0   memory length: 15411   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4352941176470588\n",
            "episode: 85   score: 2.0   memory length: 15627   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.441860465116279\n",
            "episode: 86   score: 2.0   memory length: 15826   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4482758620689655\n",
            "episode: 87   score: 1.0   memory length: 15997   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4431818181818181\n",
            "episode: 88   score: 2.0   memory length: 16197   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.449438202247191\n",
            "episode: 89   score: 2.0   memory length: 16397   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.4555555555555555\n",
            "episode: 90   score: 0.0   memory length: 16521   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4395604395604396\n",
            "episode: 91   score: 1.0   memory length: 16690   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.434782608695652\n",
            "episode: 92   score: 0.0   memory length: 16813   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4193548387096775\n",
            "episode: 93   score: 0.0   memory length: 16936   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4042553191489362\n",
            "episode: 94   score: 2.0   memory length: 17135   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4105263157894736\n",
            "episode: 95   score: 1.0   memory length: 17286   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.40625\n",
            "episode: 96   score: 0.0   memory length: 17409   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3917525773195876\n",
            "episode: 97   score: 2.0   memory length: 17627   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3979591836734695\n",
            "episode: 98   score: 3.0   memory length: 17876   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.4141414141414141\n",
            "episode: 99   score: 3.0   memory length: 18120   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 100   score: 5.0   memory length: 18411   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 101   score: 1.0   memory length: 18580   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 102   score: 1.0   memory length: 18749   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 103   score: 0.0   memory length: 18872   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 104   score: 0.0   memory length: 18995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 105   score: 4.0   memory length: 19288   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 106   score: 0.0   memory length: 19412   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 107   score: 2.0   memory length: 19611   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 108   score: 0.0   memory length: 19734   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 109   score: 2.0   memory length: 19932   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 110   score: 1.0   memory length: 20101   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 111   score: 3.0   memory length: 20350   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 112   score: 0.0   memory length: 20474   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 113   score: 2.0   memory length: 20673   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 114   score: 1.0   memory length: 20825   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 115   score: 0.0   memory length: 20948   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 116   score: 0.0   memory length: 21072   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 117   score: 1.0   memory length: 21224   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 118   score: 3.0   memory length: 21491   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 119   score: 3.0   memory length: 21718   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 120   score: 1.0   memory length: 21869   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 121   score: 3.0   memory length: 22118   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 122   score: 1.0   memory length: 22270   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 123   score: 3.0   memory length: 22519   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 124   score: 0.0   memory length: 22643   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 125   score: 5.0   memory length: 22971   epsilon: 1.0    steps: 328    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 126   score: 0.0   memory length: 23095   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 127   score: 3.0   memory length: 23307   epsilon: 1.0    steps: 212    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 128   score: 3.0   memory length: 23542   epsilon: 1.0    steps: 235    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 129   score: 3.0   memory length: 23772   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 130   score: 0.0   memory length: 23896   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 131   score: 0.0   memory length: 24020   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 132   score: 1.0   memory length: 24172   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 133   score: 1.0   memory length: 24324   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 134   score: 2.0   memory length: 24546   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 135   score: 2.0   memory length: 24765   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 136   score: 2.0   memory length: 24946   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 137   score: 0.0   memory length: 25069   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 138   score: 2.0   memory length: 25268   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 139   score: 3.0   memory length: 25516   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 140   score: 2.0   memory length: 25735   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 141   score: 2.0   memory length: 25934   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 142   score: 2.0   memory length: 26133   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 143   score: 1.0   memory length: 26286   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 144   score: 4.0   memory length: 26576   epsilon: 1.0    steps: 290    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 145   score: 1.0   memory length: 26727   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 146   score: 2.0   memory length: 26925   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 147   score: 0.0   memory length: 27049   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 148   score: 2.0   memory length: 27248   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 149   score: 3.0   memory length: 27513   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.66\n",
            "episode: 150   score: 0.0   memory length: 27636   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 151   score: 2.0   memory length: 27835   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 152   score: 0.0   memory length: 27958   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 153   score: 2.0   memory length: 28157   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 154   score: 0.0   memory length: 28281   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 155   score: 3.0   memory length: 28529   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 156   score: 2.0   memory length: 28728   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 157   score: 2.0   memory length: 28927   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 158   score: 2.0   memory length: 29126   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 159   score: 4.0   memory length: 29383   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 160   score: 3.0   memory length: 29633   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 161   score: 1.0   memory length: 29803   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 162   score: 0.0   memory length: 29927   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 163   score: 3.0   memory length: 30197   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 164   score: 0.0   memory length: 30321   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 165   score: 0.0   memory length: 30445   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 166   score: 1.0   memory length: 30599   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 167   score: 1.0   memory length: 30769   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 168   score: 2.0   memory length: 30951   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 169   score: 1.0   memory length: 31122   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 170   score: 1.0   memory length: 31295   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 171   score: 3.0   memory length: 31541   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 172   score: 1.0   memory length: 31712   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 173   score: 2.0   memory length: 31929   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 174   score: 0.0   memory length: 32053   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 175   score: 5.0   memory length: 32357   epsilon: 1.0    steps: 304    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 176   score: 3.0   memory length: 32588   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 177   score: 1.0   memory length: 32739   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 178   score: 2.0   memory length: 32959   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 179   score: 1.0   memory length: 33129   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 180   score: 0.0   memory length: 33253   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 181   score: 0.0   memory length: 33377   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 182   score: 1.0   memory length: 33530   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 183   score: 0.0   memory length: 33654   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 184   score: 1.0   memory length: 33827   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 185   score: 2.0   memory length: 34045   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 186   score: 2.0   memory length: 34244   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 187   score: 1.0   memory length: 34395   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 188   score: 2.0   memory length: 34580   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 189   score: 0.0   memory length: 34703   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 190   score: 1.0   memory length: 34873   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 191   score: 0.0   memory length: 34997   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 192   score: 1.0   memory length: 35167   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 193   score: 0.0   memory length: 35291   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 194   score: 2.0   memory length: 35489   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 195   score: 6.0   memory length: 35865   epsilon: 1.0    steps: 376    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 196   score: 3.0   memory length: 36097   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 197   score: 2.0   memory length: 36296   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 198   score: 4.0   memory length: 36573   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 199   score: 1.0   memory length: 36725   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 200   score: 1.0   memory length: 36877   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 201   score: 1.0   memory length: 37046   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 202   score: 0.0   memory length: 37170   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 203   score: 0.0   memory length: 37294   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 204   score: 3.0   memory length: 37538   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 205   score: 2.0   memory length: 37721   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 206   score: 0.0   memory length: 37844   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 207   score: 1.0   memory length: 38013   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 208   score: 2.0   memory length: 38234   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 209   score: 1.0   memory length: 38404   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 210   score: 2.0   memory length: 38623   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 211   score: 1.0   memory length: 38794   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 212   score: 3.0   memory length: 39062   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 213   score: 2.0   memory length: 39261   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 214   score: 3.0   memory length: 39530   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 215   score: 3.0   memory length: 39756   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 216   score: 1.0   memory length: 39925   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 217   score: 0.0   memory length: 40049   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
            "episode: 218   score: 0.0   memory length: 40173   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 219   score: 0.0   memory length: 40296   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 220   score: 5.0   memory length: 40640   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 221   score: 0.0   memory length: 40763   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 222   score: 1.0   memory length: 40933   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 223   score: 3.0   memory length: 41181   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 224   score: 2.0   memory length: 41397   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.6\n",
            "episode: 225   score: 2.0   memory length: 41596   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 226   score: 4.0   memory length: 41875   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 227   score: 1.0   memory length: 42044   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 228   score: 0.0   memory length: 42168   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 229   score: 1.0   memory length: 42337   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 230   score: 1.0   memory length: 42489   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 231   score: 0.0   memory length: 42612   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 232   score: 1.0   memory length: 42763   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 233   score: 1.0   memory length: 42934   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 234   score: 0.0   memory length: 43058   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 235   score: 0.0   memory length: 43182   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 236   score: 2.0   memory length: 43402   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 237   score: 2.0   memory length: 43603   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 238   score: 0.0   memory length: 43727   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 239   score: 3.0   memory length: 43990   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 240   score: 1.0   memory length: 44141   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 241   score: 3.0   memory length: 44387   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 242   score: 2.0   memory length: 44604   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 243   score: 1.0   memory length: 44774   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 244   score: 2.0   memory length: 44973   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 245   score: 0.0   memory length: 45097   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 246   score: 1.0   memory length: 45267   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 247   score: 4.0   memory length: 45525   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 248   score: 0.0   memory length: 45649   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 249   score: 1.0   memory length: 45800   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 250   score: 3.0   memory length: 46067   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 251   score: 2.0   memory length: 46266   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 252   score: 2.0   memory length: 46465   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 253   score: 2.0   memory length: 46664   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 254   score: 3.0   memory length: 46892   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 255   score: 4.0   memory length: 47168   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 256   score: 0.0   memory length: 47292   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 257   score: 4.0   memory length: 47591   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 258   score: 4.0   memory length: 47908   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 259   score: 2.0   memory length: 48106   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 260   score: 0.0   memory length: 48230   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 261   score: 2.0   memory length: 48447   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 262   score: 4.0   memory length: 48767   epsilon: 1.0    steps: 320    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 263   score: 0.0   memory length: 48891   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 264   score: 1.0   memory length: 49061   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 265   score: 0.0   memory length: 49184   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 266   score: 0.0   memory length: 49308   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 267   score: 2.0   memory length: 49507   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 268   score: 0.0   memory length: 49630   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 269   score: 1.0   memory length: 49800   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 270   score: 1.0   memory length: 49952   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 271   score: 0.0   memory length: 50076   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 272   score: 0.0   memory length: 50199   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 273   score: 2.0   memory length: 50398   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 274   score: 2.0   memory length: 50618   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 275   score: 1.0   memory length: 50790   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 276   score: 0.0   memory length: 50914   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 277   score: 0.0   memory length: 51038   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 278   score: 2.0   memory length: 51219   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 279   score: 2.0   memory length: 51438   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 280   score: 4.0   memory length: 51717   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 281   score: 2.0   memory length: 51916   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 282   score: 0.0   memory length: 52040   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 283   score: 0.0   memory length: 52164   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 284   score: 1.0   memory length: 52333   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 285   score: 1.0   memory length: 52504   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 286   score: 1.0   memory length: 52677   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 287   score: 1.0   memory length: 52847   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 288   score: 1.0   memory length: 53000   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 289   score: 1.0   memory length: 53170   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 290   score: 0.0   memory length: 53293   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 291   score: 0.0   memory length: 53417   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 292   score: 0.0   memory length: 53541   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 293   score: 0.0   memory length: 53665   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 294   score: 2.0   memory length: 53846   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 295   score: 2.0   memory length: 54045   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 296   score: 2.0   memory length: 54264   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 297   score: 1.0   memory length: 54437   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 298   score: 0.0   memory length: 54560   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 299   score: 1.0   memory length: 54732   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 300   score: 3.0   memory length: 54979   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 301   score: 2.0   memory length: 55180   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 302   score: 2.0   memory length: 55378   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 303   score: 1.0   memory length: 55547   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 304   score: 4.0   memory length: 55849   epsilon: 1.0    steps: 302    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 305   score: 1.0   memory length: 56001   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 306   score: 2.0   memory length: 56200   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 307   score: 1.0   memory length: 56373   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 308   score: 0.0   memory length: 56497   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 309   score: 1.0   memory length: 56669   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 310   score: 1.0   memory length: 56821   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 311   score: 1.0   memory length: 56973   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 312   score: 3.0   memory length: 57201   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 313   score: 0.0   memory length: 57324   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 314   score: 0.0   memory length: 57448   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 315   score: 0.0   memory length: 57572   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 316   score: 0.0   memory length: 57695   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 317   score: 4.0   memory length: 57990   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 318   score: 3.0   memory length: 58219   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 319   score: 1.0   memory length: 58391   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 320   score: 1.0   memory length: 58543   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 321   score: 0.0   memory length: 58667   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 322   score: 2.0   memory length: 58866   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 323   score: 1.0   memory length: 59036   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 324   score: 0.0   memory length: 59159   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 325   score: 2.0   memory length: 59379   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 326   score: 1.0   memory length: 59549   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 327   score: 3.0   memory length: 59797   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 328   score: 1.0   memory length: 59967   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 329   score: 0.0   memory length: 60091   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 330   score: 0.0   memory length: 60215   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 331   score: 0.0   memory length: 60338   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 332   score: 2.0   memory length: 60555   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 333   score: 2.0   memory length: 60774   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 334   score: 1.0   memory length: 60925   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 335   score: 1.0   memory length: 61077   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 336   score: 3.0   memory length: 61321   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 337   score: 1.0   memory length: 61472   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 338   score: 3.0   memory length: 61738   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 339   score: 0.0   memory length: 61862   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 340   score: 0.0   memory length: 61985   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 341   score: 3.0   memory length: 62234   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 342   score: 2.0   memory length: 62433   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 343   score: 2.0   memory length: 62632   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 344   score: 0.0   memory length: 62756   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 345   score: 0.0   memory length: 62880   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 346   score: 0.0   memory length: 63004   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 347   score: 4.0   memory length: 63325   epsilon: 1.0    steps: 321    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 348   score: 3.0   memory length: 63573   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 349   score: 1.0   memory length: 63745   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 350   score: 1.0   memory length: 63915   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 351   score: 0.0   memory length: 64038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 352   score: 0.0   memory length: 64162   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 353   score: 1.0   memory length: 64333   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 354   score: 4.0   memory length: 64613   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 355   score: 0.0   memory length: 64737   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 356   score: 2.0   memory length: 64956   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 357   score: 1.0   memory length: 65126   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 358   score: 2.0   memory length: 65327   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 359   score: 0.0   memory length: 65451   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 360   score: 3.0   memory length: 65699   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 361   score: 0.0   memory length: 65823   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 362   score: 3.0   memory length: 66091   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 363   score: 0.0   memory length: 66215   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 364   score: 1.0   memory length: 66385   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 365   score: 1.0   memory length: 66557   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 366   score: 2.0   memory length: 66775   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 367   score: 3.0   memory length: 67020   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.24\n",
            "episode: 368   score: 2.0   memory length: 67223   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 369   score: 1.0   memory length: 67395   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 370   score: 3.0   memory length: 67663   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 371   score: 0.0   memory length: 67786   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 372   score: 3.0   memory length: 68017   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 373   score: 4.0   memory length: 68296   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 374   score: 0.0   memory length: 68419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 375   score: 4.0   memory length: 68720   epsilon: 1.0    steps: 301    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 376   score: 2.0   memory length: 68901   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 377   score: 1.0   memory length: 69071   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 378   score: 2.0   memory length: 69291   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 379   score: 0.0   memory length: 69415   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 380   score: 2.0   memory length: 69598   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 381   score: 3.0   memory length: 69845   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 382   score: 3.0   memory length: 70075   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 383   score: 2.0   memory length: 70274   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 384   score: 0.0   memory length: 70398   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 385   score: 1.0   memory length: 70567   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 386   score: 1.0   memory length: 70721   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 387   score: 2.0   memory length: 70902   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 388   score: 2.0   memory length: 71102   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 389   score: 1.0   memory length: 71274   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 390   score: 4.0   memory length: 71572   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 391   score: 0.0   memory length: 71696   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 392   score: 4.0   memory length: 71993   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 393   score: 2.0   memory length: 72211   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 394   score: 0.0   memory length: 72335   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 395   score: 0.0   memory length: 72459   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 396   score: 1.0   memory length: 72610   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 397   score: 3.0   memory length: 72857   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 398   score: 1.0   memory length: 73027   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 399   score: 0.0   memory length: 73150   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 400   score: 1.0   memory length: 73301   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 401   score: 4.0   memory length: 73615   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 402   score: 3.0   memory length: 73846   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 403   score: 3.0   memory length: 74091   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 404   score: 1.0   memory length: 74243   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 405   score: 1.0   memory length: 74414   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 406   score: 3.0   memory length: 74643   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 407   score: 0.0   memory length: 74767   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 408   score: 3.0   memory length: 74997   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 409   score: 2.0   memory length: 75196   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 410   score: 0.0   memory length: 75320   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 411   score: 1.0   memory length: 75472   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 412   score: 1.0   memory length: 75642   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 413   score: 0.0   memory length: 75765   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 414   score: 2.0   memory length: 75964   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 415   score: 0.0   memory length: 76088   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 416   score: 0.0   memory length: 76212   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 417   score: 0.0   memory length: 76336   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 418   score: 1.0   memory length: 76488   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 419   score: 1.0   memory length: 76640   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 420   score: 1.0   memory length: 76791   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 421   score: 1.0   memory length: 76960   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 422   score: 0.0   memory length: 77083   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 423   score: 1.0   memory length: 77253   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 424   score: 1.0   memory length: 77422   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 425   score: 3.0   memory length: 77649   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 426   score: 0.0   memory length: 77773   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 427   score: 2.0   memory length: 77992   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 428   score: 4.0   memory length: 78268   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 429   score: 2.0   memory length: 78467   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 430   score: 1.0   memory length: 78637   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 431   score: 2.0   memory length: 78840   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 432   score: 0.0   memory length: 78964   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 433   score: 1.0   memory length: 79133   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 434   score: 4.0   memory length: 79428   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 435   score: 2.0   memory length: 79627   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 436   score: 1.0   memory length: 79797   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 437   score: 3.0   memory length: 80023   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 438   score: 0.0   memory length: 80147   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 439   score: 0.0   memory length: 80271   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 440   score: 1.0   memory length: 80441   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 441   score: 0.0   memory length: 80565   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 442   score: 1.0   memory length: 80736   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 443   score: 0.0   memory length: 80860   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 444   score: 2.0   memory length: 81059   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 445   score: 2.0   memory length: 81257   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 446   score: 1.0   memory length: 81428   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 447   score: 0.0   memory length: 81552   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 448   score: 3.0   memory length: 81779   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 449   score: 2.0   memory length: 81978   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 450   score: 1.0   memory length: 82131   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 451   score: 2.0   memory length: 82329   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 452   score: 3.0   memory length: 82556   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 453   score: 3.0   memory length: 82821   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 454   score: 1.0   memory length: 82972   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 455   score: 3.0   memory length: 83199   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 456   score: 2.0   memory length: 83381   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 457   score: 2.0   memory length: 83600   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 458   score: 0.0   memory length: 83723   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 459   score: 2.0   memory length: 83922   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 460   score: 2.0   memory length: 84143   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 461   score: 2.0   memory length: 84341   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
            "episode: 462   score: 0.0   memory length: 84465   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 463   score: 2.0   memory length: 84663   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 464   score: 0.0   memory length: 84786   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
            "episode: 465   score: 0.0   memory length: 84910   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
            "episode: 466   score: 0.0   memory length: 85034   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 467   score: 1.0   memory length: 85186   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 468   score: 4.0   memory length: 85503   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 469   score: 1.0   memory length: 85674   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 470   score: 2.0   memory length: 85878   epsilon: 1.0    steps: 204    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 471   score: 1.0   memory length: 86030   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 472   score: 0.0   memory length: 86154   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 473   score: 0.0   memory length: 86278   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 474   score: 2.0   memory length: 86476   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 475   score: 1.0   memory length: 86647   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 476   score: 0.0   memory length: 86771   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 477   score: 3.0   memory length: 87001   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 478   score: 2.0   memory length: 87200   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 479   score: 1.0   memory length: 87352   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 480   score: 0.0   memory length: 87476   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 481   score: 1.0   memory length: 87645   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 482   score: 1.0   memory length: 87815   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 483   score: 2.0   memory length: 88034   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 484   score: 1.0   memory length: 88204   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 485   score: 0.0   memory length: 88328   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 486   score: 1.0   memory length: 88501   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 487   score: 0.0   memory length: 88625   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 488   score: 3.0   memory length: 88852   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 489   score: 2.0   memory length: 89050   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 490   score: 0.0   memory length: 89173   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 491   score: 2.0   memory length: 89372   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 492   score: 2.0   memory length: 89594   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 493   score: 3.0   memory length: 89823   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 494   score: 2.0   memory length: 90022   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 495   score: 1.0   memory length: 90195   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 496   score: 1.0   memory length: 90367   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 497   score: 2.0   memory length: 90567   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 498   score: 1.0   memory length: 90737   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 499   score: 2.0   memory length: 90955   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 500   score: 0.0   memory length: 91079   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 501   score: 1.0   memory length: 91252   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 502   score: 1.0   memory length: 91403   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 503   score: 2.0   memory length: 91602   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 504   score: 3.0   memory length: 91851   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 505   score: 0.0   memory length: 91974   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 506   score: 3.0   memory length: 92221   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 507   score: 0.0   memory length: 92344   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 508   score: 0.0   memory length: 92468   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 509   score: 5.0   memory length: 92810   epsilon: 1.0    steps: 342    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 510   score: 0.0   memory length: 92934   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 511   score: 5.0   memory length: 93265   epsilon: 1.0    steps: 331    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 512   score: 1.0   memory length: 93435   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 513   score: 4.0   memory length: 93731   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 514   score: 1.0   memory length: 93882   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 515   score: 1.0   memory length: 94035   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 516   score: 3.0   memory length: 94265   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 517   score: 2.0   memory length: 94446   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 518   score: 3.0   memory length: 94712   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 519   score: 0.0   memory length: 94836   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 520   score: 4.0   memory length: 95113   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 521   score: 0.0   memory length: 95236   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 522   score: 0.0   memory length: 95360   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 523   score: 3.0   memory length: 95627   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 524   score: 1.0   memory length: 95797   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 525   score: 0.0   memory length: 95921   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 526   score: 0.0   memory length: 96045   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 527   score: 2.0   memory length: 96244   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 528   score: 1.0   memory length: 96395   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 529   score: 1.0   memory length: 96567   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 530   score: 3.0   memory length: 96797   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 531   score: 2.0   memory length: 96995   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 532   score: 2.0   memory length: 97197   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 533   score: 1.0   memory length: 97367   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 534   score: 0.0   memory length: 97491   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 535   score: 1.0   memory length: 97662   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 536   score: 3.0   memory length: 97910   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 537   score: 0.0   memory length: 98034   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 538   score: 3.0   memory length: 98305   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 539   score: 3.0   memory length: 98551   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 540   score: 1.0   memory length: 98720   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 541   score: 0.0   memory length: 98844   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 542   score: 3.0   memory length: 99072   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 543   score: 0.0   memory length: 99196   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 544   score: 0.0   memory length: 99319   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 545   score: 0.0   memory length: 99443   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 546   score: 2.0   memory length: 99642   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 547   score: 0.0   memory length: 99766   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 548   score: 2.0   memory length: 99966   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-fe06b66ed4ea>:108: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  sample = np.array(sample)\n",
            "<ipython-input-2-fe06b66ed4ea>:108: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sample = np.array(sample)\n",
            "<ipython-input-4-70fb2ca04b55>:56: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  mini_batch = np.array(mini_batch).transpose()\n",
            "<ipython-input-4-70fb2ca04b55>:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  mini_batch = np.array(mini_batch).transpose()\n",
            "<ipython-input-4-70fb2ca04b55>:81: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 549   score: 1.0   memory length: 100136   epsilon: 0.9997287400000059    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 550   score: 4.0   memory length: 100454   epsilon: 0.9990991000000196    steps: 318    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 551   score: 1.0   memory length: 100625   epsilon: 0.9987605200000269    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 552   score: 1.0   memory length: 100777   epsilon: 0.9984595600000334    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 553   score: 2.0   memory length: 100976   epsilon: 0.998065540000042    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 554   score: 0.0   memory length: 101100   epsilon: 0.9978200200000473    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 555   score: 2.0   memory length: 101299   epsilon: 0.9974260000000559    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 556   score: 6.0   memory length: 101678   epsilon: 0.9966755800000722    steps: 379    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 557   score: 0.0   memory length: 101801   epsilon: 0.9964320400000775    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 558   score: 2.0   memory length: 102020   epsilon: 0.9959984200000869    steps: 219    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 559   score: 3.0   memory length: 102249   epsilon: 0.9955450000000967    steps: 229    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 560   score: 0.0   memory length: 102372   epsilon: 0.995301460000102    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 561   score: 5.0   memory length: 102701   epsilon: 0.9946500400001161    steps: 329    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 562   score: 0.0   memory length: 102824   epsilon: 0.9944065000001214    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 563   score: 1.0   memory length: 102994   epsilon: 0.9940699000001287    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 564   score: 0.0   memory length: 103117   epsilon: 0.993826360000134    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 565   score: 1.0   memory length: 103268   epsilon: 0.9935273800001405    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 566   score: 0.0   memory length: 103391   epsilon: 0.9932838400001458    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 567   score: 1.0   memory length: 103543   epsilon: 0.9929828800001523    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 568   score: 1.0   memory length: 103714   epsilon: 0.9926443000001597    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 569   score: 0.0   memory length: 103838   epsilon: 0.992398780000165    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 570   score: 2.0   memory length: 104039   epsilon: 0.9920008000001737    steps: 201    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 571   score: 0.0   memory length: 104163   epsilon: 0.991755280000179    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 572   score: 2.0   memory length: 104362   epsilon: 0.9913612600001875    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 573   score: 0.0   memory length: 104486   epsilon: 0.9911157400001929    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 574   score: 0.0   memory length: 104610   epsilon: 0.9908702200001982    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 575   score: 0.0   memory length: 104733   epsilon: 0.9906266800002035    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 576   score: 1.0   memory length: 104904   epsilon: 0.9902881000002108    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 577   score: 0.0   memory length: 105027   epsilon: 0.9900445600002161    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 578   score: 0.0   memory length: 105150   epsilon: 0.9898010200002214    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 579   score: 2.0   memory length: 105349   epsilon: 0.98940700000023    steps: 199    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 580   score: 1.0   memory length: 105520   epsilon: 0.9890684200002373    steps: 171    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 581   score: 3.0   memory length: 105770   epsilon: 0.9885734200002481    steps: 250    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 582   score: 2.0   memory length: 105987   epsilon: 0.9881437600002574    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 583   score: 0.0   memory length: 106111   epsilon: 0.9878982400002627    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 584   score: 1.0   memory length: 106281   epsilon: 0.98756164000027    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 585   score: 1.0   memory length: 106433   epsilon: 0.9872606800002766    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 586   score: 0.0   memory length: 106556   epsilon: 0.9870171400002818    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 587   score: 1.0   memory length: 106727   epsilon: 0.9866785600002892    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 588   score: 3.0   memory length: 106992   epsilon: 0.9861538600003006    steps: 265    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 589   score: 3.0   memory length: 107237   epsilon: 0.9856687600003111    steps: 245    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 590   score: 1.0   memory length: 107407   epsilon: 0.9853321600003184    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 591   score: 3.0   memory length: 107652   epsilon: 0.984847060000329    steps: 245    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 592   score: 3.0   memory length: 107920   epsilon: 0.9843164200003405    steps: 268    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 593   score: 0.0   memory length: 108043   epsilon: 0.9840728800003458    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 594   score: 2.0   memory length: 108261   epsilon: 0.9836412400003551    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 595   score: 3.0   memory length: 108509   epsilon: 0.9831502000003658    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 596   score: 0.0   memory length: 108632   epsilon: 0.9829066600003711    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 597   score: 0.0   memory length: 108756   epsilon: 0.9826611400003764    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 598   score: 0.0   memory length: 108880   epsilon: 0.9824156200003817    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 599   score: 2.0   memory length: 109099   epsilon: 0.9819820000003912    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 600   score: 1.0   memory length: 109251   epsilon: 0.9816810400003977    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 601   score: 2.0   memory length: 109449   epsilon: 0.9812890000004062    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 602   score: 2.0   memory length: 109629   epsilon: 0.9809326000004139    steps: 180    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 603   score: 4.0   memory length: 109940   epsilon: 0.9803168200004273    steps: 311    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 604   score: 3.0   memory length: 110208   epsilon: 0.9797861800004388    steps: 268    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 605   score: 2.0   memory length: 110407   epsilon: 0.9793921600004474    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 606   score: 0.0   memory length: 110530   epsilon: 0.9791486200004527    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 607   score: 3.0   memory length: 110781   epsilon: 0.9786516400004635    steps: 251    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 608   score: 0.0   memory length: 110904   epsilon: 0.9784081000004687    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 609   score: 3.0   memory length: 111131   epsilon: 0.9779586400004785    steps: 227    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 610   score: 2.0   memory length: 111351   epsilon: 0.977523040000488    steps: 220    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 611   score: 1.0   memory length: 111504   epsilon: 0.9772201000004945    steps: 153    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 612   score: 0.0   memory length: 111628   epsilon: 0.9769745800004999    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 613   score: 0.0   memory length: 111752   epsilon: 0.9767290600005052    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 614   score: 2.0   memory length: 111950   epsilon: 0.9763370200005137    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 615   score: 1.0   memory length: 112120   epsilon: 0.976000420000521    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 616   score: 3.0   memory length: 112371   epsilon: 0.9755034400005318    steps: 251    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 617   score: 0.0   memory length: 112494   epsilon: 0.9752599000005371    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 618   score: 2.0   memory length: 112693   epsilon: 0.9748658800005456    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 619   score: 4.0   memory length: 112962   epsilon: 0.9743332600005572    steps: 269    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 620   score: 2.0   memory length: 113143   epsilon: 0.973974880000565    steps: 181    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 621   score: 0.0   memory length: 113267   epsilon: 0.9737293600005703    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 622   score: 1.0   memory length: 113418   epsilon: 0.9734303800005768    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 623   score: 0.0   memory length: 113541   epsilon: 0.9731868400005821    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 624   score: 2.0   memory length: 113740   epsilon: 0.9727928200005906    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 625   score: 3.0   memory length: 113987   epsilon: 0.9723037600006013    steps: 247    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 626   score: 3.0   memory length: 114220   epsilon: 0.9718424200006113    steps: 233    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 627   score: 2.0   memory length: 114419   epsilon: 0.9714484000006198    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 628   score: 1.0   memory length: 114589   epsilon: 0.9711118000006271    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 629   score: 3.0   memory length: 114836   epsilon: 0.9706227400006378    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 630   score: 2.0   memory length: 115035   epsilon: 0.9702287200006463    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 631   score: 5.0   memory length: 115325   epsilon: 0.9696545200006588    steps: 290    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 632   score: 3.0   memory length: 115596   epsilon: 0.9691179400006704    steps: 271    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 633   score: 0.0   memory length: 115720   epsilon: 0.9688724200006757    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 634   score: 1.0   memory length: 115892   epsilon: 0.9685318600006831    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 635   score: 2.0   memory length: 116090   epsilon: 0.9681398200006917    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 636   score: 2.0   memory length: 116308   epsilon: 0.967708180000701    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 637   score: 2.0   memory length: 116508   epsilon: 0.9673121800007096    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
            "episode: 638   score: 0.0   memory length: 116631   epsilon: 0.9670686400007149    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 639   score: 1.0   memory length: 116782   epsilon: 0.9667696600007214    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 640   score: 2.0   memory length: 117001   epsilon: 0.9663360400007308    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 641   score: 2.0   memory length: 117200   epsilon: 0.9659420200007394    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 642   score: 0.0   memory length: 117323   epsilon: 0.9656984800007447    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 643   score: 0.0   memory length: 117447   epsilon: 0.96545296000075    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 644   score: 2.0   memory length: 117646   epsilon: 0.9650589400007585    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 645   score: 2.0   memory length: 117867   epsilon: 0.964621360000768    steps: 221    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 646   score: 0.0   memory length: 117990   epsilon: 0.9643778200007733    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 647   score: 0.0   memory length: 118114   epsilon: 0.9641323000007787    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 648   score: 1.0   memory length: 118265   epsilon: 0.9638333200007851    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 649   score: 0.0   memory length: 118389   epsilon: 0.9635878000007905    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 650   score: 3.0   memory length: 118635   epsilon: 0.963100720000801    steps: 246    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 651   score: 0.0   memory length: 118759   epsilon: 0.9628552000008064    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 652   score: 2.0   memory length: 118940   epsilon: 0.9624968200008142    steps: 181    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 653   score: 2.0   memory length: 119139   epsilon: 0.9621028000008227    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 654   score: 0.0   memory length: 119263   epsilon: 0.961857280000828    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 655   score: 0.0   memory length: 119386   epsilon: 0.9616137400008333    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 656   score: 1.0   memory length: 119559   epsilon: 0.9612712000008408    steps: 173    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 657   score: 0.0   memory length: 119683   epsilon: 0.9610256800008461    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 658   score: 0.0   memory length: 119806   epsilon: 0.9607821400008514    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 659   score: 2.0   memory length: 120005   epsilon: 0.9603881200008599    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 660   score: 1.0   memory length: 120175   epsilon: 0.9600515200008672    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 661   score: 3.0   memory length: 120402   epsilon: 0.959602060000877    steps: 227    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 662   score: 2.0   memory length: 120619   epsilon: 0.9591724000008863    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 663   score: 1.0   memory length: 120790   epsilon: 0.9588338200008937    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 664   score: 2.0   memory length: 120971   epsilon: 0.9584754400009015    steps: 181    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 665   score: 1.0   memory length: 121141   epsilon: 0.9581388400009088    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 666   score: 3.0   memory length: 121369   epsilon: 0.9576874000009186    steps: 228    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 667   score: 1.0   memory length: 121520   epsilon: 0.9573884200009251    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 668   score: 3.0   memory length: 121769   epsilon: 0.9568954000009358    steps: 249    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 669   score: 1.0   memory length: 121921   epsilon: 0.9565944400009423    steps: 152    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 670   score: 0.0   memory length: 122045   epsilon: 0.9563489200009476    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 671   score: 3.0   memory length: 122272   epsilon: 0.9558994600009574    steps: 227    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 672   score: 3.0   memory length: 122521   epsilon: 0.9554064400009681    steps: 249    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 673   score: 1.0   memory length: 122673   epsilon: 0.9551054800009746    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 674   score: 1.0   memory length: 122846   epsilon: 0.954762940000982    steps: 173    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 675   score: 1.0   memory length: 122998   epsilon: 0.9544619800009886    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 676   score: 1.0   memory length: 123171   epsilon: 0.954119440000996    steps: 173    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 677   score: 0.0   memory length: 123295   epsilon: 0.9538739200010014    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 678   score: 1.0   memory length: 123466   epsilon: 0.9535353400010087    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
            "episode: 679   score: 1.0   memory length: 123636   epsilon: 0.953198740001016    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 680   score: 1.0   memory length: 123807   epsilon: 0.9528601600010234    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
            "episode: 681   score: 0.0   memory length: 123931   epsilon: 0.9526146400010287    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 682   score: 1.0   memory length: 124083   epsilon: 0.9523136800010352    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 683   score: 3.0   memory length: 124351   epsilon: 0.9517830400010467    steps: 268    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 684   score: 0.0   memory length: 124474   epsilon: 0.951539500001052    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
            "episode: 685   score: 0.0   memory length: 124598   epsilon: 0.9512939800010574    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 686   score: 2.0   memory length: 124796   epsilon: 0.9509019400010659    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 687   score: 1.0   memory length: 124948   epsilon: 0.9506009800010724    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 688   score: 1.0   memory length: 125100   epsilon: 0.9503000200010789    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 689   score: 5.0   memory length: 125429   epsilon: 0.9496486000010931    steps: 329    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 690   score: 1.0   memory length: 125581   epsilon: 0.9493476400010996    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 691   score: 1.0   memory length: 125753   epsilon: 0.949007080001107    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
            "episode: 692   score: 1.0   memory length: 125922   epsilon: 0.9486724600011143    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 693   score: 0.0   memory length: 126045   epsilon: 0.9484289200011196    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 694   score: 2.0   memory length: 126231   epsilon: 0.9480606400011276    steps: 186    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 695   score: 0.0   memory length: 126354   epsilon: 0.9478171000011328    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 696   score: 1.0   memory length: 126524   epsilon: 0.9474805000011401    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 697   score: 1.0   memory length: 126696   epsilon: 0.9471399400011475    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 698   score: 0.0   memory length: 126820   epsilon: 0.9468944200011529    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 699   score: 1.0   memory length: 126990   epsilon: 0.9465578200011602    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 700   score: 3.0   memory length: 127237   epsilon: 0.9460687600011708    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 701   score: 1.0   memory length: 127389   epsilon: 0.9457678000011773    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 702   score: 3.0   memory length: 127639   epsilon: 0.9452728000011881    steps: 250    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 703   score: 0.0   memory length: 127763   epsilon: 0.9450272800011934    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
            "episode: 704   score: 0.0   memory length: 127887   epsilon: 0.9447817600011987    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 705   score: 1.0   memory length: 128058   epsilon: 0.9444431800012061    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 706   score: 0.0   memory length: 128182   epsilon: 0.9441976600012114    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 707   score: 1.0   memory length: 128352   epsilon: 0.9438610600012187    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 708   score: 0.0   memory length: 128476   epsilon: 0.943615540001224    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 709   score: 1.0   memory length: 128646   epsilon: 0.9432789400012314    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 710   score: 1.0   memory length: 128798   epsilon: 0.9429779800012379    steps: 152    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 711   score: 2.0   memory length: 129021   epsilon: 0.9425364400012475    steps: 223    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 712   score: 0.0   memory length: 129144   epsilon: 0.9422929000012528    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 713   score: 3.0   memory length: 129389   epsilon: 0.9418078000012633    steps: 245    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 714   score: 0.0   memory length: 129512   epsilon: 0.9415642600012686    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 715   score: 0.0   memory length: 129636   epsilon: 0.9413187400012739    steps: 124    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 716   score: 3.0   memory length: 129884   epsilon: 0.9408277000012846    steps: 248    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 717   score: 1.0   memory length: 130055   epsilon: 0.9404891200012919    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
            "episode: 718   score: 1.0   memory length: 130207   epsilon: 0.9401881600012985    steps: 152    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 719   score: 1.0   memory length: 130377   epsilon: 0.9398515600013058    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 720   score: 1.0   memory length: 130529   epsilon: 0.9395506000013123    steps: 152    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 721   score: 0.0   memory length: 130653   epsilon: 0.9393050800013176    steps: 124    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 722   score: 2.0   memory length: 130851   epsilon: 0.9389130400013261    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 723   score: 3.0   memory length: 131099   epsilon: 0.9384220000013368    steps: 248    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 724   score: 1.0   memory length: 131269   epsilon: 0.9380854000013441    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 725   score: 2.0   memory length: 131490   epsilon: 0.9376478200013536    steps: 221    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 726   score: 2.0   memory length: 131688   epsilon: 0.9372557800013621    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 727   score: 3.0   memory length: 131934   epsilon: 0.9367687000013727    steps: 246    lr: 0.0001     evaluation reward: 1.29\n",
            "episode: 728   score: 3.0   memory length: 132161   epsilon: 0.9363192400013824    steps: 227    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 729   score: 0.0   memory length: 132285   epsilon: 0.9360737200013878    steps: 124    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 730   score: 1.0   memory length: 132436   epsilon: 0.9357747400013943    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
            "episode: 731   score: 0.0   memory length: 132560   epsilon: 0.9355292200013996    steps: 124    lr: 0.0001     evaluation reward: 1.22\n",
            "episode: 732   score: 0.0   memory length: 132683   epsilon: 0.9352856800014049    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 733   score: 0.0   memory length: 132807   epsilon: 0.9350401600014102    steps: 124    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 734   score: 1.0   memory length: 132977   epsilon: 0.9347035600014175    steps: 170    lr: 0.0001     evaluation reward: 1.19\n",
            "episode: 735   score: 3.0   memory length: 133224   epsilon: 0.9342145000014281    steps: 247    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 736   score: 2.0   memory length: 133422   epsilon: 0.9338224600014366    steps: 198    lr: 0.0001     evaluation reward: 1.2\n",
            "episode: 737   score: 0.0   memory length: 133546   epsilon: 0.933576940001442    steps: 124    lr: 0.0001     evaluation reward: 1.18\n",
            "episode: 738   score: 3.0   memory length: 133817   epsilon: 0.9330403600014536    steps: 271    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 739   score: 1.0   memory length: 133987   epsilon: 0.9327037600014609    steps: 170    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 740   score: 2.0   memory length: 134186   epsilon: 0.9323097400014695    steps: 199    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 741   score: 2.0   memory length: 134404   epsilon: 0.9318781000014789    steps: 218    lr: 0.0001     evaluation reward: 1.21\n",
            "episode: 742   score: 2.0   memory length: 134603   epsilon: 0.9314840800014874    steps: 199    lr: 0.0001     evaluation reward: 1.23\n",
            "episode: 743   score: 2.0   memory length: 134802   epsilon: 0.931090060001496    steps: 199    lr: 0.0001     evaluation reward: 1.25\n",
            "episode: 744   score: 3.0   memory length: 135051   epsilon: 0.9305970400015067    steps: 249    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 745   score: 2.0   memory length: 135250   epsilon: 0.9302030200015152    steps: 199    lr: 0.0001     evaluation reward: 1.26\n",
            "episode: 746   score: 2.0   memory length: 135450   epsilon: 0.9298070200015238    steps: 200    lr: 0.0001     evaluation reward: 1.28\n",
            "episode: 747   score: 2.0   memory length: 135648   epsilon: 0.9294149800015323    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 748   score: 1.0   memory length: 135821   epsilon: 0.9290724400015398    steps: 173    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 749   score: 0.0   memory length: 135945   epsilon: 0.9288269200015451    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
            "episode: 750   score: 4.0   memory length: 136221   epsilon: 0.928280440001557    steps: 276    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 751   score: 3.0   memory length: 136454   epsilon: 0.927819100001567    steps: 233    lr: 0.0001     evaluation reward: 1.34\n",
            "episode: 752   score: 1.0   memory length: 136606   epsilon: 0.9275181400015735    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 753   score: 0.0   memory length: 136730   epsilon: 0.9272726200015788    steps: 124    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 754   score: 0.0   memory length: 136853   epsilon: 0.9270290800015841    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
            "episode: 755   score: 2.0   memory length: 137052   epsilon: 0.9266350600015927    steps: 199    lr: 0.0001     evaluation reward: 1.33\n",
            "episode: 756   score: 3.0   memory length: 137279   epsilon: 0.9261856000016024    steps: 227    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 757   score: 2.0   memory length: 137477   epsilon: 0.925793560001611    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 758   score: 0.0   memory length: 137601   epsilon: 0.9255480400016163    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 759   score: 2.0   memory length: 137800   epsilon: 0.9251540200016248    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
            "episode: 760   score: 2.0   memory length: 138022   epsilon: 0.9247144600016344    steps: 222    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 761   score: 0.0   memory length: 138146   epsilon: 0.9244689400016397    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
            "episode: 762   score: 3.0   memory length: 138398   epsilon: 0.9239699800016505    steps: 252    lr: 0.0001     evaluation reward: 1.36\n",
            "episode: 763   score: 3.0   memory length: 138644   epsilon: 0.9234829000016611    steps: 246    lr: 0.0001     evaluation reward: 1.38\n",
            "episode: 764   score: 5.0   memory length: 138991   epsilon: 0.922795840001676    steps: 347    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 765   score: 1.0   memory length: 139142   epsilon: 0.9224968600016825    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 766   score: 2.0   memory length: 139340   epsilon: 0.922104820001691    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 767   score: 2.0   memory length: 139556   epsilon: 0.9216771400017003    steps: 216    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 768   score: 3.0   memory length: 139786   epsilon: 0.9212217400017102    steps: 230    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 769   score: 2.0   memory length: 139984   epsilon: 0.9208297000017187    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 770   score: 2.0   memory length: 140186   epsilon: 0.9204297400017274    steps: 202    lr: 0.0001     evaluation reward: 1.44\n",
            "episode: 771   score: 0.0   memory length: 140310   epsilon: 0.9201842200017327    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
            "episode: 772   score: 2.0   memory length: 140529   epsilon: 0.9197506000017421    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
            "episode: 773   score: 3.0   memory length: 140755   epsilon: 0.9193031200017519    steps: 226    lr: 0.0001     evaluation reward: 1.42\n",
            "episode: 774   score: 2.0   memory length: 140955   epsilon: 0.9189071200017604    steps: 200    lr: 0.0001     evaluation reward: 1.43\n",
            "episode: 775   score: 5.0   memory length: 141263   epsilon: 0.9182972800017737    steps: 308    lr: 0.0001     evaluation reward: 1.47\n",
            "episode: 776   score: 5.0   memory length: 141629   epsilon: 0.9175726000017894    steps: 366    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 777   score: 0.0   memory length: 141753   epsilon: 0.9173270800017947    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 778   score: 1.0   memory length: 141926   epsilon: 0.9169845400018022    steps: 173    lr: 0.0001     evaluation reward: 1.51\n",
            "episode: 779   score: 4.0   memory length: 142198   epsilon: 0.9164459800018139    steps: 272    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 780   score: 1.0   memory length: 142350   epsilon: 0.9161450200018204    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
            "episode: 781   score: 2.0   memory length: 142570   epsilon: 0.9157094200018299    steps: 220    lr: 0.0001     evaluation reward: 1.56\n",
            "episode: 782   score: 2.0   memory length: 142789   epsilon: 0.9152758000018393    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 783   score: 3.0   memory length: 143020   epsilon: 0.9148184200018492    steps: 231    lr: 0.0001     evaluation reward: 1.57\n",
            "episode: 784   score: 1.0   memory length: 143191   epsilon: 0.9144798400018566    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 785   score: 1.0   memory length: 143343   epsilon: 0.9141788800018631    steps: 152    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 786   score: 1.0   memory length: 143513   epsilon: 0.9138422800018704    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
            "episode: 787   score: 2.0   memory length: 143732   epsilon: 0.9134086600018798    steps: 219    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 788   score: 4.0   memory length: 144029   epsilon: 0.9128206000018926    steps: 297    lr: 0.0001     evaluation reward: 1.62\n",
            "episode: 789   score: 2.0   memory length: 144228   epsilon: 0.9124265800019011    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
            "episode: 790   score: 3.0   memory length: 144473   epsilon: 0.9119414800019117    steps: 245    lr: 0.0001     evaluation reward: 1.61\n",
            "episode: 791   score: 4.0   memory length: 144750   epsilon: 0.9113930200019236    steps: 277    lr: 0.0001     evaluation reward: 1.64\n",
            "episode: 792   score: 2.0   memory length: 144949   epsilon: 0.9109990000019321    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 793   score: 0.0   memory length: 145073   epsilon: 0.9107534800019375    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 794   score: 2.0   memory length: 145274   epsilon: 0.9103555000019461    steps: 201    lr: 0.0001     evaluation reward: 1.65\n",
            "episode: 795   score: 2.0   memory length: 145457   epsilon: 0.909993160001954    steps: 183    lr: 0.0001     evaluation reward: 1.67\n",
            "episode: 796   score: 4.0   memory length: 145735   epsilon: 0.9094427200019659    steps: 278    lr: 0.0001     evaluation reward: 1.7\n",
            "episode: 797   score: 3.0   memory length: 145964   epsilon: 0.9089893000019758    steps: 229    lr: 0.0001     evaluation reward: 1.72\n",
            "episode: 798   score: 4.0   memory length: 146260   epsilon: 0.9084032200019885    steps: 296    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 799   score: 2.0   memory length: 146477   epsilon: 0.9079735600019978    steps: 217    lr: 0.0001     evaluation reward: 1.77\n",
            "episode: 800   score: 1.0   memory length: 146647   epsilon: 0.9076369600020051    steps: 170    lr: 0.0001     evaluation reward: 1.75\n",
            "episode: 801   score: 2.0   memory length: 146848   epsilon: 0.9072389800020138    steps: 201    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 802   score: 1.0   memory length: 147000   epsilon: 0.9069380200020203    steps: 152    lr: 0.0001     evaluation reward: 1.74\n",
            "episode: 803   score: 2.0   memory length: 147184   epsilon: 0.9065737000020282    steps: 184    lr: 0.0001     evaluation reward: 1.76\n",
            "episode: 804   score: 2.0   memory length: 147383   epsilon: 0.9061796800020367    steps: 199    lr: 0.0001     evaluation reward: 1.78\n",
            "episode: 805   score: 2.0   memory length: 147582   epsilon: 0.9057856600020453    steps: 199    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 806   score: 0.0   memory length: 147706   epsilon: 0.9055401400020506    steps: 124    lr: 0.0001     evaluation reward: 1.79\n",
            "episode: 807   score: 2.0   memory length: 147924   epsilon: 0.90510850000206    steps: 218    lr: 0.0001     evaluation reward: 1.8\n",
            "episode: 808   score: 4.0   memory length: 148222   epsilon: 0.9045184600020728    steps: 298    lr: 0.0001     evaluation reward: 1.84\n",
            "episode: 809   score: 2.0   memory length: 148441   epsilon: 0.9040848400020822    steps: 219    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 810   score: 1.0   memory length: 148612   epsilon: 0.9037462600020896    steps: 171    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 811   score: 2.0   memory length: 148811   epsilon: 0.9033522400020981    steps: 199    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 812   score: 2.0   memory length: 149010   epsilon: 0.9029582200021067    steps: 199    lr: 0.0001     evaluation reward: 1.87\n",
            "episode: 813   score: 1.0   memory length: 149161   epsilon: 0.9026592400021132    steps: 151    lr: 0.0001     evaluation reward: 1.85\n",
            "episode: 814   score: 3.0   memory length: 149408   epsilon: 0.9021701800021238    steps: 247    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 815   score: 2.0   memory length: 149608   epsilon: 0.9017741800021324    steps: 200    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 816   score: 2.0   memory length: 149807   epsilon: 0.9013801600021409    steps: 199    lr: 0.0001     evaluation reward: 1.89\n",
            "episode: 817   score: 0.0   memory length: 149931   epsilon: 0.9011346400021463    steps: 124    lr: 0.0001     evaluation reward: 1.88\n",
            "episode: 818   score: 3.0   memory length: 150160   epsilon: 0.9006812200021561    steps: 229    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 819   score: 1.0   memory length: 150312   epsilon: 0.9003802600021626    steps: 152    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 820   score: 1.0   memory length: 150463   epsilon: 0.9000812800021691    steps: 151    lr: 0.0001     evaluation reward: 1.9\n",
            "episode: 821   score: 3.0   memory length: 150711   epsilon: 0.8995902400021798    steps: 248    lr: 0.0001     evaluation reward: 1.93\n",
            "episode: 822   score: 1.0   memory length: 150863   epsilon: 0.8992892800021863    steps: 152    lr: 0.0001     evaluation reward: 1.92\n",
            "episode: 823   score: 8.0   memory length: 151316   epsilon: 0.8983923400022058    steps: 453    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 824   score: 1.0   memory length: 151467   epsilon: 0.8980933600022123    steps: 151    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 825   score: 2.0   memory length: 151686   epsilon: 0.8976597400022217    steps: 219    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 826   score: 2.0   memory length: 151905   epsilon: 0.8972261200022311    steps: 219    lr: 0.0001     evaluation reward: 1.97\n",
            "episode: 827   score: 2.0   memory length: 152103   epsilon: 0.8968340800022396    steps: 198    lr: 0.0001     evaluation reward: 1.96\n",
            "episode: 828   score: 1.0   memory length: 152273   epsilon: 0.8964974800022469    steps: 170    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 829   score: 0.0   memory length: 152396   epsilon: 0.8962539400022522    steps: 123    lr: 0.0001     evaluation reward: 1.94\n",
            "episode: 830   score: 5.0   memory length: 152705   epsilon: 0.8956421200022655    steps: 309    lr: 0.0001     evaluation reward: 1.98\n",
            "episode: 831   score: 2.0   memory length: 152903   epsilon: 0.895250080002274    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
            "episode: 832   score: 3.0   memory length: 153153   epsilon: 0.8947550800022848    steps: 250    lr: 0.0001     evaluation reward: 2.03\n",
            "episode: 833   score: 2.0   memory length: 153352   epsilon: 0.8943610600022933    steps: 199    lr: 0.0001     evaluation reward: 2.05\n",
            "episode: 834   score: 3.0   memory length: 153622   epsilon: 0.8938264600023049    steps: 270    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 835   score: 2.0   memory length: 153821   epsilon: 0.8934324400023135    steps: 199    lr: 0.0001     evaluation reward: 2.06\n",
            "episode: 836   score: 3.0   memory length: 154047   epsilon: 0.8929849600023232    steps: 226    lr: 0.0001     evaluation reward: 2.07\n",
            "episode: 837   score: 2.0   memory length: 154246   epsilon: 0.8925909400023317    steps: 199    lr: 0.0001     evaluation reward: 2.09\n",
            "episode: 838   score: 5.0   memory length: 154567   epsilon: 0.8919553600023455    steps: 321    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 839   score: 1.0   memory length: 154739   epsilon: 0.8916148000023529    steps: 172    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 840   score: 3.0   memory length: 154985   epsilon: 0.8911277200023635    steps: 246    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 841   score: 3.0   memory length: 155212   epsilon: 0.8906782600023733    steps: 227    lr: 0.0001     evaluation reward: 2.13\n",
            "episode: 842   score: 0.0   memory length: 155336   epsilon: 0.8904327400023786    steps: 124    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 843   score: 4.0   memory length: 155635   epsilon: 0.8898407200023914    steps: 299    lr: 0.0001     evaluation reward: 2.13\n",
            "episode: 844   score: 0.0   memory length: 155758   epsilon: 0.8895971800023967    steps: 123    lr: 0.0001     evaluation reward: 2.1\n",
            "episode: 845   score: 3.0   memory length: 155985   epsilon: 0.8891477200024065    steps: 227    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 846   score: 2.0   memory length: 156204   epsilon: 0.8887141000024159    steps: 219    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 847   score: 2.0   memory length: 156402   epsilon: 0.8883220600024244    steps: 198    lr: 0.0001     evaluation reward: 2.11\n",
            "episode: 848   score: 2.0   memory length: 156622   epsilon: 0.8878864600024339    steps: 220    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 849   score: 2.0   memory length: 156841   epsilon: 0.8874528400024433    steps: 219    lr: 0.0001     evaluation reward: 2.14\n",
            "episode: 850   score: 2.0   memory length: 157042   epsilon: 0.8870548600024519    steps: 201    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 851   score: 3.0   memory length: 157274   epsilon: 0.8865955000024619    steps: 232    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 852   score: 1.0   memory length: 157443   epsilon: 0.8862608800024692    steps: 169    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 853   score: 0.0   memory length: 157566   epsilon: 0.8860173400024745    steps: 123    lr: 0.0001     evaluation reward: 2.12\n",
            "episode: 854   score: 8.0   memory length: 158021   epsilon: 0.885116440002494    steps: 455    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 855   score: 4.0   memory length: 158297   epsilon: 0.8845699600025059    steps: 276    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 856   score: 4.0   memory length: 158573   epsilon: 0.8840234800025177    steps: 276    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 857   score: 2.0   memory length: 158793   epsilon: 0.8835878800025272    steps: 220    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 858   score: 4.0   memory length: 159073   epsilon: 0.8830334800025392    steps: 280    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 859   score: 3.0   memory length: 159300   epsilon: 0.882584020002549    steps: 227    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 860   score: 0.0   memory length: 159423   epsilon: 0.8823404800025543    steps: 123    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 861   score: 3.0   memory length: 159672   epsilon: 0.881847460002565    steps: 249    lr: 0.0001     evaluation reward: 2.29\n",
            "episode: 862   score: 1.0   memory length: 159844   epsilon: 0.8815069000025724    steps: 172    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 863   score: 3.0   memory length: 160107   epsilon: 0.8809861600025837    steps: 263    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 864   score: 2.0   memory length: 160305   epsilon: 0.8805941200025922    steps: 198    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 865   score: 1.0   memory length: 160474   epsilon: 0.8802595000025994    steps: 169    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 866   score: 2.0   memory length: 160673   epsilon: 0.879865480002608    steps: 199    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 867   score: 2.0   memory length: 160872   epsilon: 0.8794714600026166    steps: 199    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 868   score: 2.0   memory length: 161056   epsilon: 0.8791071400026245    steps: 184    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 869   score: 2.0   memory length: 161274   epsilon: 0.8786755000026338    steps: 218    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 870   score: 3.0   memory length: 161503   epsilon: 0.8782220800026437    steps: 229    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 871   score: 5.0   memory length: 161818   epsilon: 0.8775983800026572    steps: 315    lr: 0.0001     evaluation reward: 2.29\n",
            "episode: 872   score: 1.0   memory length: 161988   epsilon: 0.8772617800026645    steps: 170    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 873   score: 1.0   memory length: 162140   epsilon: 0.8769608200026711    steps: 152    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 874   score: 1.0   memory length: 162309   epsilon: 0.8766262000026783    steps: 169    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 875   score: 3.0   memory length: 162556   epsilon: 0.8761371400026889    steps: 247    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 876   score: 3.0   memory length: 162802   epsilon: 0.8756500600026995    steps: 246    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 877   score: 0.0   memory length: 162926   epsilon: 0.8754045400027048    steps: 124    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 878   score: 1.0   memory length: 163077   epsilon: 0.8751055600027113    steps: 151    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 879   score: 0.0   memory length: 163201   epsilon: 0.8748600400027167    steps: 124    lr: 0.0001     evaluation reward: 2.17\n",
            "episode: 880   score: 3.0   memory length: 163427   epsilon: 0.8744125600027264    steps: 226    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 881   score: 2.0   memory length: 163626   epsilon: 0.8740185400027349    steps: 199    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 882   score: 2.0   memory length: 163825   epsilon: 0.8736245200027435    steps: 199    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 883   score: 0.0   memory length: 163949   epsilon: 0.8733790000027488    steps: 124    lr: 0.0001     evaluation reward: 2.16\n",
            "episode: 884   score: 2.0   memory length: 164168   epsilon: 0.8729453800027582    steps: 219    lr: 0.0001     evaluation reward: 2.17\n",
            "episode: 885   score: 4.0   memory length: 164446   epsilon: 0.8723949400027702    steps: 278    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 886   score: 1.0   memory length: 164616   epsilon: 0.8720583400027775    steps: 170    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 887   score: 4.0   memory length: 164913   epsilon: 0.8714702800027903    steps: 297    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 888   score: 1.0   memory length: 165065   epsilon: 0.8711693200027968    steps: 152    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 889   score: 2.0   memory length: 165264   epsilon: 0.8707753000028053    steps: 199    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 890   score: 2.0   memory length: 165445   epsilon: 0.8704169200028131    steps: 181    lr: 0.0001     evaluation reward: 2.18\n",
            "episode: 891   score: 5.0   memory length: 165751   epsilon: 0.8698110400028263    steps: 306    lr: 0.0001     evaluation reward: 2.19\n",
            "episode: 892   score: 4.0   memory length: 166043   epsilon: 0.8692328800028388    steps: 292    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 893   score: 1.0   memory length: 166194   epsilon: 0.8689339000028453    steps: 151    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 894   score: 4.0   memory length: 166492   epsilon: 0.8683438600028581    steps: 298    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 895   score: 3.0   memory length: 166719   epsilon: 0.8678944000028679    steps: 227    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 896   score: 0.0   memory length: 166842   epsilon: 0.8676508600028732    steps: 123    lr: 0.0001     evaluation reward: 2.21\n",
            "episode: 897   score: 2.0   memory length: 167040   epsilon: 0.8672588200028817    steps: 198    lr: 0.0001     evaluation reward: 2.2\n",
            "episode: 898   score: 6.0   memory length: 167418   epsilon: 0.8665103800028979    steps: 378    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 899   score: 2.0   memory length: 167598   epsilon: 0.8661539800029057    steps: 180    lr: 0.0001     evaluation reward: 2.22\n",
            "episode: 900   score: 3.0   memory length: 167845   epsilon: 0.8656649200029163    steps: 247    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 901   score: 1.0   memory length: 167997   epsilon: 0.8653639600029228    steps: 152    lr: 0.0001     evaluation reward: 2.23\n",
            "episode: 902   score: 2.0   memory length: 168217   epsilon: 0.8649283600029323    steps: 220    lr: 0.0001     evaluation reward: 2.24\n",
            "episode: 903   score: 3.0   memory length: 168487   epsilon: 0.8643937600029439    steps: 270    lr: 0.0001     evaluation reward: 2.25\n",
            "episode: 904   score: 3.0   memory length: 168732   epsilon: 0.8639086600029544    steps: 245    lr: 0.0001     evaluation reward: 2.26\n",
            "episode: 905   score: 3.0   memory length: 168959   epsilon: 0.8634592000029642    steps: 227    lr: 0.0001     evaluation reward: 2.27\n",
            "episode: 906   score: 1.0   memory length: 169130   epsilon: 0.8631206200029715    steps: 171    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 907   score: 4.0   memory length: 169406   epsilon: 0.8625741400029834    steps: 276    lr: 0.0001     evaluation reward: 2.3\n",
            "episode: 908   score: 2.0   memory length: 169624   epsilon: 0.8621425000029928    steps: 218    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 909   score: 2.0   memory length: 169823   epsilon: 0.8617484800030013    steps: 199    lr: 0.0001     evaluation reward: 2.28\n",
            "episode: 910   score: 7.0   memory length: 170229   epsilon: 0.8609446000030188    steps: 406    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 911   score: 0.0   memory length: 170352   epsilon: 0.860701060003024    steps: 123    lr: 0.0001     evaluation reward: 2.32\n",
            "episode: 912   score: 3.0   memory length: 170584   epsilon: 0.860241700003034    steps: 232    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 913   score: 4.0   memory length: 170880   epsilon: 0.8596556200030467    steps: 296    lr: 0.0001     evaluation reward: 2.36\n",
            "episode: 914   score: 3.0   memory length: 171128   epsilon: 0.8591645800030574    steps: 248    lr: 0.0001     evaluation reward: 2.36\n",
            "episode: 915   score: 2.0   memory length: 171347   epsilon: 0.8587309600030668    steps: 219    lr: 0.0001     evaluation reward: 2.36\n",
            "episode: 916   score: 2.0   memory length: 171547   epsilon: 0.8583349600030754    steps: 200    lr: 0.0001     evaluation reward: 2.36\n",
            "episode: 917   score: 2.0   memory length: 171746   epsilon: 0.857940940003084    steps: 199    lr: 0.0001     evaluation reward: 2.38\n",
            "episode: 918   score: 2.0   memory length: 171945   epsilon: 0.8575469200030925    steps: 199    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 919   score: 4.0   memory length: 172243   epsilon: 0.8569568800031053    steps: 298    lr: 0.0001     evaluation reward: 2.4\n",
            "episode: 920   score: 3.0   memory length: 172470   epsilon: 0.8565074200031151    steps: 227    lr: 0.0001     evaluation reward: 2.42\n",
            "episode: 921   score: 1.0   memory length: 172623   epsilon: 0.8562044800031217    steps: 153    lr: 0.0001     evaluation reward: 2.4\n",
            "episode: 922   score: 0.0   memory length: 172746   epsilon: 0.855960940003127    steps: 123    lr: 0.0001     evaluation reward: 2.39\n",
            "episode: 923   score: 3.0   memory length: 172994   epsilon: 0.8554699000031376    steps: 248    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 924   score: 1.0   memory length: 173163   epsilon: 0.8551352800031449    steps: 169    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 925   score: 2.0   memory length: 173344   epsilon: 0.8547769000031527    steps: 181    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 926   score: 2.0   memory length: 173564   epsilon: 0.8543413000031621    steps: 220    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 927   score: 1.0   memory length: 173733   epsilon: 0.8540066800031694    steps: 169    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 928   score: 5.0   memory length: 174056   epsilon: 0.8533671400031833    steps: 323    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 929   score: 4.0   memory length: 174314   epsilon: 0.8528563000031943    steps: 258    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 930   score: 0.0   memory length: 174438   epsilon: 0.8526107800031997    steps: 124    lr: 0.0001     evaluation reward: 2.36\n",
            "episode: 931   score: 5.0   memory length: 174751   epsilon: 0.8519910400032131    steps: 313    lr: 0.0001     evaluation reward: 2.39\n",
            "episode: 932   score: 4.0   memory length: 175011   epsilon: 0.8514762400032243    steps: 260    lr: 0.0001     evaluation reward: 2.4\n",
            "episode: 933   score: 4.0   memory length: 175325   epsilon: 0.8508545200032378    steps: 314    lr: 0.0001     evaluation reward: 2.42\n",
            "episode: 934   score: 0.0   memory length: 175449   epsilon: 0.8506090000032431    steps: 124    lr: 0.0001     evaluation reward: 2.39\n",
            "episode: 935   score: 1.0   memory length: 175619   epsilon: 0.8502724000032504    steps: 170    lr: 0.0001     evaluation reward: 2.38\n",
            "episode: 936   score: 4.0   memory length: 175932   epsilon: 0.8496526600032639    steps: 313    lr: 0.0001     evaluation reward: 2.39\n",
            "episode: 937   score: 3.0   memory length: 176161   epsilon: 0.8491992400032737    steps: 229    lr: 0.0001     evaluation reward: 2.4\n",
            "episode: 938   score: 0.0   memory length: 176285   epsilon: 0.8489537200032791    steps: 124    lr: 0.0001     evaluation reward: 2.35\n",
            "episode: 939   score: 1.0   memory length: 176437   epsilon: 0.8486527600032856    steps: 152    lr: 0.0001     evaluation reward: 2.35\n",
            "episode: 940   score: 2.0   memory length: 176636   epsilon: 0.8482587400032942    steps: 199    lr: 0.0001     evaluation reward: 2.34\n",
            "episode: 941   score: 2.0   memory length: 176835   epsilon: 0.8478647200033027    steps: 199    lr: 0.0001     evaluation reward: 2.33\n",
            "episode: 942   score: 6.0   memory length: 177208   epsilon: 0.8471261800033187    steps: 373    lr: 0.0001     evaluation reward: 2.39\n",
            "episode: 943   score: 2.0   memory length: 177407   epsilon: 0.8467321600033273    steps: 199    lr: 0.0001     evaluation reward: 2.37\n",
            "episode: 944   score: 4.0   memory length: 177702   epsilon: 0.84614806000334    steps: 295    lr: 0.0001     evaluation reward: 2.41\n",
            "episode: 945   score: 4.0   memory length: 177998   epsilon: 0.8455619800033527    steps: 296    lr: 0.0001     evaluation reward: 2.42\n",
            "episode: 946   score: 3.0   memory length: 178246   epsilon: 0.8450709400033634    steps: 248    lr: 0.0001     evaluation reward: 2.43\n",
            "episode: 947   score: 3.0   memory length: 178497   epsilon: 0.8445739600033741    steps: 251    lr: 0.0001     evaluation reward: 2.44\n",
            "episode: 948   score: 4.0   memory length: 178815   epsilon: 0.8439443200033878    steps: 318    lr: 0.0001     evaluation reward: 2.46\n",
            "episode: 949   score: 4.0   memory length: 179093   epsilon: 0.8433938800033998    steps: 278    lr: 0.0001     evaluation reward: 2.48\n",
            "episode: 950   score: 2.0   memory length: 179294   epsilon: 0.8429959000034084    steps: 201    lr: 0.0001     evaluation reward: 2.48\n",
            "episode: 951   score: 1.0   memory length: 179446   epsilon: 0.8426949400034149    steps: 152    lr: 0.0001     evaluation reward: 2.46\n",
            "episode: 952   score: 7.0   memory length: 179822   epsilon: 0.8419504600034311    steps: 376    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 953   score: 6.0   memory length: 180160   epsilon: 0.8412812200034456    steps: 338    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 954   score: 5.0   memory length: 180466   epsilon: 0.8406753400034588    steps: 306    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 955   score: 2.0   memory length: 180648   epsilon: 0.8403149800034666    steps: 182    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 956   score: 2.0   memory length: 180847   epsilon: 0.8399209600034752    steps: 199    lr: 0.0001     evaluation reward: 2.51\n",
            "episode: 957   score: 5.0   memory length: 181195   epsilon: 0.8392319200034901    steps: 348    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 958   score: 2.0   memory length: 181394   epsilon: 0.8388379000034987    steps: 199    lr: 0.0001     evaluation reward: 2.52\n",
            "episode: 959   score: 1.0   memory length: 181546   epsilon: 0.8385369400035052    steps: 152    lr: 0.0001     evaluation reward: 2.5\n",
            "episode: 960   score: 4.0   memory length: 181842   epsilon: 0.8379508600035179    steps: 296    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 961   score: 3.0   memory length: 182056   epsilon: 0.8375271400035271    steps: 214    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 962   score: 0.0   memory length: 182180   epsilon: 0.8372816200035325    steps: 124    lr: 0.0001     evaluation reward: 2.53\n",
            "episode: 963   score: 4.0   memory length: 182479   epsilon: 0.8366896000035453    steps: 299    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 964   score: 2.0   memory length: 182678   epsilon: 0.8362955800035539    steps: 199    lr: 0.0001     evaluation reward: 2.54\n",
            "episode: 965   score: 3.0   memory length: 182922   epsilon: 0.8358124600035644    steps: 244    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 966   score: 2.0   memory length: 183121   epsilon: 0.8354184400035729    steps: 199    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 967   score: 1.0   memory length: 183291   epsilon: 0.8350818400035802    steps: 170    lr: 0.0001     evaluation reward: 2.55\n",
            "episode: 968   score: 3.0   memory length: 183556   epsilon: 0.8345571400035916    steps: 265    lr: 0.0001     evaluation reward: 2.56\n",
            "episode: 969   score: 3.0   memory length: 183766   epsilon: 0.8341413400036006    steps: 210    lr: 0.0001     evaluation reward: 2.57\n",
            "episode: 970   score: 7.0   memory length: 184170   epsilon: 0.833341420003618    steps: 404    lr: 0.0001     evaluation reward: 2.61\n",
            "episode: 971   score: 2.0   memory length: 184388   epsilon: 0.8329097800036274    steps: 218    lr: 0.0001     evaluation reward: 2.58\n",
            "episode: 972   score: 2.0   memory length: 184588   epsilon: 0.832513780003636    steps: 200    lr: 0.0001     evaluation reward: 2.59\n",
            "episode: 973   score: 2.0   memory length: 184770   epsilon: 0.8321534200036438    steps: 182    lr: 0.0001     evaluation reward: 2.6\n",
            "episode: 974   score: 5.0   memory length: 185044   epsilon: 0.8316109000036556    steps: 274    lr: 0.0001     evaluation reward: 2.64\n",
            "episode: 975   score: 5.0   memory length: 185370   epsilon: 0.8309654200036696    steps: 326    lr: 0.0001     evaluation reward: 2.66\n",
            "episode: 976   score: 2.0   memory length: 185551   epsilon: 0.8306070400036774    steps: 181    lr: 0.0001     evaluation reward: 2.65\n",
            "episode: 977   score: 4.0   memory length: 185848   epsilon: 0.8300189800036901    steps: 297    lr: 0.0001     evaluation reward: 2.69\n",
            "episode: 978   score: 3.0   memory length: 186082   epsilon: 0.8295556600037002    steps: 234    lr: 0.0001     evaluation reward: 2.71\n",
            "episode: 979   score: 4.0   memory length: 186358   epsilon: 0.829009180003712    steps: 276    lr: 0.0001     evaluation reward: 2.75\n",
            "episode: 980   score: 4.0   memory length: 186635   epsilon: 0.828460720003724    steps: 277    lr: 0.0001     evaluation reward: 2.76\n",
            "episode: 981   score: 0.0   memory length: 186759   epsilon: 0.8282152000037293    steps: 124    lr: 0.0001     evaluation reward: 2.74\n",
            "episode: 982   score: 5.0   memory length: 187082   epsilon: 0.8275756600037432    steps: 323    lr: 0.0001     evaluation reward: 2.77\n",
            "episode: 983   score: 2.0   memory length: 187298   epsilon: 0.8271479800037524    steps: 216    lr: 0.0001     evaluation reward: 2.79\n",
            "episode: 984   score: 7.0   memory length: 187601   epsilon: 0.8265480400037655    steps: 303    lr: 0.0001     evaluation reward: 2.84\n",
            "episode: 985   score: 5.0   memory length: 187926   epsilon: 0.8259045400037794    steps: 325    lr: 0.0001     evaluation reward: 2.85\n",
            "episode: 986   score: 5.0   memory length: 188251   epsilon: 0.8252610400037934    steps: 325    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 987   score: 4.0   memory length: 188526   epsilon: 0.8247165400038052    steps: 275    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 988   score: 1.0   memory length: 188696   epsilon: 0.8243799400038125    steps: 170    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 989   score: 2.0   memory length: 188897   epsilon: 0.8239819600038212    steps: 201    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 990   score: 2.0   memory length: 189098   epsilon: 0.8235839800038298    steps: 201    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 991   score: 0.0   memory length: 189221   epsilon: 0.8233404400038351    steps: 123    lr: 0.0001     evaluation reward: 2.84\n",
            "episode: 992   score: 2.0   memory length: 189440   epsilon: 0.8229068200038445    steps: 219    lr: 0.0001     evaluation reward: 2.82\n",
            "episode: 993   score: 2.0   memory length: 189622   epsilon: 0.8225464600038523    steps: 182    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 994   score: 3.0   memory length: 189849   epsilon: 0.8220970000038621    steps: 227    lr: 0.0001     evaluation reward: 2.82\n",
            "episode: 995   score: 4.0   memory length: 190125   epsilon: 0.821550520003874    steps: 276    lr: 0.0001     evaluation reward: 2.83\n",
            "episode: 996   score: 7.0   memory length: 190482   epsilon: 0.8208436600038893    steps: 357    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 997   score: 4.0   memory length: 190780   epsilon: 0.8202536200039021    steps: 298    lr: 0.0001     evaluation reward: 2.92\n",
            "episode: 998   score: 2.0   memory length: 190979   epsilon: 0.8198596000039107    steps: 199    lr: 0.0001     evaluation reward: 2.88\n",
            "episode: 999   score: 2.0   memory length: 191177   epsilon: 0.8194675600039192    steps: 198    lr: 0.0001     evaluation reward: 2.88\n",
            "episode: 1000   score: 5.0   memory length: 191503   epsilon: 0.8188220800039332    steps: 326    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 1001   score: 3.0   memory length: 191751   epsilon: 0.8183310400039439    steps: 248    lr: 0.0001     evaluation reward: 2.92\n",
            "episode: 1002   score: 4.0   memory length: 192071   epsilon: 0.8176974400039576    steps: 320    lr: 0.0001     evaluation reward: 2.94\n",
            "episode: 1003   score: 2.0   memory length: 192270   epsilon: 0.8173034200039662    steps: 199    lr: 0.0001     evaluation reward: 2.93\n",
            "episode: 1004   score: 4.0   memory length: 192549   epsilon: 0.8167510000039782    steps: 279    lr: 0.0001     evaluation reward: 2.94\n",
            "episode: 1005   score: 2.0   memory length: 192767   epsilon: 0.8163193600039875    steps: 218    lr: 0.0001     evaluation reward: 2.93\n",
            "episode: 1006   score: 1.0   memory length: 192940   epsilon: 0.815976820003995    steps: 173    lr: 0.0001     evaluation reward: 2.93\n",
            "episode: 1007   score: 2.0   memory length: 193139   epsilon: 0.8155828000040035    steps: 199    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 1008   score: 1.0   memory length: 193312   epsilon: 0.815240260004011    steps: 173    lr: 0.0001     evaluation reward: 2.9\n",
            "episode: 1009   score: 3.0   memory length: 193559   epsilon: 0.8147512000040216    steps: 247    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 1010   score: 2.0   memory length: 193761   epsilon: 0.8143512400040303    steps: 202    lr: 0.0001     evaluation reward: 2.86\n",
            "episode: 1011   score: 3.0   memory length: 194010   epsilon: 0.813858220004041    steps: 249    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 1012   score: 5.0   memory length: 194313   epsilon: 0.813258280004054    steps: 303    lr: 0.0001     evaluation reward: 2.91\n",
            "episode: 1013   score: 2.0   memory length: 194531   epsilon: 0.8128266400040633    steps: 218    lr: 0.0001     evaluation reward: 2.89\n",
            "episode: 1014   score: 2.0   memory length: 194730   epsilon: 0.8124326200040719    steps: 199    lr: 0.0001     evaluation reward: 2.88\n",
            "episode: 1015   score: 6.0   memory length: 195119   epsilon: 0.8116624000040886    steps: 389    lr: 0.0001     evaluation reward: 2.92\n",
            "episode: 1016   score: 5.0   memory length: 195407   epsilon: 0.811092160004101    steps: 288    lr: 0.0001     evaluation reward: 2.95\n",
            "episode: 1017   score: 3.0   memory length: 195655   epsilon: 0.8106011200041117    steps: 248    lr: 0.0001     evaluation reward: 2.96\n",
            "episode: 1018   score: 1.0   memory length: 195824   epsilon: 0.8102665000041189    steps: 169    lr: 0.0001     evaluation reward: 2.95\n",
            "episode: 1019   score: 6.0   memory length: 196150   epsilon: 0.8096210200041329    steps: 326    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 1020   score: 2.0   memory length: 196368   epsilon: 0.8091893800041423    steps: 218    lr: 0.0001     evaluation reward: 2.96\n",
            "episode: 1021   score: 2.0   memory length: 196590   epsilon: 0.8087498200041519    steps: 222    lr: 0.0001     evaluation reward: 2.97\n",
            "episode: 1022   score: 4.0   memory length: 196889   epsilon: 0.8081578000041647    steps: 299    lr: 0.0001     evaluation reward: 3.01\n",
            "episode: 1023   score: 3.0   memory length: 197116   epsilon: 0.8077083400041745    steps: 227    lr: 0.0001     evaluation reward: 3.01\n",
            "episode: 1024   score: 4.0   memory length: 197393   epsilon: 0.8071598800041864    steps: 277    lr: 0.0001     evaluation reward: 3.04\n",
            "episode: 1025   score: 3.0   memory length: 197620   epsilon: 0.8067104200041961    steps: 227    lr: 0.0001     evaluation reward: 3.05\n",
            "episode: 1026   score: 2.0   memory length: 197819   epsilon: 0.8063164000042047    steps: 199    lr: 0.0001     evaluation reward: 3.05\n",
            "episode: 1027   score: 5.0   memory length: 198144   epsilon: 0.8056729000042187    steps: 325    lr: 0.0001     evaluation reward: 3.09\n",
            "episode: 1028   score: 2.0   memory length: 198363   epsilon: 0.8052392800042281    steps: 219    lr: 0.0001     evaluation reward: 3.06\n",
            "episode: 1029   score: 2.0   memory length: 198563   epsilon: 0.8048432800042367    steps: 200    lr: 0.0001     evaluation reward: 3.04\n",
            "episode: 1030   score: 2.0   memory length: 198763   epsilon: 0.8044472800042453    steps: 200    lr: 0.0001     evaluation reward: 3.06\n",
            "episode: 1031   score: 6.0   memory length: 199085   epsilon: 0.8038097200042591    steps: 322    lr: 0.0001     evaluation reward: 3.07\n",
            "episode: 1032   score: 4.0   memory length: 199358   epsilon: 0.8032691800042708    steps: 273    lr: 0.0001     evaluation reward: 3.07\n",
            "episode: 1033   score: 5.0   memory length: 199684   epsilon: 0.8026237000042848    steps: 326    lr: 0.0001     evaluation reward: 3.08\n",
            "episode: 1034   score: 2.0   memory length: 199882   epsilon: 0.8022316600042934    steps: 198    lr: 0.0001     evaluation reward: 3.1\n",
            "episode: 1035   score: 3.0   memory length: 200109   epsilon: 0.8017822000043031    steps: 227    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1036   score: 1.0   memory length: 200282   epsilon: 0.8014396600043106    steps: 173    lr: 0.0001     evaluation reward: 3.09\n",
            "episode: 1037   score: 0.0   memory length: 200406   epsilon: 0.8011941400043159    steps: 124    lr: 0.0001     evaluation reward: 3.06\n",
            "episode: 1038   score: 5.0   memory length: 200753   epsilon: 0.8005070800043308    steps: 347    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1039   score: 3.0   memory length: 201001   epsilon: 0.8000160400043415    steps: 248    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1040   score: 2.0   memory length: 201219   epsilon: 0.7995844000043508    steps: 218    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1041   score: 2.0   memory length: 201402   epsilon: 0.7992220600043587    steps: 183    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1042   score: 4.0   memory length: 201681   epsilon: 0.7986696400043707    steps: 279    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1043   score: 2.0   memory length: 201880   epsilon: 0.7982756200043792    steps: 199    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1044   score: 0.0   memory length: 202004   epsilon: 0.7980301000043846    steps: 124    lr: 0.0001     evaluation reward: 3.07\n",
            "episode: 1045   score: 5.0   memory length: 202309   epsilon: 0.7974262000043977    steps: 305    lr: 0.0001     evaluation reward: 3.08\n",
            "episode: 1046   score: 2.0   memory length: 202528   epsilon: 0.7969925800044071    steps: 219    lr: 0.0001     evaluation reward: 3.07\n",
            "episode: 1047   score: 2.0   memory length: 202746   epsilon: 0.7965609400044165    steps: 218    lr: 0.0001     evaluation reward: 3.06\n",
            "episode: 1048   score: 3.0   memory length: 202977   epsilon: 0.7961035600044264    steps: 231    lr: 0.0001     evaluation reward: 3.05\n",
            "episode: 1049   score: 1.0   memory length: 203150   epsilon: 0.7957610200044338    steps: 173    lr: 0.0001     evaluation reward: 3.02\n",
            "episode: 1050   score: 3.0   memory length: 203399   epsilon: 0.7952680000044445    steps: 249    lr: 0.0001     evaluation reward: 3.03\n",
            "episode: 1051   score: 5.0   memory length: 203726   epsilon: 0.7946205400044586    steps: 327    lr: 0.0001     evaluation reward: 3.07\n",
            "episode: 1052   score: 5.0   memory length: 204033   epsilon: 0.7940126800044718    steps: 307    lr: 0.0001     evaluation reward: 3.05\n",
            "episode: 1053   score: 4.0   memory length: 204290   epsilon: 0.7935038200044828    steps: 257    lr: 0.0001     evaluation reward: 3.03\n",
            "episode: 1054   score: 3.0   memory length: 204518   epsilon: 0.7930523800044926    steps: 228    lr: 0.0001     evaluation reward: 3.01\n",
            "episode: 1055   score: 3.0   memory length: 204766   epsilon: 0.7925613400045033    steps: 248    lr: 0.0001     evaluation reward: 3.02\n",
            "episode: 1056   score: 1.0   memory length: 204917   epsilon: 0.7922623600045098    steps: 151    lr: 0.0001     evaluation reward: 3.01\n",
            "episode: 1057   score: 4.0   memory length: 205178   epsilon: 0.791745580004521    steps: 261    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 1058   score: 2.0   memory length: 205377   epsilon: 0.7913515600045296    steps: 199    lr: 0.0001     evaluation reward: 3.0\n",
            "episode: 1059   score: 6.0   memory length: 205793   epsilon: 0.7905278800045474    steps: 416    lr: 0.0001     evaluation reward: 3.05\n",
            "episode: 1060   score: 1.0   memory length: 205964   epsilon: 0.7901893000045548    steps: 171    lr: 0.0001     evaluation reward: 3.02\n",
            "episode: 1061   score: 6.0   memory length: 206303   epsilon: 0.7895180800045694    steps: 339    lr: 0.0001     evaluation reward: 3.05\n",
            "episode: 1062   score: 5.0   memory length: 206618   epsilon: 0.7888943800045829    steps: 315    lr: 0.0001     evaluation reward: 3.1\n",
            "episode: 1063   score: 4.0   memory length: 206904   epsilon: 0.7883281000045952    steps: 286    lr: 0.0001     evaluation reward: 3.1\n",
            "episode: 1064   score: 2.0   memory length: 207123   epsilon: 0.7878944800046046    steps: 219    lr: 0.0001     evaluation reward: 3.1\n",
            "episode: 1065   score: 4.0   memory length: 207400   epsilon: 0.7873460200046165    steps: 277    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1066   score: 3.0   memory length: 207647   epsilon: 0.7868569600046271    steps: 247    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1067   score: 2.0   memory length: 207845   epsilon: 0.7864649200046356    steps: 198    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1068   score: 1.0   memory length: 207997   epsilon: 0.7861639600046422    steps: 152    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1069   score: 3.0   memory length: 208246   epsilon: 0.7856709400046529    steps: 249    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1070   score: 5.0   memory length: 208565   epsilon: 0.7850393200046666    steps: 319    lr: 0.0001     evaluation reward: 3.09\n",
            "episode: 1071   score: 5.0   memory length: 208857   epsilon: 0.7844611600046791    steps: 292    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1072   score: 0.0   memory length: 208981   epsilon: 0.7842156400046845    steps: 124    lr: 0.0001     evaluation reward: 3.1\n",
            "episode: 1073   score: 7.0   memory length: 209386   epsilon: 0.7834137400047019    steps: 405    lr: 0.0001     evaluation reward: 3.15\n",
            "episode: 1074   score: 3.0   memory length: 209612   epsilon: 0.7829662600047116    steps: 226    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1075   score: 3.0   memory length: 209839   epsilon: 0.7825168000047213    steps: 227    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1076   score: 4.0   memory length: 210116   epsilon: 0.7819683400047333    steps: 277    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1077   score: 3.0   memory length: 210343   epsilon: 0.781518880004743    steps: 227    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1078   score: 3.0   memory length: 210570   epsilon: 0.7810694200047528    steps: 227    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1079   score: 4.0   memory length: 210850   epsilon: 0.7805150200047648    steps: 280    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1080   score: 5.0   memory length: 211174   epsilon: 0.7798735000047787    steps: 324    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1081   score: 1.0   memory length: 211327   epsilon: 0.7795705600047853    steps: 153    lr: 0.0001     evaluation reward: 3.14\n",
            "episode: 1082   score: 3.0   memory length: 211555   epsilon: 0.7791191200047951    steps: 228    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1083   score: 4.0   memory length: 211823   epsilon: 0.7785884800048066    steps: 268    lr: 0.0001     evaluation reward: 3.14\n",
            "episode: 1084   score: 5.0   memory length: 212174   epsilon: 0.7778935000048217    steps: 351    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1085   score: 5.0   memory length: 212471   epsilon: 0.7773054400048345    steps: 297    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1086   score: 3.0   memory length: 212715   epsilon: 0.776822320004845    steps: 244    lr: 0.0001     evaluation reward: 3.1\n",
            "episode: 1087   score: 8.0   memory length: 213131   epsilon: 0.7759986400048628    steps: 416    lr: 0.0001     evaluation reward: 3.14\n",
            "episode: 1088   score: 4.0   memory length: 213393   epsilon: 0.7754798800048741    steps: 262    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1089   score: 4.0   memory length: 213654   epsilon: 0.7749631000048853    steps: 261    lr: 0.0001     evaluation reward: 3.19\n",
            "episode: 1090   score: 2.0   memory length: 213853   epsilon: 0.7745690800048939    steps: 199    lr: 0.0001     evaluation reward: 3.19\n",
            "episode: 1091   score: 3.0   memory length: 214084   epsilon: 0.7741117000049038    steps: 231    lr: 0.0001     evaluation reward: 3.22\n",
            "episode: 1092   score: 3.0   memory length: 214332   epsilon: 0.7736206600049145    steps: 248    lr: 0.0001     evaluation reward: 3.23\n",
            "episode: 1093   score: 3.0   memory length: 214580   epsilon: 0.7731296200049251    steps: 248    lr: 0.0001     evaluation reward: 3.24\n",
            "episode: 1094   score: 1.0   memory length: 214731   epsilon: 0.7728306400049316    steps: 151    lr: 0.0001     evaluation reward: 3.22\n",
            "episode: 1095   score: 2.0   memory length: 214912   epsilon: 0.7724722600049394    steps: 181    lr: 0.0001     evaluation reward: 3.2\n",
            "episode: 1096   score: 1.0   memory length: 215063   epsilon: 0.7721732800049459    steps: 151    lr: 0.0001     evaluation reward: 3.14\n",
            "episode: 1097   score: 2.0   memory length: 215265   epsilon: 0.7717733200049546    steps: 202    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1098   score: 3.0   memory length: 215493   epsilon: 0.7713218800049644    steps: 228    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1099   score: 3.0   memory length: 215722   epsilon: 0.7708684600049742    steps: 229    lr: 0.0001     evaluation reward: 3.14\n",
            "episode: 1100   score: 1.0   memory length: 215894   epsilon: 0.7705279000049816    steps: 172    lr: 0.0001     evaluation reward: 3.1\n",
            "episode: 1101   score: 4.0   memory length: 216170   epsilon: 0.7699814200049935    steps: 276    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1102   score: 4.0   memory length: 216448   epsilon: 0.7694309800050054    steps: 278    lr: 0.0001     evaluation reward: 3.11\n",
            "episode: 1103   score: 4.0   memory length: 216745   epsilon: 0.7688429200050182    steps: 297    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1104   score: 3.0   memory length: 216973   epsilon: 0.768391480005028    steps: 228    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1105   score: 2.0   memory length: 217196   epsilon: 0.7679499400050376    steps: 223    lr: 0.0001     evaluation reward: 3.12\n",
            "episode: 1106   score: 2.0   memory length: 217394   epsilon: 0.7675579000050461    steps: 198    lr: 0.0001     evaluation reward: 3.13\n",
            "episode: 1107   score: 4.0   memory length: 217652   epsilon: 0.7670470600050572    steps: 258    lr: 0.0001     evaluation reward: 3.15\n",
            "episode: 1108   score: 4.0   memory length: 217947   epsilon: 0.7664629600050699    steps: 295    lr: 0.0001     evaluation reward: 3.18\n",
            "episode: 1109   score: 3.0   memory length: 218174   epsilon: 0.7660135000050796    steps: 227    lr: 0.0001     evaluation reward: 3.18\n",
            "episode: 1110   score: 2.0   memory length: 218373   epsilon: 0.7656194800050882    steps: 199    lr: 0.0001     evaluation reward: 3.18\n",
            "episode: 1111   score: 2.0   memory length: 218590   epsilon: 0.7651898200050975    steps: 217    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1112   score: 4.0   memory length: 218865   epsilon: 0.7646453200051093    steps: 275    lr: 0.0001     evaluation reward: 3.16\n",
            "episode: 1113   score: 6.0   memory length: 219242   epsilon: 0.7638988600051255    steps: 377    lr: 0.0001     evaluation reward: 3.2\n",
            "episode: 1114   score: 2.0   memory length: 219441   epsilon: 0.7635048400051341    steps: 199    lr: 0.0001     evaluation reward: 3.2\n",
            "episode: 1115   score: 3.0   memory length: 219667   epsilon: 0.7630573600051438    steps: 226    lr: 0.0001     evaluation reward: 3.17\n",
            "episode: 1116   score: 6.0   memory length: 220030   epsilon: 0.7623386200051594    steps: 363    lr: 0.0001     evaluation reward: 3.18\n",
            "episode: 1117   score: 4.0   memory length: 220308   epsilon: 0.7617881800051713    steps: 278    lr: 0.0001     evaluation reward: 3.19\n",
            "episode: 1118   score: 11.0   memory length: 220839   epsilon: 0.7607368000051942    steps: 531    lr: 0.0001     evaluation reward: 3.29\n",
            "episode: 1119   score: 2.0   memory length: 221037   epsilon: 0.7603447600052027    steps: 198    lr: 0.0001     evaluation reward: 3.25\n",
            "episode: 1120   score: 2.0   memory length: 221220   epsilon: 0.7599824200052105    steps: 183    lr: 0.0001     evaluation reward: 3.25\n",
            "episode: 1121   score: 5.0   memory length: 221513   epsilon: 0.7594022800052231    steps: 293    lr: 0.0001     evaluation reward: 3.28\n",
            "episode: 1122   score: 3.0   memory length: 221743   epsilon: 0.758946880005233    steps: 230    lr: 0.0001     evaluation reward: 3.27\n",
            "episode: 1123   score: 3.0   memory length: 222010   epsilon: 0.7584182200052445    steps: 267    lr: 0.0001     evaluation reward: 3.27\n",
            "episode: 1124   score: 1.0   memory length: 222162   epsilon: 0.758117260005251    steps: 152    lr: 0.0001     evaluation reward: 3.24\n",
            "episode: 1125   score: 6.0   memory length: 222556   epsilon: 0.757337140005268    steps: 394    lr: 0.0001     evaluation reward: 3.27\n",
            "episode: 1126   score: 3.0   memory length: 222802   epsilon: 0.7568500600052785    steps: 246    lr: 0.0001     evaluation reward: 3.28\n",
            "episode: 1127   score: 2.0   memory length: 222983   epsilon: 0.7564916800052863    steps: 181    lr: 0.0001     evaluation reward: 3.25\n",
            "episode: 1128   score: 4.0   memory length: 223243   epsilon: 0.7559768800052975    steps: 260    lr: 0.0001     evaluation reward: 3.27\n",
            "episode: 1129   score: 0.0   memory length: 223367   epsilon: 0.7557313600053028    steps: 124    lr: 0.0001     evaluation reward: 3.25\n",
            "episode: 1130   score: 0.0   memory length: 223491   epsilon: 0.7554858400053082    steps: 124    lr: 0.0001     evaluation reward: 3.23\n",
            "episode: 1131   score: 3.0   memory length: 223723   epsilon: 0.7550264800053181    steps: 232    lr: 0.0001     evaluation reward: 3.2\n",
            "episode: 1132   score: 3.0   memory length: 223973   epsilon: 0.7545314800053289    steps: 250    lr: 0.0001     evaluation reward: 3.19\n",
            "episode: 1133   score: 2.0   memory length: 224172   epsilon: 0.7541374600053374    steps: 199    lr: 0.0001     evaluation reward: 3.16\n",
            "episode: 1134   score: 6.0   memory length: 224546   epsilon: 0.7533969400053535    steps: 374    lr: 0.0001     evaluation reward: 3.2\n",
            "episode: 1135   score: 5.0   memory length: 224852   epsilon: 0.7527910600053667    steps: 306    lr: 0.0001     evaluation reward: 3.22\n",
            "episode: 1136   score: 4.0   memory length: 225147   epsilon: 0.7522069600053793    steps: 295    lr: 0.0001     evaluation reward: 3.25\n",
            "episode: 1137   score: 4.0   memory length: 225443   epsilon: 0.7516208800053921    steps: 296    lr: 0.0001     evaluation reward: 3.29\n",
            "episode: 1138   score: 4.0   memory length: 225718   epsilon: 0.7510763800054039    steps: 275    lr: 0.0001     evaluation reward: 3.28\n",
            "episode: 1139   score: 3.0   memory length: 225966   epsilon: 0.7505853400054145    steps: 248    lr: 0.0001     evaluation reward: 3.28\n",
            "episode: 1140   score: 4.0   memory length: 226242   epsilon: 0.7500388600054264    steps: 276    lr: 0.0001     evaluation reward: 3.3\n",
            "episode: 1141   score: 2.0   memory length: 226425   epsilon: 0.7496765200054343    steps: 183    lr: 0.0001     evaluation reward: 3.3\n",
            "episode: 1142   score: 5.0   memory length: 226733   epsilon: 0.7490666800054475    steps: 308    lr: 0.0001     evaluation reward: 3.31\n",
            "episode: 1143   score: 1.0   memory length: 226884   epsilon: 0.748767700005454    steps: 151    lr: 0.0001     evaluation reward: 3.3\n",
            "episode: 1144   score: 3.0   memory length: 227098   epsilon: 0.7483439800054632    steps: 214    lr: 0.0001     evaluation reward: 3.33\n",
            "episode: 1145   score: 3.0   memory length: 227344   epsilon: 0.7478569000054738    steps: 246    lr: 0.0001     evaluation reward: 3.31\n",
            "episode: 1146   score: 4.0   memory length: 227633   epsilon: 0.7472846800054862    steps: 289    lr: 0.0001     evaluation reward: 3.33\n",
            "episode: 1147   score: 3.0   memory length: 227848   epsilon: 0.7468589800054954    steps: 215    lr: 0.0001     evaluation reward: 3.34\n",
            "episode: 1148   score: 4.0   memory length: 228169   epsilon: 0.7462234000055092    steps: 321    lr: 0.0001     evaluation reward: 3.35\n",
            "episode: 1149   score: 4.0   memory length: 228445   epsilon: 0.7456769200055211    steps: 276    lr: 0.0001     evaluation reward: 3.38\n",
            "episode: 1150   score: 3.0   memory length: 228691   epsilon: 0.7451898400055317    steps: 246    lr: 0.0001     evaluation reward: 3.38\n",
            "episode: 1151   score: 5.0   memory length: 228988   epsilon: 0.7446017800055444    steps: 297    lr: 0.0001     evaluation reward: 3.38\n",
            "episode: 1152   score: 2.0   memory length: 229186   epsilon: 0.744209740005553    steps: 198    lr: 0.0001     evaluation reward: 3.35\n",
            "episode: 1153   score: 1.0   memory length: 229339   epsilon: 0.7439068000055595    steps: 153    lr: 0.0001     evaluation reward: 3.32\n",
            "episode: 1154   score: 3.0   memory length: 229568   epsilon: 0.7434533800055694    steps: 229    lr: 0.0001     evaluation reward: 3.32\n",
            "episode: 1155   score: 2.0   memory length: 229766   epsilon: 0.7430613400055779    steps: 198    lr: 0.0001     evaluation reward: 3.31\n",
            "episode: 1156   score: 3.0   memory length: 229995   epsilon: 0.7426079200055877    steps: 229    lr: 0.0001     evaluation reward: 3.33\n",
            "episode: 1157   score: 5.0   memory length: 230339   epsilon: 0.7419268000056025    steps: 344    lr: 0.0001     evaluation reward: 3.34\n",
            "episode: 1158   score: 4.0   memory length: 230657   epsilon: 0.7412971600056162    steps: 318    lr: 0.0001     evaluation reward: 3.36\n",
            "episode: 1159   score: 3.0   memory length: 230904   epsilon: 0.7408081000056268    steps: 247    lr: 0.0001     evaluation reward: 3.33\n",
            "episode: 1160   score: 6.0   memory length: 231258   epsilon: 0.740107180005642    steps: 354    lr: 0.0001     evaluation reward: 3.38\n",
            "episode: 1161   score: 6.0   memory length: 231652   epsilon: 0.739327060005659    steps: 394    lr: 0.0001     evaluation reward: 3.38\n",
            "episode: 1162   score: 2.0   memory length: 231850   epsilon: 0.7389350200056675    steps: 198    lr: 0.0001     evaluation reward: 3.35\n",
            "episode: 1163   score: 4.0   memory length: 232127   epsilon: 0.7383865600056794    steps: 277    lr: 0.0001     evaluation reward: 3.35\n",
            "episode: 1164   score: 2.0   memory length: 232308   epsilon: 0.7380281800056872    steps: 181    lr: 0.0001     evaluation reward: 3.35\n",
            "episode: 1165   score: 2.0   memory length: 232507   epsilon: 0.7376341600056957    steps: 199    lr: 0.0001     evaluation reward: 3.33\n",
            "episode: 1166   score: 0.0   memory length: 232631   epsilon: 0.737388640005701    steps: 124    lr: 0.0001     evaluation reward: 3.3\n",
            "episode: 1167   score: 4.0   memory length: 232907   epsilon: 0.7368421600057129    steps: 276    lr: 0.0001     evaluation reward: 3.32\n",
            "episode: 1168   score: 3.0   memory length: 233154   epsilon: 0.7363531000057235    steps: 247    lr: 0.0001     evaluation reward: 3.34\n",
            "episode: 1169   score: 5.0   memory length: 233478   epsilon: 0.7357115800057374    steps: 324    lr: 0.0001     evaluation reward: 3.36\n",
            "episode: 1170   score: 6.0   memory length: 233836   epsilon: 0.7350027400057528    steps: 358    lr: 0.0001     evaluation reward: 3.37\n",
            "episode: 1171   score: 5.0   memory length: 234127   epsilon: 0.7344265600057653    steps: 291    lr: 0.0001     evaluation reward: 3.37\n",
            "episode: 1172   score: 6.0   memory length: 234465   epsilon: 0.7337573200057799    steps: 338    lr: 0.0001     evaluation reward: 3.43\n",
            "episode: 1173   score: 3.0   memory length: 234674   epsilon: 0.7333435000057889    steps: 209    lr: 0.0001     evaluation reward: 3.39\n",
            "episode: 1174   score: 5.0   memory length: 235017   epsilon: 0.7326643600058036    steps: 343    lr: 0.0001     evaluation reward: 3.41\n",
            "episode: 1175   score: 5.0   memory length: 235343   epsilon: 0.7320188800058176    steps: 326    lr: 0.0001     evaluation reward: 3.43\n",
            "episode: 1176   score: 7.0   memory length: 235768   epsilon: 0.7311773800058359    steps: 425    lr: 0.0001     evaluation reward: 3.46\n",
            "episode: 1177   score: 5.0   memory length: 236049   epsilon: 0.730621000005848    steps: 281    lr: 0.0001     evaluation reward: 3.48\n",
            "episode: 1178   score: 0.0   memory length: 236173   epsilon: 0.7303754800058533    steps: 124    lr: 0.0001     evaluation reward: 3.45\n",
            "episode: 1179   score: 3.0   memory length: 236420   epsilon: 0.7298864200058639    steps: 247    lr: 0.0001     evaluation reward: 3.44\n",
            "episode: 1180   score: 4.0   memory length: 236696   epsilon: 0.7293399400058758    steps: 276    lr: 0.0001     evaluation reward: 3.43\n",
            "episode: 1181   score: 1.0   memory length: 236866   epsilon: 0.7290033400058831    steps: 170    lr: 0.0001     evaluation reward: 3.43\n",
            "episode: 1182   score: 1.0   memory length: 237018   epsilon: 0.7287023800058896    steps: 152    lr: 0.0001     evaluation reward: 3.41\n",
            "episode: 1183   score: 6.0   memory length: 237271   epsilon: 0.7282014400059005    steps: 253    lr: 0.0001     evaluation reward: 3.43\n",
            "episode: 1184   score: 10.0   memory length: 237680   epsilon: 0.7273916200059181    steps: 409    lr: 0.0001     evaluation reward: 3.48\n",
            "episode: 1185   score: 3.0   memory length: 237928   epsilon: 0.7269005800059287    steps: 248    lr: 0.0001     evaluation reward: 3.46\n",
            "episode: 1186   score: 3.0   memory length: 238155   epsilon: 0.7264511200059385    steps: 227    lr: 0.0001     evaluation reward: 3.46\n",
            "episode: 1187   score: 4.0   memory length: 238431   epsilon: 0.7259046400059503    steps: 276    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1188   score: 5.0   memory length: 238790   epsilon: 0.7251938200059658    steps: 359    lr: 0.0001     evaluation reward: 3.43\n",
            "episode: 1189   score: 0.0   memory length: 238914   epsilon: 0.7249483000059711    steps: 124    lr: 0.0001     evaluation reward: 3.39\n",
            "episode: 1190   score: 3.0   memory length: 239161   epsilon: 0.7244592400059817    steps: 247    lr: 0.0001     evaluation reward: 3.4\n",
            "episode: 1191   score: 3.0   memory length: 239408   epsilon: 0.7239701800059923    steps: 247    lr: 0.0001     evaluation reward: 3.4\n",
            "episode: 1192   score: 4.0   memory length: 239728   epsilon: 0.7233365800060061    steps: 320    lr: 0.0001     evaluation reward: 3.41\n",
            "episode: 1193   score: 4.0   memory length: 240004   epsilon: 0.722790100006018    steps: 276    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1194   score: 1.0   memory length: 240155   epsilon: 0.7224911200060244    steps: 151    lr: 0.0001     evaluation reward: 3.42\n",
            "episode: 1195   score: 6.0   memory length: 240477   epsilon: 0.7218535600060383    steps: 322    lr: 0.0001     evaluation reward: 3.46\n",
            "episode: 1196   score: 3.0   memory length: 240742   epsilon: 0.7213288600060497    steps: 265    lr: 0.0001     evaluation reward: 3.48\n",
            "episode: 1197   score: 4.0   memory length: 241005   epsilon: 0.720808120006061    steps: 263    lr: 0.0001     evaluation reward: 3.5\n",
            "episode: 1198   score: 1.0   memory length: 241174   epsilon: 0.7204735000060682    steps: 169    lr: 0.0001     evaluation reward: 3.48\n",
            "episode: 1199   score: 2.0   memory length: 241373   epsilon: 0.7200794800060768    steps: 199    lr: 0.0001     evaluation reward: 3.47\n",
            "episode: 1200   score: 12.0   memory length: 241897   epsilon: 0.7190419600060993    steps: 524    lr: 0.0001     evaluation reward: 3.58\n",
            "episode: 1201   score: 4.0   memory length: 242193   epsilon: 0.718455880006112    steps: 296    lr: 0.0001     evaluation reward: 3.58\n",
            "episode: 1202   score: 1.0   memory length: 242365   epsilon: 0.7181153200061194    steps: 172    lr: 0.0001     evaluation reward: 3.55\n",
            "episode: 1203   score: 5.0   memory length: 242671   epsilon: 0.7175094400061326    steps: 306    lr: 0.0001     evaluation reward: 3.56\n",
            "episode: 1204   score: 1.0   memory length: 242840   epsilon: 0.7171748200061399    steps: 169    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1205   score: 2.0   memory length: 243039   epsilon: 0.7167808000061484    steps: 199    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1206   score: 1.0   memory length: 243190   epsilon: 0.7164818200061549    steps: 151    lr: 0.0001     evaluation reward: 3.53\n",
            "episode: 1207   score: 3.0   memory length: 243436   epsilon: 0.7159947400061655    steps: 246    lr: 0.0001     evaluation reward: 3.52\n",
            "episode: 1208   score: 6.0   memory length: 243755   epsilon: 0.7153631200061792    steps: 319    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1209   score: 3.0   memory length: 244004   epsilon: 0.7148701000061899    steps: 249    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1210   score: 2.0   memory length: 244202   epsilon: 0.7144780600061984    steps: 198    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1211   score: 7.0   memory length: 244601   epsilon: 0.7136880400062156    steps: 399    lr: 0.0001     evaluation reward: 3.59\n",
            "episode: 1212   score: 5.0   memory length: 244903   epsilon: 0.7130900800062285    steps: 302    lr: 0.0001     evaluation reward: 3.6\n",
            "episode: 1213   score: 3.0   memory length: 245150   epsilon: 0.7126010200062391    steps: 247    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1214   score: 2.0   memory length: 245349   epsilon: 0.7122070000062477    steps: 199    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1215   score: 9.0   memory length: 245693   epsilon: 0.7115258800062625    steps: 344    lr: 0.0001     evaluation reward: 3.63\n",
            "episode: 1216   score: 3.0   memory length: 245920   epsilon: 0.7110764200062722    steps: 227    lr: 0.0001     evaluation reward: 3.6\n",
            "episode: 1217   score: 3.0   memory length: 246164   epsilon: 0.7105933000062827    steps: 244    lr: 0.0001     evaluation reward: 3.59\n",
            "episode: 1218   score: 1.0   memory length: 246316   epsilon: 0.7102923400062893    steps: 152    lr: 0.0001     evaluation reward: 3.49\n",
            "episode: 1219   score: 3.0   memory length: 246529   epsilon: 0.7098706000062984    steps: 213    lr: 0.0001     evaluation reward: 3.5\n",
            "episode: 1220   score: 2.0   memory length: 246727   epsilon: 0.7094785600063069    steps: 198    lr: 0.0001     evaluation reward: 3.5\n",
            "episode: 1221   score: 3.0   memory length: 246994   epsilon: 0.7089499000063184    steps: 267    lr: 0.0001     evaluation reward: 3.48\n",
            "episode: 1222   score: 5.0   memory length: 247320   epsilon: 0.7083044200063324    steps: 326    lr: 0.0001     evaluation reward: 3.5\n",
            "episode: 1223   score: 5.0   memory length: 247636   epsilon: 0.707678740006346    steps: 316    lr: 0.0001     evaluation reward: 3.52\n",
            "episode: 1224   score: 3.0   memory length: 247850   epsilon: 0.7072550200063552    steps: 214    lr: 0.0001     evaluation reward: 3.54\n",
            "episode: 1225   score: 4.0   memory length: 248126   epsilon: 0.7067085400063671    steps: 276    lr: 0.0001     evaluation reward: 3.52\n",
            "episode: 1226   score: 1.0   memory length: 248277   epsilon: 0.7064095600063736    steps: 151    lr: 0.0001     evaluation reward: 3.5\n",
            "episode: 1227   score: 1.0   memory length: 248450   epsilon: 0.706067020006381    steps: 173    lr: 0.0001     evaluation reward: 3.49\n",
            "episode: 1228   score: 3.0   memory length: 248680   epsilon: 0.7056116200063909    steps: 230    lr: 0.0001     evaluation reward: 3.48\n",
            "episode: 1229   score: 3.0   memory length: 248927   epsilon: 0.7051225600064015    steps: 247    lr: 0.0001     evaluation reward: 3.51\n",
            "episode: 1230   score: 4.0   memory length: 249204   epsilon: 0.7045741000064134    steps: 277    lr: 0.0001     evaluation reward: 3.55\n",
            "episode: 1231   score: 6.0   memory length: 249549   epsilon: 0.7038910000064282    steps: 345    lr: 0.0001     evaluation reward: 3.58\n",
            "episode: 1232   score: 2.0   memory length: 249730   epsilon: 0.703532620006436    steps: 181    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1233   score: 5.0   memory length: 250075   epsilon: 0.7028495200064508    steps: 345    lr: 0.0001     evaluation reward: 3.6\n",
            "episode: 1234   score: 4.0   memory length: 250369   epsilon: 0.7022674000064635    steps: 294    lr: 0.0001     evaluation reward: 3.58\n",
            "episode: 1235   score: 4.0   memory length: 250646   epsilon: 0.7017189400064754    steps: 277    lr: 0.0001     evaluation reward: 3.57\n",
            "episode: 1236   score: 5.0   memory length: 250983   epsilon: 0.7010516800064899    steps: 337    lr: 0.0001     evaluation reward: 3.58\n",
            "episode: 1237   score: 5.0   memory length: 251310   epsilon: 0.7004042200065039    steps: 327    lr: 0.0001     evaluation reward: 3.59\n",
            "episode: 1238   score: 6.0   memory length: 251685   epsilon: 0.69966172000652    steps: 375    lr: 0.0001     evaluation reward: 3.61\n",
            "episode: 1239   score: 4.0   memory length: 251982   epsilon: 0.6990736600065328    steps: 297    lr: 0.0001     evaluation reward: 3.62\n",
            "episode: 1240   score: 6.0   memory length: 252300   epsilon: 0.6984440200065465    steps: 318    lr: 0.0001     evaluation reward: 3.64\n",
            "episode: 1241   score: 7.0   memory length: 252710   epsilon: 0.6976322200065641    steps: 410    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1242   score: 3.0   memory length: 252937   epsilon: 0.6971827600065739    steps: 227    lr: 0.0001     evaluation reward: 3.67\n",
            "episode: 1243   score: 3.0   memory length: 253167   epsilon: 0.6967273600065838    steps: 230    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1244   score: 3.0   memory length: 253414   epsilon: 0.6962383000065944    steps: 247    lr: 0.0001     evaluation reward: 3.69\n",
            "episode: 1245   score: 4.0   memory length: 253711   epsilon: 0.6956502400066071    steps: 297    lr: 0.0001     evaluation reward: 3.7\n",
            "episode: 1246   score: 6.0   memory length: 254070   epsilon: 0.6949394200066226    steps: 359    lr: 0.0001     evaluation reward: 3.72\n",
            "episode: 1247   score: 5.0   memory length: 254394   epsilon: 0.6942979000066365    steps: 324    lr: 0.0001     evaluation reward: 3.74\n",
            "episode: 1248   score: 7.0   memory length: 254838   epsilon: 0.6934187800066556    steps: 444    lr: 0.0001     evaluation reward: 3.77\n",
            "episode: 1249   score: 3.0   memory length: 255065   epsilon: 0.6929693200066653    steps: 227    lr: 0.0001     evaluation reward: 3.76\n",
            "episode: 1250   score: 1.0   memory length: 255217   epsilon: 0.6926683600066719    steps: 152    lr: 0.0001     evaluation reward: 3.74\n",
            "episode: 1251   score: 3.0   memory length: 255463   epsilon: 0.6921812800066824    steps: 246    lr: 0.0001     evaluation reward: 3.72\n",
            "episode: 1252   score: 4.0   memory length: 255739   epsilon: 0.6916348000066943    steps: 276    lr: 0.0001     evaluation reward: 3.74\n",
            "episode: 1253   score: 2.0   memory length: 255939   epsilon: 0.6912388000067029    steps: 200    lr: 0.0001     evaluation reward: 3.75\n",
            "episode: 1254   score: 3.0   memory length: 256149   epsilon: 0.6908230000067119    steps: 210    lr: 0.0001     evaluation reward: 3.75\n",
            "episode: 1255   score: 6.0   memory length: 256458   epsilon: 0.6902111800067252    steps: 309    lr: 0.0001     evaluation reward: 3.79\n",
            "episode: 1256   score: 6.0   memory length: 256827   epsilon: 0.6894805600067411    steps: 369    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1257   score: 4.0   memory length: 257089   epsilon: 0.6889618000067523    steps: 262    lr: 0.0001     evaluation reward: 3.81\n",
            "episode: 1258   score: 7.0   memory length: 257472   epsilon: 0.6882034600067688    steps: 383    lr: 0.0001     evaluation reward: 3.84\n",
            "episode: 1259   score: 3.0   memory length: 257702   epsilon: 0.6877480600067787    steps: 230    lr: 0.0001     evaluation reward: 3.84\n",
            "episode: 1260   score: 5.0   memory length: 258011   epsilon: 0.687136240006792    steps: 309    lr: 0.0001     evaluation reward: 3.83\n",
            "episode: 1261   score: 4.0   memory length: 258305   epsilon: 0.6865541200068046    steps: 294    lr: 0.0001     evaluation reward: 3.81\n",
            "episode: 1262   score: 4.0   memory length: 258619   epsilon: 0.6859324000068181    steps: 314    lr: 0.0001     evaluation reward: 3.83\n",
            "episode: 1263   score: 5.0   memory length: 258906   epsilon: 0.6853641400068304    steps: 287    lr: 0.0001     evaluation reward: 3.84\n",
            "episode: 1264   score: 3.0   memory length: 259154   epsilon: 0.6848731000068411    steps: 248    lr: 0.0001     evaluation reward: 3.85\n",
            "episode: 1265   score: 2.0   memory length: 259376   epsilon: 0.6844335400068506    steps: 222    lr: 0.0001     evaluation reward: 3.85\n",
            "episode: 1266   score: 5.0   memory length: 259701   epsilon: 0.6837900400068646    steps: 325    lr: 0.0001     evaluation reward: 3.9\n",
            "episode: 1267   score: 2.0   memory length: 259901   epsilon: 0.6833940400068732    steps: 200    lr: 0.0001     evaluation reward: 3.88\n",
            "episode: 1268   score: 3.0   memory length: 260173   epsilon: 0.6828554800068849    steps: 272    lr: 0.0001     evaluation reward: 3.88\n",
            "episode: 1269   score: 3.0   memory length: 260421   epsilon: 0.6823644400068956    steps: 248    lr: 0.0001     evaluation reward: 3.86\n",
            "episode: 1270   score: 5.0   memory length: 260728   epsilon: 0.6817565800069088    steps: 307    lr: 0.0001     evaluation reward: 3.85\n",
            "episode: 1271   score: 5.0   memory length: 260996   epsilon: 0.6812259400069203    steps: 268    lr: 0.0001     evaluation reward: 3.85\n",
            "episode: 1272   score: 4.0   memory length: 261273   epsilon: 0.6806774800069322    steps: 277    lr: 0.0001     evaluation reward: 3.83\n",
            "episode: 1273   score: 2.0   memory length: 261471   epsilon: 0.6802854400069407    steps: 198    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1274   score: 8.0   memory length: 261967   epsilon: 0.679303360006962    steps: 496    lr: 0.0001     evaluation reward: 3.85\n",
            "episode: 1275   score: 5.0   memory length: 262285   epsilon: 0.6786737200069757    steps: 318    lr: 0.0001     evaluation reward: 3.85\n",
            "episode: 1276   score: 3.0   memory length: 262497   epsilon: 0.6782539600069848    steps: 212    lr: 0.0001     evaluation reward: 3.81\n",
            "episode: 1277   score: 2.0   memory length: 262678   epsilon: 0.6778955800069926    steps: 181    lr: 0.0001     evaluation reward: 3.78\n",
            "episode: 1278   score: 4.0   memory length: 262921   epsilon: 0.677414440007003    steps: 243    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1279   score: 3.0   memory length: 263149   epsilon: 0.6769630000070128    steps: 228    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1280   score: 7.0   memory length: 263565   epsilon: 0.6761393200070307    steps: 416    lr: 0.0001     evaluation reward: 3.85\n",
            "episode: 1281   score: 4.0   memory length: 263841   epsilon: 0.6755928400070426    steps: 276    lr: 0.0001     evaluation reward: 3.88\n",
            "episode: 1282   score: 1.0   memory length: 264013   epsilon: 0.67525228000705    steps: 172    lr: 0.0001     evaluation reward: 3.88\n",
            "episode: 1283   score: 5.0   memory length: 264343   epsilon: 0.6745988800070641    steps: 330    lr: 0.0001     evaluation reward: 3.87\n",
            "episode: 1284   score: 4.0   memory length: 264638   epsilon: 0.6740147800070768    steps: 295    lr: 0.0001     evaluation reward: 3.81\n",
            "episode: 1285   score: 1.0   memory length: 264790   epsilon: 0.6737138200070834    steps: 152    lr: 0.0001     evaluation reward: 3.79\n",
            "episode: 1286   score: 6.0   memory length: 265155   epsilon: 0.672991120007099    steps: 365    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1287   score: 2.0   memory length: 265355   epsilon: 0.6725951200071076    steps: 200    lr: 0.0001     evaluation reward: 3.8\n",
            "episode: 1288   score: 1.0   memory length: 265507   epsilon: 0.6722941600071142    steps: 152    lr: 0.0001     evaluation reward: 3.76\n",
            "episode: 1289   score: 4.0   memory length: 265767   epsilon: 0.6717793600071253    steps: 260    lr: 0.0001     evaluation reward: 3.8\n",
            "episode: 1290   score: 1.0   memory length: 265919   epsilon: 0.6714784000071319    steps: 152    lr: 0.0001     evaluation reward: 3.78\n",
            "episode: 1291   score: 7.0   memory length: 266291   epsilon: 0.6707418400071479    steps: 372    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1292   score: 3.0   memory length: 266542   epsilon: 0.6702448600071587    steps: 251    lr: 0.0001     evaluation reward: 3.81\n",
            "episode: 1293   score: 3.0   memory length: 266770   epsilon: 0.6697934200071685    steps: 228    lr: 0.0001     evaluation reward: 3.8\n",
            "episode: 1294   score: 3.0   memory length: 266997   epsilon: 0.6693439600071782    steps: 227    lr: 0.0001     evaluation reward: 3.82\n",
            "episode: 1295   score: 3.0   memory length: 267244   epsilon: 0.6688549000071888    steps: 247    lr: 0.0001     evaluation reward: 3.79\n",
            "episode: 1296   score: 3.0   memory length: 267491   epsilon: 0.6683658400071995    steps: 247    lr: 0.0001     evaluation reward: 3.79\n",
            "episode: 1297   score: 3.0   memory length: 267719   epsilon: 0.6679144000072093    steps: 228    lr: 0.0001     evaluation reward: 3.78\n",
            "episode: 1298   score: 6.0   memory length: 268084   epsilon: 0.6671917000072249    steps: 365    lr: 0.0001     evaluation reward: 3.83\n",
            "episode: 1299   score: 3.0   memory length: 268333   epsilon: 0.6666986800072356    steps: 249    lr: 0.0001     evaluation reward: 3.84\n",
            "episode: 1300   score: 9.0   memory length: 268798   epsilon: 0.6657779800072556    steps: 465    lr: 0.0001     evaluation reward: 3.81\n",
            "episode: 1301   score: 9.0   memory length: 269255   epsilon: 0.6648731200072753    steps: 457    lr: 0.0001     evaluation reward: 3.86\n",
            "episode: 1302   score: 5.0   memory length: 269586   epsilon: 0.6642177400072895    steps: 331    lr: 0.0001     evaluation reward: 3.9\n",
            "episode: 1303   score: 4.0   memory length: 269862   epsilon: 0.6636712600073014    steps: 276    lr: 0.0001     evaluation reward: 3.89\n",
            "episode: 1304   score: 3.0   memory length: 270092   epsilon: 0.6632158600073113    steps: 230    lr: 0.0001     evaluation reward: 3.91\n",
            "episode: 1305   score: 7.0   memory length: 270471   epsilon: 0.6624654400073275    steps: 379    lr: 0.0001     evaluation reward: 3.96\n",
            "episode: 1306   score: 4.0   memory length: 270746   epsilon: 0.6619209400073394    steps: 275    lr: 0.0001     evaluation reward: 3.99\n",
            "episode: 1307   score: 6.0   memory length: 271138   epsilon: 0.6611447800073562    steps: 392    lr: 0.0001     evaluation reward: 4.02\n",
            "episode: 1308   score: 5.0   memory length: 271463   epsilon: 0.6605012800073702    steps: 325    lr: 0.0001     evaluation reward: 4.01\n",
            "episode: 1309   score: 4.0   memory length: 271759   epsilon: 0.6599152000073829    steps: 296    lr: 0.0001     evaluation reward: 4.02\n",
            "episode: 1310   score: 3.0   memory length: 271989   epsilon: 0.6594598000073928    steps: 230    lr: 0.0001     evaluation reward: 4.03\n",
            "episode: 1311   score: 2.0   memory length: 272171   epsilon: 0.6590994400074006    steps: 182    lr: 0.0001     evaluation reward: 3.98\n",
            "episode: 1312   score: 2.0   memory length: 272390   epsilon: 0.65866582000741    steps: 219    lr: 0.0001     evaluation reward: 3.95\n",
            "episode: 1313   score: 5.0   memory length: 272705   epsilon: 0.6580421200074236    steps: 315    lr: 0.0001     evaluation reward: 3.97\n",
            "episode: 1314   score: 5.0   memory length: 272995   epsilon: 0.657467920007436    steps: 290    lr: 0.0001     evaluation reward: 4.0\n",
            "episode: 1315   score: 7.0   memory length: 273401   epsilon: 0.6566640400074535    steps: 406    lr: 0.0001     evaluation reward: 3.98\n",
            "episode: 1316   score: 5.0   memory length: 273720   epsilon: 0.6560324200074672    steps: 319    lr: 0.0001     evaluation reward: 4.0\n",
            "episode: 1317   score: 5.0   memory length: 274036   epsilon: 0.6554067400074808    steps: 316    lr: 0.0001     evaluation reward: 4.02\n",
            "episode: 1318   score: 8.0   memory length: 274384   epsilon: 0.6547177000074957    steps: 348    lr: 0.0001     evaluation reward: 4.09\n",
            "episode: 1319   score: 2.0   memory length: 274583   epsilon: 0.6543236800075043    steps: 199    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1320   score: 5.0   memory length: 274908   epsilon: 0.6536801800075183    steps: 325    lr: 0.0001     evaluation reward: 4.11\n",
            "episode: 1321   score: 7.0   memory length: 275333   epsilon: 0.6528386800075365    steps: 425    lr: 0.0001     evaluation reward: 4.15\n",
            "episode: 1322   score: 4.0   memory length: 275633   epsilon: 0.6522446800075494    steps: 300    lr: 0.0001     evaluation reward: 4.14\n",
            "episode: 1323   score: 3.0   memory length: 275863   epsilon: 0.6517892800075593    steps: 230    lr: 0.0001     evaluation reward: 4.12\n",
            "episode: 1324   score: 4.0   memory length: 276122   epsilon: 0.6512764600075704    steps: 259    lr: 0.0001     evaluation reward: 4.13\n",
            "episode: 1325   score: 3.0   memory length: 276373   epsilon: 0.6507794800075812    steps: 251    lr: 0.0001     evaluation reward: 4.12\n",
            "episode: 1326   score: 1.0   memory length: 276544   epsilon: 0.6504409000075886    steps: 171    lr: 0.0001     evaluation reward: 4.12\n",
            "episode: 1327   score: 6.0   memory length: 276864   epsilon: 0.6498073000076023    steps: 320    lr: 0.0001     evaluation reward: 4.17\n",
            "episode: 1328   score: 5.0   memory length: 277189   epsilon: 0.6491638000076163    steps: 325    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1329   score: 4.0   memory length: 277431   epsilon: 0.6486846400076267    steps: 242    lr: 0.0001     evaluation reward: 4.2\n",
            "episode: 1330   score: 7.0   memory length: 277857   epsilon: 0.647841160007645    steps: 426    lr: 0.0001     evaluation reward: 4.23\n",
            "episode: 1331   score: 2.0   memory length: 278074   epsilon: 0.6474115000076544    steps: 217    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1332   score: 5.0   memory length: 278401   epsilon: 0.6467640400076684    steps: 327    lr: 0.0001     evaluation reward: 4.22\n",
            "episode: 1333   score: 1.0   memory length: 278571   epsilon: 0.6464274400076757    steps: 170    lr: 0.0001     evaluation reward: 4.18\n",
            "episode: 1334   score: 7.0   memory length: 278983   epsilon: 0.6456116800076934    steps: 412    lr: 0.0001     evaluation reward: 4.21\n",
            "episode: 1335   score: 2.0   memory length: 279165   epsilon: 0.6452513200077012    steps: 182    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1336   score: 2.0   memory length: 279366   epsilon: 0.6448533400077099    steps: 201    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1337   score: 2.0   memory length: 279565   epsilon: 0.6444593200077184    steps: 199    lr: 0.0001     evaluation reward: 4.13\n",
            "episode: 1338   score: 4.0   memory length: 279825   epsilon: 0.6439445200077296    steps: 260    lr: 0.0001     evaluation reward: 4.11\n",
            "episode: 1339   score: 6.0   memory length: 280222   epsilon: 0.6431584600077467    steps: 397    lr: 0.0001     evaluation reward: 4.13\n",
            "episode: 1340   score: 3.0   memory length: 280452   epsilon: 0.6427030600077566    steps: 230    lr: 0.0001     evaluation reward: 4.1\n",
            "episode: 1341   score: 5.0   memory length: 280743   epsilon: 0.6421268800077691    steps: 291    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1342   score: 3.0   memory length: 281009   epsilon: 0.6416002000077805    steps: 266    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1343   score: 3.0   memory length: 281236   epsilon: 0.6411507400077903    steps: 227    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1344   score: 2.0   memory length: 281437   epsilon: 0.6407527600077989    steps: 201    lr: 0.0001     evaluation reward: 4.07\n",
            "episode: 1345   score: 6.0   memory length: 281833   epsilon: 0.6399686800078159    steps: 396    lr: 0.0001     evaluation reward: 4.09\n",
            "episode: 1346   score: 5.0   memory length: 282160   epsilon: 0.63932122000783    steps: 327    lr: 0.0001     evaluation reward: 4.08\n",
            "episode: 1347   score: 8.0   memory length: 282602   epsilon: 0.638446060007849    steps: 442    lr: 0.0001     evaluation reward: 4.11\n",
            "episode: 1348   score: 9.0   memory length: 283083   epsilon: 0.6374936800078697    steps: 481    lr: 0.0001     evaluation reward: 4.13\n",
            "episode: 1349   score: 6.0   memory length: 283451   epsilon: 0.6367650400078855    steps: 368    lr: 0.0001     evaluation reward: 4.16\n",
            "episode: 1350   score: 4.0   memory length: 283732   epsilon: 0.6362086600078976    steps: 281    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1351   score: 3.0   memory length: 283960   epsilon: 0.6357572200079074    steps: 228    lr: 0.0001     evaluation reward: 4.19\n",
            "episode: 1352   score: 9.0   memory length: 284462   epsilon: 0.6347632600079289    steps: 502    lr: 0.0001     evaluation reward: 4.24\n",
            "episode: 1353   score: 7.0   memory length: 284889   epsilon: 0.6339178000079473    steps: 427    lr: 0.0001     evaluation reward: 4.29\n",
            "episode: 1354   score: 4.0   memory length: 285164   epsilon: 0.6333733000079591    steps: 275    lr: 0.0001     evaluation reward: 4.3\n",
            "episode: 1355   score: 2.0   memory length: 285363   epsilon: 0.6329792800079677    steps: 199    lr: 0.0001     evaluation reward: 4.26\n",
            "episode: 1356   score: 5.0   memory length: 285680   epsilon: 0.6323516200079813    steps: 317    lr: 0.0001     evaluation reward: 4.25\n",
            "episode: 1357   score: 7.0   memory length: 286063   epsilon: 0.6315932800079977    steps: 383    lr: 0.0001     evaluation reward: 4.28\n",
            "episode: 1358   score: 7.0   memory length: 286457   epsilon: 0.6308131600080147    steps: 394    lr: 0.0001     evaluation reward: 4.28\n",
            "episode: 1359   score: 7.0   memory length: 286865   epsilon: 0.6300053200080322    steps: 408    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1360   score: 2.0   memory length: 287064   epsilon: 0.6296113000080408    steps: 199    lr: 0.0001     evaluation reward: 4.29\n",
            "episode: 1361   score: 7.0   memory length: 287481   epsilon: 0.6287856400080587    steps: 417    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1362   score: 5.0   memory length: 287791   epsilon: 0.628171840008072    steps: 310    lr: 0.0001     evaluation reward: 4.33\n",
            "episode: 1363   score: 2.0   memory length: 287990   epsilon: 0.6277778200080806    steps: 199    lr: 0.0001     evaluation reward: 4.3\n",
            "episode: 1364   score: 5.0   memory length: 288316   epsilon: 0.6271323400080946    steps: 326    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1365   score: 4.0   memory length: 288576   epsilon: 0.6266175400081058    steps: 260    lr: 0.0001     evaluation reward: 4.34\n",
            "episode: 1366   score: 8.0   memory length: 289032   epsilon: 0.6257146600081254    steps: 456    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1367   score: 0.0   memory length: 289156   epsilon: 0.6254691400081307    steps: 124    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1368   score: 3.0   memory length: 289383   epsilon: 0.6250196800081405    steps: 227    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1369   score: 4.0   memory length: 289628   epsilon: 0.624534580008151    steps: 245    lr: 0.0001     evaluation reward: 4.36\n",
            "episode: 1370   score: 5.0   memory length: 289995   epsilon: 0.6238079200081668    steps: 367    lr: 0.0001     evaluation reward: 4.36\n",
            "episode: 1371   score: 4.0   memory length: 290272   epsilon: 0.6232594600081787    steps: 277    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1372   score: 6.0   memory length: 290599   epsilon: 0.6226120000081927    steps: 327    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1373   score: 3.0   memory length: 290814   epsilon: 0.622186300008202    steps: 215    lr: 0.0001     evaluation reward: 4.38\n",
            "episode: 1374   score: 5.0   memory length: 291125   epsilon: 0.6215705200082153    steps: 311    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1375   score: 3.0   memory length: 291371   epsilon: 0.6210834400082259    steps: 246    lr: 0.0001     evaluation reward: 4.33\n",
            "episode: 1376   score: 3.0   memory length: 291599   epsilon: 0.6206320000082357    steps: 228    lr: 0.0001     evaluation reward: 4.33\n",
            "episode: 1377   score: 6.0   memory length: 291992   epsilon: 0.6198538600082526    steps: 393    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1378   score: 2.0   memory length: 292191   epsilon: 0.6194598400082612    steps: 199    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1379   score: 5.0   memory length: 292487   epsilon: 0.6188737600082739    steps: 296    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1380   score: 2.0   memory length: 292671   epsilon: 0.6185094400082818    steps: 184    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1381   score: 3.0   memory length: 292918   epsilon: 0.6180203800082924    steps: 247    lr: 0.0001     evaluation reward: 4.31\n",
            "episode: 1382   score: 4.0   memory length: 293194   epsilon: 0.6174739000083043    steps: 276    lr: 0.0001     evaluation reward: 4.34\n",
            "episode: 1383   score: 4.0   memory length: 293468   epsilon: 0.616931380008316    steps: 274    lr: 0.0001     evaluation reward: 4.33\n",
            "episode: 1384   score: 4.0   memory length: 293744   epsilon: 0.6163849000083279    steps: 276    lr: 0.0001     evaluation reward: 4.33\n",
            "episode: 1385   score: 5.0   memory length: 294051   epsilon: 0.6157770400083411    steps: 307    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1386   score: 2.0   memory length: 294250   epsilon: 0.6153830200083497    steps: 199    lr: 0.0001     evaluation reward: 4.33\n",
            "episode: 1387   score: 5.0   memory length: 294557   epsilon: 0.6147751600083629    steps: 307    lr: 0.0001     evaluation reward: 4.36\n",
            "episode: 1388   score: 4.0   memory length: 294802   epsilon: 0.6142900600083734    steps: 245    lr: 0.0001     evaluation reward: 4.39\n",
            "episode: 1389   score: 6.0   memory length: 295126   epsilon: 0.6136485400083873    steps: 324    lr: 0.0001     evaluation reward: 4.41\n",
            "episode: 1390   score: 1.0   memory length: 295295   epsilon: 0.6133139200083946    steps: 169    lr: 0.0001     evaluation reward: 4.41\n",
            "episode: 1391   score: 6.0   memory length: 295620   epsilon: 0.6126704200084085    steps: 325    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1392   score: 3.0   memory length: 295834   epsilon: 0.6122467000084177    steps: 214    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1393   score: 5.0   memory length: 296122   epsilon: 0.6116764600084301    steps: 288    lr: 0.0001     evaluation reward: 4.42\n",
            "episode: 1394   score: 4.0   memory length: 296378   epsilon: 0.6111695800084411    steps: 256    lr: 0.0001     evaluation reward: 4.43\n",
            "episode: 1395   score: 4.0   memory length: 296654   epsilon: 0.610623100008453    steps: 276    lr: 0.0001     evaluation reward: 4.44\n",
            "episode: 1396   score: 1.0   memory length: 296806   epsilon: 0.6103221400084595    steps: 152    lr: 0.0001     evaluation reward: 4.42\n",
            "episode: 1397   score: 2.0   memory length: 296988   epsilon: 0.6099617800084673    steps: 182    lr: 0.0001     evaluation reward: 4.41\n",
            "episode: 1398   score: 4.0   memory length: 297283   epsilon: 0.60937768000848    steps: 295    lr: 0.0001     evaluation reward: 4.39\n",
            "episode: 1399   score: 4.0   memory length: 297524   epsilon: 0.6089005000084904    steps: 241    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1400   score: 4.0   memory length: 297800   epsilon: 0.6083540200085022    steps: 276    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1401   score: 5.0   memory length: 298145   epsilon: 0.6076709200085171    steps: 345    lr: 0.0001     evaluation reward: 4.31\n",
            "episode: 1402   score: 6.0   memory length: 298515   epsilon: 0.606938320008533    steps: 370    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1403   score: 4.0   memory length: 298757   epsilon: 0.6064591600085434    steps: 242    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1404   score: 7.0   memory length: 299132   epsilon: 0.6057166600085595    steps: 375    lr: 0.0001     evaluation reward: 4.36\n",
            "episode: 1405   score: 5.0   memory length: 299441   epsilon: 0.6051048400085728    steps: 309    lr: 0.0001     evaluation reward: 4.34\n",
            "episode: 1406   score: 7.0   memory length: 299843   epsilon: 0.6043088800085901    steps: 402    lr: 0.0001     evaluation reward: 4.37\n",
            "episode: 1407   score: 10.0   memory length: 300229   epsilon: 0.6035446000086067    steps: 386    lr: 0.0001     evaluation reward: 4.41\n",
            "episode: 1408   score: 7.0   memory length: 300634   epsilon: 0.6027427000086241    steps: 405    lr: 0.0001     evaluation reward: 4.43\n",
            "episode: 1409   score: 8.0   memory length: 301087   epsilon: 0.6018457600086435    steps: 453    lr: 0.0001     evaluation reward: 4.47\n",
            "episode: 1410   score: 3.0   memory length: 301335   epsilon: 0.6013547200086542    steps: 248    lr: 0.0001     evaluation reward: 4.47\n",
            "episode: 1411   score: 5.0   memory length: 301684   epsilon: 0.6006637000086692    steps: 349    lr: 0.0001     evaluation reward: 4.5\n",
            "episode: 1412   score: 4.0   memory length: 301962   epsilon: 0.6001132600086811    steps: 278    lr: 0.0001     evaluation reward: 4.52\n",
            "episode: 1413   score: 2.0   memory length: 302181   epsilon: 0.5996796400086906    steps: 219    lr: 0.0001     evaluation reward: 4.49\n",
            "episode: 1414   score: 0.0   memory length: 302305   epsilon: 0.5994341200086959    steps: 124    lr: 0.0001     evaluation reward: 4.44\n",
            "episode: 1415   score: 2.0   memory length: 302487   epsilon: 0.5990737600087037    steps: 182    lr: 0.0001     evaluation reward: 4.39\n",
            "episode: 1416   score: 1.0   memory length: 302641   epsilon: 0.5987688400087103    steps: 154    lr: 0.0001     evaluation reward: 4.35\n",
            "episode: 1417   score: 4.0   memory length: 302899   epsilon: 0.5982580000087214    steps: 258    lr: 0.0001     evaluation reward: 4.34\n",
            "episode: 1418   score: 4.0   memory length: 303160   epsilon: 0.5977412200087326    steps: 261    lr: 0.0001     evaluation reward: 4.3\n",
            "episode: 1419   score: 4.0   memory length: 303418   epsilon: 0.5972303800087437    steps: 258    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1420   score: 1.0   memory length: 303569   epsilon: 0.5969314000087502    steps: 151    lr: 0.0001     evaluation reward: 4.28\n",
            "episode: 1421   score: 3.0   memory length: 303800   epsilon: 0.5964740200087602    steps: 231    lr: 0.0001     evaluation reward: 4.24\n",
            "episode: 1422   score: 3.0   memory length: 304014   epsilon: 0.5960503000087694    steps: 214    lr: 0.0001     evaluation reward: 4.23\n",
            "episode: 1423   score: 4.0   memory length: 304313   epsilon: 0.5954582800087822    steps: 299    lr: 0.0001     evaluation reward: 4.24\n",
            "episode: 1424   score: 12.0   memory length: 304788   epsilon: 0.5945177800088026    steps: 475    lr: 0.0001     evaluation reward: 4.32\n",
            "episode: 1425   score: 5.0   memory length: 305080   epsilon: 0.5939396200088152    steps: 292    lr: 0.0001     evaluation reward: 4.34\n",
            "episode: 1426   score: 7.0   memory length: 305486   epsilon: 0.5931357400088326    steps: 406    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1427   score: 11.0   memory length: 306038   epsilon: 0.5920427800088564    steps: 552    lr: 0.0001     evaluation reward: 4.45\n",
            "episode: 1428   score: 6.0   memory length: 306432   epsilon: 0.5912626600088733    steps: 394    lr: 0.0001     evaluation reward: 4.46\n",
            "episode: 1429   score: 2.0   memory length: 306631   epsilon: 0.5908686400088818    steps: 199    lr: 0.0001     evaluation reward: 4.44\n",
            "episode: 1430   score: 3.0   memory length: 306878   epsilon: 0.5903795800088925    steps: 247    lr: 0.0001     evaluation reward: 4.4\n",
            "episode: 1431   score: 5.0   memory length: 307212   epsilon: 0.5897182600089068    steps: 334    lr: 0.0001     evaluation reward: 4.43\n",
            "episode: 1432   score: 8.0   memory length: 307604   epsilon: 0.5889421000089237    steps: 392    lr: 0.0001     evaluation reward: 4.46\n",
            "episode: 1433   score: 3.0   memory length: 307868   epsilon: 0.588419380008935    steps: 264    lr: 0.0001     evaluation reward: 4.48\n",
            "episode: 1434   score: 3.0   memory length: 308118   epsilon: 0.5879243800089458    steps: 250    lr: 0.0001     evaluation reward: 4.44\n",
            "episode: 1435   score: 6.0   memory length: 308474   epsilon: 0.5872195000089611    steps: 356    lr: 0.0001     evaluation reward: 4.48\n",
            "episode: 1436   score: 5.0   memory length: 308781   epsilon: 0.5866116400089743    steps: 307    lr: 0.0001     evaluation reward: 4.51\n",
            "episode: 1437   score: 6.0   memory length: 309119   epsilon: 0.5859424000089888    steps: 338    lr: 0.0001     evaluation reward: 4.55\n",
            "episode: 1438   score: 6.0   memory length: 309474   epsilon: 0.585239500009004    steps: 355    lr: 0.0001     evaluation reward: 4.57\n",
            "episode: 1439   score: 8.0   memory length: 309899   epsilon: 0.5843980000090223    steps: 425    lr: 0.0001     evaluation reward: 4.59\n",
            "episode: 1440   score: 9.0   memory length: 310399   epsilon: 0.5834080000090438    steps: 500    lr: 0.0001     evaluation reward: 4.65\n",
            "episode: 1441   score: 6.0   memory length: 310743   epsilon: 0.5827268800090586    steps: 344    lr: 0.0001     evaluation reward: 4.66\n",
            "episode: 1442   score: 3.0   memory length: 310973   epsilon: 0.5822714800090685    steps: 230    lr: 0.0001     evaluation reward: 4.66\n",
            "episode: 1443   score: 8.0   memory length: 311403   epsilon: 0.581420080009087    steps: 430    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1444   score: 9.0   memory length: 311834   epsilon: 0.5805667000091055    steps: 431    lr: 0.0001     evaluation reward: 4.78\n",
            "episode: 1445   score: 3.0   memory length: 312065   epsilon: 0.5801093200091154    steps: 231    lr: 0.0001     evaluation reward: 4.75\n",
            "episode: 1446   score: 2.0   memory length: 312264   epsilon: 0.579715300009124    steps: 199    lr: 0.0001     evaluation reward: 4.72\n",
            "episode: 1447   score: 7.0   memory length: 312687   epsilon: 0.5788777600091422    steps: 423    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1448   score: 8.0   memory length: 313127   epsilon: 0.5780065600091611    steps: 440    lr: 0.0001     evaluation reward: 4.7\n",
            "episode: 1449   score: 7.0   memory length: 313534   epsilon: 0.5772007000091786    steps: 407    lr: 0.0001     evaluation reward: 4.71\n",
            "episode: 1450   score: 6.0   memory length: 313870   epsilon: 0.576535420009193    steps: 336    lr: 0.0001     evaluation reward: 4.73\n",
            "episode: 1451   score: 0.0   memory length: 313994   epsilon: 0.5762899000091983    steps: 124    lr: 0.0001     evaluation reward: 4.7\n",
            "episode: 1452   score: 1.0   memory length: 314145   epsilon: 0.5759909200092048    steps: 151    lr: 0.0001     evaluation reward: 4.62\n",
            "episode: 1453   score: 3.0   memory length: 314376   epsilon: 0.5755335400092148    steps: 231    lr: 0.0001     evaluation reward: 4.58\n",
            "episode: 1454   score: 5.0   memory length: 314665   epsilon: 0.5749613200092272    steps: 289    lr: 0.0001     evaluation reward: 4.59\n",
            "episode: 1455   score: 5.0   memory length: 314975   epsilon: 0.5743475200092405    steps: 310    lr: 0.0001     evaluation reward: 4.62\n",
            "episode: 1456   score: 4.0   memory length: 315236   epsilon: 0.5738307400092517    steps: 261    lr: 0.0001     evaluation reward: 4.61\n",
            "episode: 1457   score: 4.0   memory length: 315534   epsilon: 0.5732407000092645    steps: 298    lr: 0.0001     evaluation reward: 4.58\n",
            "episode: 1458   score: 7.0   memory length: 315918   epsilon: 0.572480380009281    steps: 384    lr: 0.0001     evaluation reward: 4.58\n",
            "episode: 1459   score: 4.0   memory length: 316215   epsilon: 0.5718923200092938    steps: 297    lr: 0.0001     evaluation reward: 4.55\n",
            "episode: 1460   score: 6.0   memory length: 316558   epsilon: 0.5712131800093085    steps: 343    lr: 0.0001     evaluation reward: 4.59\n",
            "episode: 1461   score: 9.0   memory length: 316976   epsilon: 0.5703855400093265    steps: 418    lr: 0.0001     evaluation reward: 4.61\n",
            "episode: 1462   score: 4.0   memory length: 317254   epsilon: 0.5698351000093385    steps: 278    lr: 0.0001     evaluation reward: 4.6\n",
            "episode: 1463   score: 5.0   memory length: 317580   epsilon: 0.5691896200093525    steps: 326    lr: 0.0001     evaluation reward: 4.63\n",
            "episode: 1464   score: 8.0   memory length: 318025   epsilon: 0.5683085200093716    steps: 445    lr: 0.0001     evaluation reward: 4.66\n",
            "episode: 1465   score: 3.0   memory length: 318258   epsilon: 0.5678471800093816    steps: 233    lr: 0.0001     evaluation reward: 4.65\n",
            "episode: 1466   score: 8.0   memory length: 318711   epsilon: 0.5669502400094011    steps: 453    lr: 0.0001     evaluation reward: 4.65\n",
            "episode: 1467   score: 8.0   memory length: 319149   epsilon: 0.5660830000094199    steps: 438    lr: 0.0001     evaluation reward: 4.73\n",
            "episode: 1468   score: 7.0   memory length: 319531   epsilon: 0.5653266400094363    steps: 382    lr: 0.0001     evaluation reward: 4.77\n",
            "episode: 1469   score: 5.0   memory length: 319857   epsilon: 0.5646811600094503    steps: 326    lr: 0.0001     evaluation reward: 4.78\n",
            "episode: 1470   score: 2.0   memory length: 320056   epsilon: 0.5642871400094589    steps: 199    lr: 0.0001     evaluation reward: 4.75\n",
            "episode: 1471   score: 3.0   memory length: 320270   epsilon: 0.5638634200094681    steps: 214    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1472   score: 4.0   memory length: 320547   epsilon: 0.56331496000948    steps: 277    lr: 0.0001     evaluation reward: 4.72\n",
            "episode: 1473   score: 7.0   memory length: 320990   epsilon: 0.562437820009499    steps: 443    lr: 0.0001     evaluation reward: 4.76\n",
            "episode: 1474   score: 8.0   memory length: 321426   epsilon: 0.5615745400095178    steps: 436    lr: 0.0001     evaluation reward: 4.79\n",
            "episode: 1475   score: 2.0   memory length: 321625   epsilon: 0.5611805200095263    steps: 199    lr: 0.0001     evaluation reward: 4.78\n",
            "episode: 1476   score: 0.0   memory length: 321749   epsilon: 0.5609350000095317    steps: 124    lr: 0.0001     evaluation reward: 4.75\n",
            "episode: 1477   score: 5.0   memory length: 322076   epsilon: 0.5602875400095457    steps: 327    lr: 0.0001     evaluation reward: 4.74\n",
            "episode: 1478   score: 6.0   memory length: 322454   epsilon: 0.559539100009562    steps: 378    lr: 0.0001     evaluation reward: 4.78\n",
            "episode: 1479   score: 6.0   memory length: 322850   epsilon: 0.558755020009579    steps: 396    lr: 0.0001     evaluation reward: 4.79\n",
            "episode: 1480   score: 5.0   memory length: 323178   epsilon: 0.5581055800095931    steps: 328    lr: 0.0001     evaluation reward: 4.82\n",
            "episode: 1481   score: 5.0   memory length: 323506   epsilon: 0.5574561400096072    steps: 328    lr: 0.0001     evaluation reward: 4.84\n",
            "episode: 1482   score: 5.0   memory length: 323809   epsilon: 0.5568562000096202    steps: 303    lr: 0.0001     evaluation reward: 4.85\n",
            "episode: 1483   score: 5.0   memory length: 324137   epsilon: 0.5562067600096343    steps: 328    lr: 0.0001     evaluation reward: 4.86\n",
            "episode: 1484   score: 3.0   memory length: 324351   epsilon: 0.5557830400096435    steps: 214    lr: 0.0001     evaluation reward: 4.85\n",
            "episode: 1485   score: 8.0   memory length: 324775   epsilon: 0.5549435200096617    steps: 424    lr: 0.0001     evaluation reward: 4.88\n",
            "episode: 1486   score: 4.0   memory length: 325036   epsilon: 0.554426740009673    steps: 261    lr: 0.0001     evaluation reward: 4.9\n",
            "episode: 1487   score: 3.0   memory length: 325247   epsilon: 0.554008960009682    steps: 211    lr: 0.0001     evaluation reward: 4.88\n",
            "episode: 1488   score: 7.0   memory length: 325621   epsilon: 0.5532684400096981    steps: 374    lr: 0.0001     evaluation reward: 4.91\n",
            "episode: 1489   score: 6.0   memory length: 325940   epsilon: 0.5526368200097118    steps: 319    lr: 0.0001     evaluation reward: 4.91\n",
            "episode: 1490   score: 10.0   memory length: 326474   epsilon: 0.5515795000097348    steps: 534    lr: 0.0001     evaluation reward: 5.0\n",
            "episode: 1491   score: 3.0   memory length: 326703   epsilon: 0.5511260800097446    steps: 229    lr: 0.0001     evaluation reward: 4.97\n",
            "episode: 1492   score: 6.0   memory length: 327056   epsilon: 0.5504271400097598    steps: 353    lr: 0.0001     evaluation reward: 5.0\n",
            "episode: 1493   score: 2.0   memory length: 327276   epsilon: 0.5499915400097692    steps: 220    lr: 0.0001     evaluation reward: 4.97\n",
            "episode: 1494   score: 15.0   memory length: 327838   epsilon: 0.5488787800097934    steps: 562    lr: 0.0001     evaluation reward: 5.08\n",
            "episode: 1495   score: 4.0   memory length: 328097   epsilon: 0.5483659600098045    steps: 259    lr: 0.0001     evaluation reward: 5.08\n",
            "episode: 1496   score: 4.0   memory length: 328375   epsilon: 0.5478155200098165    steps: 278    lr: 0.0001     evaluation reward: 5.11\n",
            "episode: 1497   score: 6.0   memory length: 328735   epsilon: 0.547102720009832    steps: 360    lr: 0.0001     evaluation reward: 5.15\n",
            "episode: 1498   score: 6.0   memory length: 329111   epsilon: 0.5463582400098481    steps: 376    lr: 0.0001     evaluation reward: 5.17\n",
            "episode: 1499   score: 9.0   memory length: 329455   epsilon: 0.5456771200098629    steps: 344    lr: 0.0001     evaluation reward: 5.22\n",
            "episode: 1500   score: 3.0   memory length: 329668   epsilon: 0.5452553800098721    steps: 213    lr: 0.0001     evaluation reward: 5.21\n",
            "episode: 1501   score: 5.0   memory length: 329978   epsilon: 0.5446415800098854    steps: 310    lr: 0.0001     evaluation reward: 5.21\n",
            "episode: 1502   score: 6.0   memory length: 330331   epsilon: 0.5439426400099006    steps: 353    lr: 0.0001     evaluation reward: 5.21\n",
            "episode: 1503   score: 11.0   memory length: 330714   epsilon: 0.543184300009917    steps: 383    lr: 0.0001     evaluation reward: 5.28\n",
            "episode: 1504   score: 3.0   memory length: 330944   epsilon: 0.5427289000099269    steps: 230    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1505   score: 5.0   memory length: 331235   epsilon: 0.5421527200099394    steps: 291    lr: 0.0001     evaluation reward: 5.24\n",
            "episode: 1506   score: 4.0   memory length: 331513   epsilon: 0.5416022800099514    steps: 278    lr: 0.0001     evaluation reward: 5.21\n",
            "episode: 1507   score: 7.0   memory length: 331888   epsilon: 0.5408597800099675    steps: 375    lr: 0.0001     evaluation reward: 5.18\n",
            "episode: 1508   score: 8.0   memory length: 332339   epsilon: 0.5399668000099869    steps: 451    lr: 0.0001     evaluation reward: 5.19\n",
            "episode: 1509   score: 3.0   memory length: 332565   epsilon: 0.5395193200099966    steps: 226    lr: 0.0001     evaluation reward: 5.14\n",
            "episode: 1510   score: 5.0   memory length: 332893   epsilon: 0.5388698800100107    steps: 328    lr: 0.0001     evaluation reward: 5.16\n",
            "episode: 1511   score: 6.0   memory length: 333232   epsilon: 0.5381986600100253    steps: 339    lr: 0.0001     evaluation reward: 5.17\n",
            "episode: 1512   score: 5.0   memory length: 333554   epsilon: 0.5375611000100391    steps: 322    lr: 0.0001     evaluation reward: 5.18\n",
            "episode: 1513   score: 9.0   memory length: 334077   epsilon: 0.5365255600100616    steps: 523    lr: 0.0001     evaluation reward: 5.25\n",
            "episode: 1514   score: 3.0   memory length: 334305   epsilon: 0.5360741200100714    steps: 228    lr: 0.0001     evaluation reward: 5.28\n",
            "episode: 1515   score: 3.0   memory length: 334535   epsilon: 0.5356187200100813    steps: 230    lr: 0.0001     evaluation reward: 5.29\n",
            "episode: 1516   score: 5.0   memory length: 334855   epsilon: 0.534985120010095    steps: 320    lr: 0.0001     evaluation reward: 5.33\n",
            "episode: 1517   score: 6.0   memory length: 335175   epsilon: 0.5343515200101088    steps: 320    lr: 0.0001     evaluation reward: 5.35\n",
            "episode: 1518   score: 8.0   memory length: 335610   epsilon: 0.5334902200101275    steps: 435    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1519   score: 3.0   memory length: 335822   epsilon: 0.5330704600101366    steps: 212    lr: 0.0001     evaluation reward: 5.38\n",
            "episode: 1520   score: 3.0   memory length: 336071   epsilon: 0.5325774400101473    steps: 249    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1521   score: 5.0   memory length: 336378   epsilon: 0.5319695800101605    steps: 307    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1522   score: 5.0   memory length: 336703   epsilon: 0.5313260800101745    steps: 325    lr: 0.0001     evaluation reward: 5.44\n",
            "episode: 1523   score: 10.0   memory length: 337185   epsilon: 0.5303717200101952    steps: 482    lr: 0.0001     evaluation reward: 5.5\n",
            "episode: 1524   score: 4.0   memory length: 337430   epsilon: 0.5298866200102057    steps: 245    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1525   score: 3.0   memory length: 337660   epsilon: 0.5294312200102156    steps: 230    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1526   score: 6.0   memory length: 337979   epsilon: 0.5287996000102293    steps: 319    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1527   score: 13.0   memory length: 338604   epsilon: 0.5275621000102562    steps: 625    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1528   score: 4.0   memory length: 338887   epsilon: 0.5270017600102683    steps: 283    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1529   score: 5.0   memory length: 339204   epsilon: 0.526374100010282    steps: 317    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1530   score: 5.0   memory length: 339549   epsilon: 0.5256910000102968    steps: 345    lr: 0.0001     evaluation reward: 5.44\n",
            "episode: 1531   score: 3.0   memory length: 339777   epsilon: 0.5252395600103066    steps: 228    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1532   score: 4.0   memory length: 340058   epsilon: 0.5246831800103187    steps: 281    lr: 0.0001     evaluation reward: 5.38\n",
            "episode: 1533   score: 5.0   memory length: 340334   epsilon: 0.5241367000103305    steps: 276    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1534   score: 4.0   memory length: 340595   epsilon: 0.5236199200103417    steps: 261    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1535   score: 3.0   memory length: 340829   epsilon: 0.5231566000103518    steps: 234    lr: 0.0001     evaluation reward: 5.38\n",
            "episode: 1536   score: 6.0   memory length: 341187   epsilon: 0.5224477600103672    steps: 358    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1537   score: 7.0   memory length: 341562   epsilon: 0.5217052600103833    steps: 375    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1538   score: 5.0   memory length: 341891   epsilon: 0.5210538400103975    steps: 329    lr: 0.0001     evaluation reward: 5.39\n",
            "episode: 1539   score: 6.0   memory length: 342247   epsilon: 0.5203489600104128    steps: 356    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1540   score: 6.0   memory length: 342592   epsilon: 0.5196658600104276    steps: 345    lr: 0.0001     evaluation reward: 5.34\n",
            "episode: 1541   score: 4.0   memory length: 342856   epsilon: 0.5191431400104389    steps: 264    lr: 0.0001     evaluation reward: 5.32\n",
            "episode: 1542   score: 7.0   memory length: 343222   epsilon: 0.5184184600104547    steps: 366    lr: 0.0001     evaluation reward: 5.36\n",
            "episode: 1543   score: 4.0   memory length: 343499   epsilon: 0.5178700000104666    steps: 277    lr: 0.0001     evaluation reward: 5.32\n",
            "episode: 1544   score: 5.0   memory length: 343803   epsilon: 0.5172680800104796    steps: 304    lr: 0.0001     evaluation reward: 5.28\n",
            "episode: 1545   score: 6.0   memory length: 344134   epsilon: 0.5166127000104939    steps: 331    lr: 0.0001     evaluation reward: 5.31\n",
            "episode: 1546   score: 8.0   memory length: 344586   epsilon: 0.5157177400105133    steps: 452    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1547   score: 5.0   memory length: 344892   epsilon: 0.5151118600105264    steps: 306    lr: 0.0001     evaluation reward: 5.35\n",
            "episode: 1548   score: 7.0   memory length: 345305   epsilon: 0.5142941200105442    steps: 413    lr: 0.0001     evaluation reward: 5.34\n",
            "episode: 1549   score: 7.0   memory length: 345699   epsilon: 0.5135140000105611    steps: 394    lr: 0.0001     evaluation reward: 5.34\n",
            "episode: 1550   score: 7.0   memory length: 346069   epsilon: 0.512781400010577    steps: 370    lr: 0.0001     evaluation reward: 5.35\n",
            "episode: 1551   score: 6.0   memory length: 346424   epsilon: 0.5120785000105923    steps: 355    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1552   score: 3.0   memory length: 346651   epsilon: 0.511629040010602    steps: 227    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1553   score: 3.0   memory length: 346863   epsilon: 0.5112092800106112    steps: 212    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1554   score: 5.0   memory length: 347150   epsilon: 0.5106410200106235    steps: 287    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1555   score: 5.0   memory length: 347474   epsilon: 0.5099995000106374    steps: 324    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1556   score: 9.0   memory length: 347930   epsilon: 0.509096620010657    steps: 456    lr: 0.0001     evaluation reward: 5.48\n",
            "episode: 1557   score: 4.0   memory length: 348189   epsilon: 0.5085838000106682    steps: 259    lr: 0.0001     evaluation reward: 5.48\n",
            "episode: 1558   score: 7.0   memory length: 348580   epsilon: 0.507809620010685    steps: 391    lr: 0.0001     evaluation reward: 5.48\n",
            "episode: 1559   score: 2.0   memory length: 348779   epsilon: 0.5074156000106935    steps: 199    lr: 0.0001     evaluation reward: 5.46\n",
            "episode: 1560   score: 3.0   memory length: 348989   epsilon: 0.5069998000107026    steps: 210    lr: 0.0001     evaluation reward: 5.43\n",
            "episode: 1561   score: 3.0   memory length: 349218   epsilon: 0.5065463800107124    steps: 229    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1562   score: 4.0   memory length: 349477   epsilon: 0.5060335600107235    steps: 259    lr: 0.0001     evaluation reward: 5.37\n",
            "episode: 1563   score: 4.0   memory length: 349759   epsilon: 0.5054752000107356    steps: 282    lr: 0.0001     evaluation reward: 5.36\n",
            "episode: 1564   score: 5.0   memory length: 350046   epsilon: 0.504906940010748    steps: 287    lr: 0.0001     evaluation reward: 5.33\n",
            "episode: 1565   score: 8.0   memory length: 350463   epsilon: 0.5040812800107659    steps: 417    lr: 0.0001     evaluation reward: 5.38\n",
            "episode: 1566   score: 4.0   memory length: 350758   epsilon: 0.5034971800107786    steps: 295    lr: 0.0001     evaluation reward: 5.34\n",
            "episode: 1567   score: 6.0   memory length: 351135   epsilon: 0.5027507200107948    steps: 377    lr: 0.0001     evaluation reward: 5.32\n",
            "episode: 1568   score: 6.0   memory length: 351478   epsilon: 0.5020715800108095    steps: 343    lr: 0.0001     evaluation reward: 5.31\n",
            "episode: 1569   score: 6.0   memory length: 351831   epsilon: 0.5013726400108247    steps: 353    lr: 0.0001     evaluation reward: 5.32\n",
            "episode: 1570   score: 11.0   memory length: 352383   epsilon: 0.5002796800108484    steps: 552    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1571   score: 4.0   memory length: 352665   epsilon: 0.49972132001085273    steps: 282    lr: 0.0001     evaluation reward: 5.42\n",
            "episode: 1572   score: 6.0   memory length: 353008   epsilon: 0.49904218001084844    steps: 343    lr: 0.0001     evaluation reward: 5.44\n",
            "episode: 1573   score: 4.0   memory length: 353250   epsilon: 0.4985630200108454    steps: 242    lr: 0.0001     evaluation reward: 5.41\n",
            "episode: 1574   score: 7.0   memory length: 353627   epsilon: 0.4978165600108407    steps: 377    lr: 0.0001     evaluation reward: 5.4\n",
            "episode: 1575   score: 8.0   memory length: 354034   epsilon: 0.4970107000108356    steps: 407    lr: 0.0001     evaluation reward: 5.46\n",
            "episode: 1576   score: 9.0   memory length: 354527   epsilon: 0.4960345600108294    steps: 493    lr: 0.0001     evaluation reward: 5.55\n",
            "episode: 1577   score: 7.0   memory length: 354925   epsilon: 0.4952465200108244    steps: 398    lr: 0.0001     evaluation reward: 5.57\n",
            "episode: 1578   score: 5.0   memory length: 355221   epsilon: 0.4946604400108207    steps: 296    lr: 0.0001     evaluation reward: 5.56\n",
            "episode: 1579   score: 5.0   memory length: 355550   epsilon: 0.4940090200108166    steps: 329    lr: 0.0001     evaluation reward: 5.55\n",
            "episode: 1580   score: 6.0   memory length: 355890   epsilon: 0.49333582001081233    steps: 340    lr: 0.0001     evaluation reward: 5.56\n",
            "episode: 1581   score: 7.0   memory length: 356279   epsilon: 0.49256560001080746    steps: 389    lr: 0.0001     evaluation reward: 5.58\n",
            "episode: 1582   score: 7.0   memory length: 356671   epsilon: 0.49178944001080255    steps: 392    lr: 0.0001     evaluation reward: 5.6\n",
            "episode: 1583   score: 5.0   memory length: 356993   epsilon: 0.4911518800107985    steps: 322    lr: 0.0001     evaluation reward: 5.6\n",
            "episode: 1584   score: 5.0   memory length: 357320   epsilon: 0.4905044200107944    steps: 327    lr: 0.0001     evaluation reward: 5.62\n",
            "episode: 1585   score: 7.0   memory length: 357743   epsilon: 0.4896668800107891    steps: 423    lr: 0.0001     evaluation reward: 5.61\n",
            "episode: 1586   score: 6.0   memory length: 358083   epsilon: 0.48899368001078486    steps: 340    lr: 0.0001     evaluation reward: 5.63\n",
            "episode: 1587   score: 3.0   memory length: 358332   epsilon: 0.48850066001078174    steps: 249    lr: 0.0001     evaluation reward: 5.63\n",
            "episode: 1588   score: 6.0   memory length: 358705   epsilon: 0.48776212001077707    steps: 373    lr: 0.0001     evaluation reward: 5.62\n",
            "episode: 1589   score: 4.0   memory length: 358966   epsilon: 0.4872453400107738    steps: 261    lr: 0.0001     evaluation reward: 5.6\n",
            "episode: 1590   score: 3.0   memory length: 359199   epsilon: 0.4867840000107709    steps: 233    lr: 0.0001     evaluation reward: 5.53\n",
            "episode: 1591   score: 7.0   memory length: 359586   epsilon: 0.48601774001076603    steps: 387    lr: 0.0001     evaluation reward: 5.57\n",
            "episode: 1592   score: 9.0   memory length: 359931   epsilon: 0.4853346400107617    steps: 345    lr: 0.0001     evaluation reward: 5.6\n",
            "episode: 1593   score: 9.0   memory length: 360405   epsilon: 0.48439612001075577    steps: 474    lr: 0.0001     evaluation reward: 5.67\n",
            "episode: 1594   score: 2.0   memory length: 360604   epsilon: 0.4840021000107533    steps: 199    lr: 0.0001     evaluation reward: 5.54\n",
            "episode: 1595   score: 7.0   memory length: 361028   epsilon: 0.48316258001074797    steps: 424    lr: 0.0001     evaluation reward: 5.57\n",
            "episode: 1596   score: 7.0   memory length: 361439   epsilon: 0.4823488000107428    steps: 411    lr: 0.0001     evaluation reward: 5.6\n",
            "episode: 1597   score: 7.0   memory length: 361843   epsilon: 0.48154888001073776    steps: 404    lr: 0.0001     evaluation reward: 5.61\n",
            "episode: 1598   score: 4.0   memory length: 362086   epsilon: 0.4810677400107347    steps: 243    lr: 0.0001     evaluation reward: 5.59\n",
            "episode: 1599   score: 5.0   memory length: 362381   epsilon: 0.480483640010731    steps: 295    lr: 0.0001     evaluation reward: 5.55\n",
            "episode: 1600   score: 7.0   memory length: 362769   epsilon: 0.47971540001072616    steps: 388    lr: 0.0001     evaluation reward: 5.59\n",
            "episode: 1601   score: 7.0   memory length: 363163   epsilon: 0.4789352800107212    steps: 394    lr: 0.0001     evaluation reward: 5.61\n",
            "episode: 1602   score: 7.0   memory length: 363553   epsilon: 0.47816308001071633    steps: 390    lr: 0.0001     evaluation reward: 5.62\n",
            "episode: 1603   score: 7.0   memory length: 363957   epsilon: 0.4773631600107113    steps: 404    lr: 0.0001     evaluation reward: 5.58\n",
            "episode: 1604   score: 7.0   memory length: 364343   epsilon: 0.47659888001070644    steps: 386    lr: 0.0001     evaluation reward: 5.62\n",
            "episode: 1605   score: 5.0   memory length: 364651   epsilon: 0.4759890400107026    steps: 308    lr: 0.0001     evaluation reward: 5.62\n",
            "episode: 1606   score: 5.0   memory length: 364983   epsilon: 0.4753316800106984    steps: 332    lr: 0.0001     evaluation reward: 5.63\n",
            "episode: 1607   score: 5.0   memory length: 365329   epsilon: 0.4746466000106941    steps: 346    lr: 0.0001     evaluation reward: 5.61\n",
            "episode: 1608   score: 8.0   memory length: 365800   epsilon: 0.4737140200106882    steps: 471    lr: 0.0001     evaluation reward: 5.61\n",
            "episode: 1609   score: 5.0   memory length: 366117   epsilon: 0.4730863600106842    steps: 317    lr: 0.0001     evaluation reward: 5.63\n",
            "episode: 1610   score: 8.0   memory length: 366586   epsilon: 0.47215774001067834    steps: 469    lr: 0.0001     evaluation reward: 5.66\n",
            "episode: 1611   score: 5.0   memory length: 366896   epsilon: 0.47154394001067446    steps: 310    lr: 0.0001     evaluation reward: 5.65\n",
            "episode: 1612   score: 8.0   memory length: 367330   epsilon: 0.470684620010669    steps: 434    lr: 0.0001     evaluation reward: 5.68\n",
            "episode: 1613   score: 0.0   memory length: 367453   epsilon: 0.4704410800106675    steps: 123    lr: 0.0001     evaluation reward: 5.59\n",
            "episode: 1614   score: 9.0   memory length: 367884   epsilon: 0.4695877000106621    steps: 431    lr: 0.0001     evaluation reward: 5.65\n",
            "episode: 1615   score: 7.0   memory length: 368276   epsilon: 0.46881154001065717    steps: 392    lr: 0.0001     evaluation reward: 5.69\n",
            "episode: 1616   score: 6.0   memory length: 368620   epsilon: 0.46813042001065286    steps: 344    lr: 0.0001     evaluation reward: 5.7\n",
            "episode: 1617   score: 4.0   memory length: 368898   epsilon: 0.4675799800106494    steps: 278    lr: 0.0001     evaluation reward: 5.68\n",
            "episode: 1618   score: 10.0   memory length: 369461   epsilon: 0.4664652400106423    steps: 563    lr: 0.0001     evaluation reward: 5.7\n",
            "episode: 1619   score: 5.0   memory length: 369733   epsilon: 0.4659266800106389    steps: 272    lr: 0.0001     evaluation reward: 5.72\n",
            "episode: 1620   score: 4.0   memory length: 369975   epsilon: 0.4654475200106359    steps: 242    lr: 0.0001     evaluation reward: 5.73\n",
            "episode: 1621   score: 6.0   memory length: 370297   epsilon: 0.46480996001063185    steps: 322    lr: 0.0001     evaluation reward: 5.74\n",
            "episode: 1622   score: 9.0   memory length: 370777   epsilon: 0.46385956001062584    steps: 480    lr: 0.0001     evaluation reward: 5.78\n",
            "episode: 1623   score: 7.0   memory length: 371183   epsilon: 0.46305568001062075    steps: 406    lr: 0.0001     evaluation reward: 5.75\n",
            "episode: 1624   score: 3.0   memory length: 371409   epsilon: 0.4626082000106179    steps: 226    lr: 0.0001     evaluation reward: 5.74\n",
            "episode: 1625   score: 8.0   memory length: 371790   epsilon: 0.46185382001061315    steps: 381    lr: 0.0001     evaluation reward: 5.79\n",
            "episode: 1626   score: 8.0   memory length: 372218   epsilon: 0.4610063800106078    steps: 428    lr: 0.0001     evaluation reward: 5.81\n",
            "episode: 1627   score: 8.0   memory length: 372675   epsilon: 0.46010152001060206    steps: 457    lr: 0.0001     evaluation reward: 5.76\n",
            "episode: 1628   score: 6.0   memory length: 373000   epsilon: 0.459458020010598    steps: 325    lr: 0.0001     evaluation reward: 5.78\n",
            "episode: 1629   score: 7.0   memory length: 373423   epsilon: 0.4586204800105927    steps: 423    lr: 0.0001     evaluation reward: 5.8\n",
            "episode: 1630   score: 5.0   memory length: 373746   epsilon: 0.45798094001058864    steps: 323    lr: 0.0001     evaluation reward: 5.8\n",
            "episode: 1631   score: 5.0   memory length: 374040   epsilon: 0.45739882001058496    steps: 294    lr: 0.0001     evaluation reward: 5.82\n",
            "episode: 1632   score: 7.0   memory length: 374415   epsilon: 0.45665632001058026    steps: 375    lr: 0.0001     evaluation reward: 5.85\n",
            "episode: 1633   score: 5.0   memory length: 374726   epsilon: 0.45604054001057637    steps: 311    lr: 0.0001     evaluation reward: 5.85\n",
            "episode: 1634   score: 7.0   memory length: 375125   epsilon: 0.45525052001057137    steps: 399    lr: 0.0001     evaluation reward: 5.88\n",
            "episode: 1635   score: 16.0   memory length: 375723   epsilon: 0.4540664800105639    steps: 598    lr: 0.0001     evaluation reward: 6.01\n",
            "episode: 1636   score: 11.0   memory length: 376264   epsilon: 0.4529953000105571    steps: 541    lr: 0.0001     evaluation reward: 6.06\n",
            "episode: 1637   score: 6.0   memory length: 376620   epsilon: 0.45229042001055264    steps: 356    lr: 0.0001     evaluation reward: 6.05\n",
            "episode: 1638   score: 6.0   memory length: 376979   epsilon: 0.45157960001054814    steps: 359    lr: 0.0001     evaluation reward: 6.06\n",
            "episode: 1639   score: 8.0   memory length: 377403   epsilon: 0.45074008001054283    steps: 424    lr: 0.0001     evaluation reward: 6.08\n",
            "episode: 1640   score: 8.0   memory length: 377847   epsilon: 0.44986096001053727    steps: 444    lr: 0.0001     evaluation reward: 6.1\n",
            "episode: 1641   score: 5.0   memory length: 378174   epsilon: 0.4492135000105332    steps: 327    lr: 0.0001     evaluation reward: 6.11\n",
            "episode: 1642   score: 7.0   memory length: 378560   epsilon: 0.44844922001052834    steps: 386    lr: 0.0001     evaluation reward: 6.11\n",
            "episode: 1643   score: 7.0   memory length: 378970   epsilon: 0.4476374200105232    steps: 410    lr: 0.0001     evaluation reward: 6.14\n",
            "episode: 1644   score: 9.0   memory length: 379405   epsilon: 0.44677612001051775    steps: 435    lr: 0.0001     evaluation reward: 6.18\n",
            "episode: 1645   score: 5.0   memory length: 379716   epsilon: 0.44616034001051386    steps: 311    lr: 0.0001     evaluation reward: 6.17\n",
            "episode: 1646   score: 5.0   memory length: 380045   epsilon: 0.44550892001050973    steps: 329    lr: 0.0001     evaluation reward: 6.14\n",
            "episode: 1647   score: 4.0   memory length: 380303   epsilon: 0.4449980800105065    steps: 258    lr: 0.0001     evaluation reward: 6.13\n",
            "episode: 1648   score: 5.0   memory length: 380612   epsilon: 0.44438626001050263    steps: 309    lr: 0.0001     evaluation reward: 6.11\n",
            "episode: 1649   score: 5.0   memory length: 380899   epsilon: 0.44381800001049904    steps: 287    lr: 0.0001     evaluation reward: 6.09\n",
            "episode: 1650   score: 3.0   memory length: 381146   epsilon: 0.44332894001049594    steps: 247    lr: 0.0001     evaluation reward: 6.05\n",
            "episode: 1651   score: 4.0   memory length: 381403   epsilon: 0.4428200800104927    steps: 257    lr: 0.0001     evaluation reward: 6.03\n",
            "episode: 1652   score: 6.0   memory length: 381722   epsilon: 0.4421884600104887    steps: 319    lr: 0.0001     evaluation reward: 6.06\n",
            "episode: 1653   score: 5.0   memory length: 382039   epsilon: 0.44156080001048476    steps: 317    lr: 0.0001     evaluation reward: 6.08\n",
            "episode: 1654   score: 8.0   memory length: 382488   epsilon: 0.44067178001047913    steps: 449    lr: 0.0001     evaluation reward: 6.11\n",
            "episode: 1655   score: 7.0   memory length: 382891   epsilon: 0.4398738400104741    steps: 403    lr: 0.0001     evaluation reward: 6.13\n",
            "episode: 1656   score: 7.0   memory length: 383231   epsilon: 0.4392006400104698    steps: 340    lr: 0.0001     evaluation reward: 6.11\n",
            "episode: 1657   score: 7.0   memory length: 383625   epsilon: 0.4384205200104649    steps: 394    lr: 0.0001     evaluation reward: 6.14\n",
            "episode: 1658   score: 6.0   memory length: 383947   epsilon: 0.43778296001046085    steps: 322    lr: 0.0001     evaluation reward: 6.13\n",
            "episode: 1659   score: 12.0   memory length: 384457   epsilon: 0.43677316001045446    steps: 510    lr: 0.0001     evaluation reward: 6.23\n",
            "episode: 1660   score: 5.0   memory length: 384766   epsilon: 0.4361613400104506    steps: 309    lr: 0.0001     evaluation reward: 6.25\n",
            "episode: 1661   score: 8.0   memory length: 385183   epsilon: 0.43533568001044537    steps: 417    lr: 0.0001     evaluation reward: 6.3\n",
            "episode: 1662   score: 6.0   memory length: 385521   epsilon: 0.43466644001044114    steps: 338    lr: 0.0001     evaluation reward: 6.32\n",
            "episode: 1663   score: 7.0   memory length: 385908   epsilon: 0.4339001800104363    steps: 387    lr: 0.0001     evaluation reward: 6.35\n",
            "episode: 1664   score: 7.0   memory length: 386298   epsilon: 0.4331279800104314    steps: 390    lr: 0.0001     evaluation reward: 6.37\n",
            "episode: 1665   score: 9.0   memory length: 386827   epsilon: 0.4320805600104248    steps: 529    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1666   score: 3.0   memory length: 387055   epsilon: 0.4316291200104219    steps: 228    lr: 0.0001     evaluation reward: 6.37\n",
            "episode: 1667   score: 8.0   memory length: 387452   epsilon: 0.43084306001041695    steps: 397    lr: 0.0001     evaluation reward: 6.39\n",
            "episode: 1668   score: 8.0   memory length: 387892   epsilon: 0.42997186001041143    steps: 440    lr: 0.0001     evaluation reward: 6.41\n",
            "episode: 1669   score: 13.0   memory length: 388385   epsilon: 0.42899572001040526    steps: 493    lr: 0.0001     evaluation reward: 6.48\n",
            "episode: 1670   score: 6.0   memory length: 388741   epsilon: 0.4282908400104008    steps: 356    lr: 0.0001     evaluation reward: 6.43\n",
            "episode: 1671   score: 9.0   memory length: 389227   epsilon: 0.4273285600103947    steps: 486    lr: 0.0001     evaluation reward: 6.48\n",
            "episode: 1672   score: 7.0   memory length: 389637   epsilon: 0.4265167600103896    steps: 410    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1673   score: 3.0   memory length: 389851   epsilon: 0.4260930400103869    steps: 214    lr: 0.0001     evaluation reward: 6.48\n",
            "episode: 1674   score: 5.0   memory length: 390178   epsilon: 0.4254455800103828    steps: 327    lr: 0.0001     evaluation reward: 6.46\n",
            "episode: 1675   score: 4.0   memory length: 390458   epsilon: 0.4248911800103793    steps: 280    lr: 0.0001     evaluation reward: 6.42\n",
            "episode: 1676   score: 3.0   memory length: 390669   epsilon: 0.42447340001037664    steps: 211    lr: 0.0001     evaluation reward: 6.36\n",
            "episode: 1677   score: 10.0   memory length: 391176   epsilon: 0.4234695400103703    steps: 507    lr: 0.0001     evaluation reward: 6.39\n",
            "episode: 1678   score: 9.0   memory length: 391648   epsilon: 0.4225349800103644    steps: 472    lr: 0.0001     evaluation reward: 6.43\n",
            "episode: 1679   score: 9.0   memory length: 392068   epsilon: 0.4217033800103591    steps: 420    lr: 0.0001     evaluation reward: 6.47\n",
            "episode: 1680   score: 6.0   memory length: 392442   epsilon: 0.42096286001035443    steps: 374    lr: 0.0001     evaluation reward: 6.47\n",
            "episode: 1681   score: 8.0   memory length: 392865   epsilon: 0.42012532001034913    steps: 423    lr: 0.0001     evaluation reward: 6.48\n",
            "episode: 1682   score: 1.0   memory length: 393017   epsilon: 0.41982436001034723    steps: 152    lr: 0.0001     evaluation reward: 6.42\n",
            "episode: 1683   score: 8.0   memory length: 393454   epsilon: 0.41895910001034176    steps: 437    lr: 0.0001     evaluation reward: 6.45\n",
            "episode: 1684   score: 8.0   memory length: 393860   epsilon: 0.41815522001033667    steps: 406    lr: 0.0001     evaluation reward: 6.48\n",
            "episode: 1685   score: 8.0   memory length: 394334   epsilon: 0.41721670001033073    steps: 474    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1686   score: 7.0   memory length: 394756   epsilon: 0.41638114001032545    steps: 422    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1687   score: 6.0   memory length: 395070   epsilon: 0.4157594200103215    steps: 314    lr: 0.0001     evaluation reward: 6.53\n",
            "episode: 1688   score: 5.0   memory length: 395362   epsilon: 0.41518126001031785    steps: 292    lr: 0.0001     evaluation reward: 6.52\n",
            "episode: 1689   score: 6.0   memory length: 395671   epsilon: 0.414569440010314    steps: 309    lr: 0.0001     evaluation reward: 6.54\n",
            "episode: 1690   score: 5.0   memory length: 395982   epsilon: 0.4139536600103101    steps: 311    lr: 0.0001     evaluation reward: 6.56\n",
            "episode: 1691   score: 9.0   memory length: 396436   epsilon: 0.4130547400103044    steps: 454    lr: 0.0001     evaluation reward: 6.58\n",
            "episode: 1692   score: 3.0   memory length: 396663   epsilon: 0.41260528001030156    steps: 227    lr: 0.0001     evaluation reward: 6.52\n",
            "episode: 1693   score: 6.0   memory length: 397046   epsilon: 0.41184694001029676    steps: 383    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1694   score: 7.0   memory length: 397409   epsilon: 0.4111282000102922    steps: 363    lr: 0.0001     evaluation reward: 6.54\n",
            "episode: 1695   score: 11.0   memory length: 397948   epsilon: 0.41006098001028546    steps: 539    lr: 0.0001     evaluation reward: 6.58\n",
            "episode: 1696   score: 11.0   memory length: 398481   epsilon: 0.4090056400102788    steps: 533    lr: 0.0001     evaluation reward: 6.62\n",
            "episode: 1697   score: 6.0   memory length: 398838   epsilon: 0.4082987800102743    steps: 357    lr: 0.0001     evaluation reward: 6.61\n",
            "episode: 1698   score: 10.0   memory length: 399400   epsilon: 0.40718602001026727    steps: 562    lr: 0.0001     evaluation reward: 6.67\n",
            "episode: 1699   score: 3.0   memory length: 399626   epsilon: 0.40673854001026444    steps: 226    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1700   score: 10.0   memory length: 400147   epsilon: 0.4057069600102579    steps: 521    lr: 0.0001     evaluation reward: 6.68\n",
            "episode: 1701   score: 8.0   memory length: 400561   epsilon: 0.4048872400102527    steps: 414    lr: 0.0001     evaluation reward: 6.69\n",
            "episode: 1702   score: 5.0   memory length: 400867   epsilon: 0.4042813600102489    steps: 306    lr: 0.0001     evaluation reward: 6.67\n",
            "episode: 1703   score: 9.0   memory length: 401325   epsilon: 0.40337452001024315    steps: 458    lr: 0.0001     evaluation reward: 6.69\n",
            "episode: 1704   score: 9.0   memory length: 401779   epsilon: 0.40247560001023747    steps: 454    lr: 0.0001     evaluation reward: 6.71\n",
            "episode: 1705   score: 4.0   memory length: 402023   epsilon: 0.4019924800102344    steps: 244    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1706   score: 10.0   memory length: 402493   epsilon: 0.4010618800102285    steps: 470    lr: 0.0001     evaluation reward: 6.75\n",
            "episode: 1707   score: 5.0   memory length: 402783   epsilon: 0.4004876800102249    steps: 290    lr: 0.0001     evaluation reward: 6.75\n",
            "episode: 1708   score: 3.0   memory length: 402993   epsilon: 0.40007188001022226    steps: 210    lr: 0.0001     evaluation reward: 6.7\n",
            "episode: 1709   score: 3.0   memory length: 403204   epsilon: 0.3996541000102196    steps: 211    lr: 0.0001     evaluation reward: 6.68\n",
            "episode: 1710   score: 4.0   memory length: 403483   epsilon: 0.3991016800102161    steps: 279    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1711   score: 6.0   memory length: 403840   epsilon: 0.39839482001021165    steps: 357    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1712   score: 7.0   memory length: 404249   epsilon: 0.3975850000102065    steps: 409    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1713   score: 4.0   memory length: 404526   epsilon: 0.39703654001020305    steps: 277    lr: 0.0001     evaluation reward: 6.68\n",
            "episode: 1714   score: 9.0   memory length: 404987   epsilon: 0.3961237600101973    steps: 461    lr: 0.0001     evaluation reward: 6.68\n",
            "episode: 1715   score: 4.0   memory length: 405267   epsilon: 0.39556936001019377    steps: 280    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1716   score: 5.0   memory length: 405558   epsilon: 0.3949931800101901    steps: 291    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1717   score: 5.0   memory length: 405857   epsilon: 0.3944011600101864    steps: 299    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1718   score: 3.0   memory length: 406086   epsilon: 0.3939477400101835    steps: 229    lr: 0.0001     evaluation reward: 6.58\n",
            "episode: 1719   score: 8.0   memory length: 406521   epsilon: 0.39308644001017806    steps: 435    lr: 0.0001     evaluation reward: 6.61\n",
            "episode: 1720   score: 7.0   memory length: 406894   epsilon: 0.3923479000101734    steps: 373    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1721   score: 5.0   memory length: 407202   epsilon: 0.39173806001016953    steps: 308    lr: 0.0001     evaluation reward: 6.63\n",
            "episode: 1722   score: 10.0   memory length: 407723   epsilon: 0.390706480010163    steps: 521    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1723   score: 6.0   memory length: 408062   epsilon: 0.39003526001015876    steps: 339    lr: 0.0001     evaluation reward: 6.63\n",
            "episode: 1724   score: 4.0   memory length: 408304   epsilon: 0.3895561000101557    steps: 242    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1725   score: 6.0   memory length: 408627   epsilon: 0.3889165600101517    steps: 323    lr: 0.0001     evaluation reward: 6.62\n",
            "episode: 1726   score: 6.0   memory length: 409010   epsilon: 0.3881582200101469    steps: 383    lr: 0.0001     evaluation reward: 6.6\n",
            "episode: 1727   score: 7.0   memory length: 409411   epsilon: 0.38736424001014186    steps: 401    lr: 0.0001     evaluation reward: 6.59\n",
            "episode: 1728   score: 6.0   memory length: 409790   epsilon: 0.3866138200101371    steps: 379    lr: 0.0001     evaluation reward: 6.59\n",
            "episode: 1729   score: 4.0   memory length: 410051   epsilon: 0.38609704001013384    steps: 261    lr: 0.0001     evaluation reward: 6.56\n",
            "episode: 1730   score: 4.0   memory length: 410294   epsilon: 0.3856159000101308    steps: 243    lr: 0.0001     evaluation reward: 6.55\n",
            "episode: 1731   score: 10.0   memory length: 410779   epsilon: 0.3846556000101247    steps: 485    lr: 0.0001     evaluation reward: 6.6\n",
            "episode: 1732   score: 6.0   memory length: 411124   epsilon: 0.3839725000101204    steps: 345    lr: 0.0001     evaluation reward: 6.59\n",
            "episode: 1733   score: 7.0   memory length: 411529   epsilon: 0.3831706000101153    steps: 405    lr: 0.0001     evaluation reward: 6.61\n",
            "episode: 1734   score: 10.0   memory length: 412055   epsilon: 0.38212912001010874    steps: 526    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1735   score: 5.0   memory length: 412361   epsilon: 0.3815232400101049    steps: 306    lr: 0.0001     evaluation reward: 6.53\n",
            "episode: 1736   score: 6.0   memory length: 412717   epsilon: 0.38081836001010044    steps: 356    lr: 0.0001     evaluation reward: 6.48\n",
            "episode: 1737   score: 8.0   memory length: 413105   epsilon: 0.3800501200100956    steps: 388    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1738   score: 6.0   memory length: 413440   epsilon: 0.3793868200100914    steps: 335    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1739   score: 7.0   memory length: 413850   epsilon: 0.37857502001008625    steps: 410    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1740   score: 9.0   memory length: 414279   epsilon: 0.3777256000100809    steps: 429    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1741   score: 6.0   memory length: 414616   epsilon: 0.37705834001007665    steps: 337    lr: 0.0001     evaluation reward: 6.51\n",
            "episode: 1742   score: 5.0   memory length: 414929   epsilon: 0.37643860001007273    steps: 313    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1743   score: 5.0   memory length: 415236   epsilon: 0.3758307400100689    steps: 307    lr: 0.0001     evaluation reward: 6.47\n",
            "episode: 1744   score: 6.0   memory length: 415591   epsilon: 0.37512784001006444    steps: 355    lr: 0.0001     evaluation reward: 6.44\n",
            "episode: 1745   score: 4.0   memory length: 415852   epsilon: 0.37461106001006117    steps: 261    lr: 0.0001     evaluation reward: 6.43\n",
            "episode: 1746   score: 9.0   memory length: 416311   epsilon: 0.3737022400100554    steps: 459    lr: 0.0001     evaluation reward: 6.47\n",
            "episode: 1747   score: 10.0   memory length: 416794   epsilon: 0.37274590001004937    steps: 483    lr: 0.0001     evaluation reward: 6.53\n",
            "episode: 1748   score: 7.0   memory length: 417191   epsilon: 0.3719598400100444    steps: 397    lr: 0.0001     evaluation reward: 6.55\n",
            "episode: 1749   score: 4.0   memory length: 417434   epsilon: 0.37147870001004135    steps: 243    lr: 0.0001     evaluation reward: 6.54\n",
            "episode: 1750   score: 4.0   memory length: 417735   epsilon: 0.3708827200100376    steps: 301    lr: 0.0001     evaluation reward: 6.55\n",
            "episode: 1751   score: 1.0   memory length: 417887   epsilon: 0.3705817600100357    steps: 152    lr: 0.0001     evaluation reward: 6.52\n",
            "episode: 1752   score: 7.0   memory length: 418292   epsilon: 0.3697798600100306    steps: 405    lr: 0.0001     evaluation reward: 6.53\n",
            "episode: 1753   score: 5.0   memory length: 418585   epsilon: 0.36919972001002693    steps: 293    lr: 0.0001     evaluation reward: 6.53\n",
            "episode: 1754   score: 6.0   memory length: 418935   epsilon: 0.36850672001002255    steps: 350    lr: 0.0001     evaluation reward: 6.51\n",
            "episode: 1755   score: 7.0   memory length: 419341   epsilon: 0.36770284001001746    steps: 406    lr: 0.0001     evaluation reward: 6.51\n",
            "episode: 1756   score: 6.0   memory length: 419700   epsilon: 0.36699202001001296    steps: 359    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1757   score: 6.0   memory length: 420059   epsilon: 0.36628120001000847    steps: 359    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1758   score: 6.0   memory length: 420434   epsilon: 0.36553870001000377    steps: 375    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1759   score: 7.0   memory length: 420798   epsilon: 0.3648179800099992    steps: 364    lr: 0.0001     evaluation reward: 6.44\n",
            "episode: 1760   score: 6.0   memory length: 421148   epsilon: 0.3641249800099948    steps: 350    lr: 0.0001     evaluation reward: 6.45\n",
            "episode: 1761   score: 6.0   memory length: 421493   epsilon: 0.3634418800099905    steps: 345    lr: 0.0001     evaluation reward: 6.43\n",
            "episode: 1762   score: 12.0   memory length: 422100   epsilon: 0.3622400200099829    steps: 607    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1763   score: 8.0   memory length: 422523   epsilon: 0.3614024800099776    steps: 423    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1764   score: 8.0   memory length: 422928   epsilon: 0.3606005800099725    steps: 405    lr: 0.0001     evaluation reward: 6.51\n",
            "episode: 1765   score: 8.0   memory length: 423370   epsilon: 0.359725420009967    steps: 442    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1766   score: 6.0   memory length: 423741   epsilon: 0.35899084000996234    steps: 371    lr: 0.0001     evaluation reward: 6.53\n",
            "episode: 1767   score: 8.0   memory length: 424181   epsilon: 0.35811964000995683    steps: 440    lr: 0.0001     evaluation reward: 6.53\n",
            "episode: 1768   score: 4.0   memory length: 424438   epsilon: 0.3576107800099536    steps: 257    lr: 0.0001     evaluation reward: 6.49\n",
            "episode: 1769   score: 6.0   memory length: 424793   epsilon: 0.35690788000994916    steps: 355    lr: 0.0001     evaluation reward: 6.42\n",
            "episode: 1770   score: 7.0   memory length: 425175   epsilon: 0.3561515200099444    steps: 382    lr: 0.0001     evaluation reward: 6.43\n",
            "episode: 1771   score: 4.0   memory length: 425433   epsilon: 0.35564068000994115    steps: 258    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1772   score: 7.0   memory length: 425820   epsilon: 0.3548744200099363    steps: 387    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1773   score: 5.0   memory length: 426127   epsilon: 0.35426656000993245    steps: 307    lr: 0.0001     evaluation reward: 6.4\n",
            "episode: 1774   score: 3.0   memory length: 426374   epsilon: 0.35377750000992936    steps: 247    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1775   score: 6.0   memory length: 426711   epsilon: 0.35311024000992514    steps: 337    lr: 0.0001     evaluation reward: 6.4\n",
            "episode: 1776   score: 8.0   memory length: 427109   epsilon: 0.35232220000992015    steps: 398    lr: 0.0001     evaluation reward: 6.45\n",
            "episode: 1777   score: 5.0   memory length: 427416   epsilon: 0.3517143400099163    steps: 307    lr: 0.0001     evaluation reward: 6.4\n",
            "episode: 1778   score: 9.0   memory length: 427852   epsilon: 0.35085106000991084    steps: 436    lr: 0.0001     evaluation reward: 6.4\n",
            "episode: 1779   score: 8.0   memory length: 428309   epsilon: 0.3499462000099051    steps: 457    lr: 0.0001     evaluation reward: 6.39\n",
            "episode: 1780   score: 5.0   memory length: 428599   epsilon: 0.3493720000099015    steps: 290    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1781   score: 6.0   memory length: 428955   epsilon: 0.348667120009897    steps: 356    lr: 0.0001     evaluation reward: 6.36\n",
            "episode: 1782   score: 5.0   memory length: 429282   epsilon: 0.34801966000989293    steps: 327    lr: 0.0001     evaluation reward: 6.4\n",
            "episode: 1783   score: 8.0   memory length: 429560   epsilon: 0.34746922000988945    steps: 278    lr: 0.0001     evaluation reward: 6.4\n",
            "episode: 1784   score: 5.0   memory length: 429852   epsilon: 0.3468910600098858    steps: 292    lr: 0.0001     evaluation reward: 6.37\n",
            "episode: 1785   score: 8.0   memory length: 430296   epsilon: 0.3460119400098802    steps: 444    lr: 0.0001     evaluation reward: 6.37\n",
            "episode: 1786   score: 7.0   memory length: 430684   epsilon: 0.34524370000987536    steps: 388    lr: 0.0001     evaluation reward: 6.37\n",
            "episode: 1787   score: 7.0   memory length: 431089   epsilon: 0.3444418000098703    steps: 405    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1788   score: 5.0   memory length: 431436   epsilon: 0.34375474000986594    steps: 347    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1789   score: 4.0   memory length: 431694   epsilon: 0.3432439000098627    steps: 258    lr: 0.0001     evaluation reward: 6.36\n",
            "episode: 1790   score: 6.0   memory length: 432031   epsilon: 0.3425766400098585    steps: 337    lr: 0.0001     evaluation reward: 6.37\n",
            "episode: 1791   score: 7.0   memory length: 432406   epsilon: 0.3418341400098538    steps: 375    lr: 0.0001     evaluation reward: 6.35\n",
            "episode: 1792   score: 9.0   memory length: 432843   epsilon: 0.3409688800098483    steps: 437    lr: 0.0001     evaluation reward: 6.41\n",
            "episode: 1793   score: 5.0   memory length: 433150   epsilon: 0.3403610200098445    steps: 307    lr: 0.0001     evaluation reward: 6.4\n",
            "episode: 1794   score: 13.0   memory length: 433691   epsilon: 0.3392898400098377    steps: 541    lr: 0.0001     evaluation reward: 6.46\n",
            "episode: 1795   score: 4.0   memory length: 433970   epsilon: 0.3387374200098342    steps: 279    lr: 0.0001     evaluation reward: 6.39\n",
            "episode: 1796   score: 6.0   memory length: 434325   epsilon: 0.33803452000982975    steps: 355    lr: 0.0001     evaluation reward: 6.34\n",
            "episode: 1797   score: 5.0   memory length: 434624   epsilon: 0.337442500009826    steps: 299    lr: 0.0001     evaluation reward: 6.33\n",
            "episode: 1798   score: 4.0   memory length: 434864   epsilon: 0.336967300009823    steps: 240    lr: 0.0001     evaluation reward: 6.27\n",
            "episode: 1799   score: 4.0   memory length: 435142   epsilon: 0.3364168600098195    steps: 278    lr: 0.0001     evaluation reward: 6.28\n",
            "episode: 1800   score: 7.0   memory length: 435549   epsilon: 0.3356110000098144    steps: 407    lr: 0.0001     evaluation reward: 6.25\n",
            "episode: 1801   score: 7.0   memory length: 435937   epsilon: 0.33484276000980956    steps: 388    lr: 0.0001     evaluation reward: 6.24\n",
            "episode: 1802   score: 3.0   memory length: 436167   epsilon: 0.3343873600098067    steps: 230    lr: 0.0001     evaluation reward: 6.22\n",
            "episode: 1803   score: 6.0   memory length: 436524   epsilon: 0.3336805000098022    steps: 357    lr: 0.0001     evaluation reward: 6.19\n",
            "episode: 1804   score: 4.0   memory length: 436767   epsilon: 0.33319936000979916    steps: 243    lr: 0.0001     evaluation reward: 6.14\n",
            "episode: 1805   score: 12.0   memory length: 437304   epsilon: 0.33213610000979243    steps: 537    lr: 0.0001     evaluation reward: 6.22\n",
            "episode: 1806   score: 4.0   memory length: 437581   epsilon: 0.33158764000978896    steps: 277    lr: 0.0001     evaluation reward: 6.16\n",
            "episode: 1807   score: 4.0   memory length: 437823   epsilon: 0.33110848000978593    steps: 242    lr: 0.0001     evaluation reward: 6.15\n",
            "episode: 1808   score: 10.0   memory length: 438305   epsilon: 0.3301541200097799    steps: 482    lr: 0.0001     evaluation reward: 6.22\n",
            "episode: 1809   score: 6.0   memory length: 438659   epsilon: 0.32945320000977546    steps: 354    lr: 0.0001     evaluation reward: 6.25\n",
            "episode: 1810   score: 7.0   memory length: 439064   epsilon: 0.3286513000097704    steps: 405    lr: 0.0001     evaluation reward: 6.28\n",
            "episode: 1811   score: 10.0   memory length: 439561   epsilon: 0.32766724000976416    steps: 497    lr: 0.0001     evaluation reward: 6.32\n",
            "episode: 1812   score: 6.0   memory length: 439883   epsilon: 0.3270296800097601    steps: 322    lr: 0.0001     evaluation reward: 6.31\n",
            "episode: 1813   score: 12.0   memory length: 440434   epsilon: 0.3259387000097532    steps: 551    lr: 0.0001     evaluation reward: 6.39\n",
            "episode: 1814   score: 8.0   memory length: 440872   epsilon: 0.32507146000974774    steps: 438    lr: 0.0001     evaluation reward: 6.38\n",
            "episode: 1815   score: 10.0   memory length: 441394   epsilon: 0.3240379000097412    steps: 522    lr: 0.0001     evaluation reward: 6.44\n",
            "episode: 1816   score: 6.0   memory length: 441777   epsilon: 0.3232795600097364    steps: 383    lr: 0.0001     evaluation reward: 6.45\n",
            "episode: 1817   score: 6.0   memory length: 442108   epsilon: 0.32262418000973225    steps: 331    lr: 0.0001     evaluation reward: 6.46\n",
            "episode: 1818   score: 7.0   memory length: 442516   epsilon: 0.32181634000972714    steps: 408    lr: 0.0001     evaluation reward: 6.5\n",
            "episode: 1819   score: 11.0   memory length: 443058   epsilon: 0.32074318000972035    steps: 542    lr: 0.0001     evaluation reward: 6.53\n",
            "episode: 1820   score: 19.0   memory length: 443662   epsilon: 0.3195472600097128    steps: 604    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1821   score: 5.0   memory length: 443952   epsilon: 0.31897306000970915    steps: 290    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1822   score: 9.0   memory length: 444388   epsilon: 0.3181097800097037    steps: 436    lr: 0.0001     evaluation reward: 6.64\n",
            "episode: 1823   score: 5.0   memory length: 444719   epsilon: 0.31745440000969954    steps: 331    lr: 0.0001     evaluation reward: 6.63\n",
            "episode: 1824   score: 6.0   memory length: 445059   epsilon: 0.3167812000096953    steps: 340    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1825   score: 6.0   memory length: 445418   epsilon: 0.3160703800096908    steps: 359    lr: 0.0001     evaluation reward: 6.65\n",
            "episode: 1826   score: 10.0   memory length: 445907   epsilon: 0.31510216000968466    steps: 489    lr: 0.0001     evaluation reward: 6.69\n",
            "episode: 1827   score: 6.0   memory length: 446245   epsilon: 0.31443292000968043    steps: 338    lr: 0.0001     evaluation reward: 6.68\n",
            "episode: 1828   score: 9.0   memory length: 446709   epsilon: 0.3135142000096746    steps: 464    lr: 0.0001     evaluation reward: 6.71\n",
            "episode: 1829   score: 7.0   memory length: 447081   epsilon: 0.31277764000966995    steps: 372    lr: 0.0001     evaluation reward: 6.74\n",
            "episode: 1830   score: 9.0   memory length: 447556   epsilon: 0.311837140009664    steps: 475    lr: 0.0001     evaluation reward: 6.79\n",
            "episode: 1831   score: 5.0   memory length: 447847   epsilon: 0.31126096000966036    steps: 291    lr: 0.0001     evaluation reward: 6.74\n",
            "episode: 1832   score: 8.0   memory length: 448251   epsilon: 0.3104610400096553    steps: 404    lr: 0.0001     evaluation reward: 6.76\n",
            "episode: 1833   score: 8.0   memory length: 448678   epsilon: 0.30961558000964995    steps: 427    lr: 0.0001     evaluation reward: 6.77\n",
            "episode: 1834   score: 9.0   memory length: 449091   epsilon: 0.3087978400096448    steps: 413    lr: 0.0001     evaluation reward: 6.76\n",
            "episode: 1835   score: 10.0   memory length: 449617   epsilon: 0.3077563600096382    steps: 526    lr: 0.0001     evaluation reward: 6.81\n",
            "episode: 1836   score: 9.0   memory length: 450071   epsilon: 0.3068574400096325    steps: 454    lr: 0.0001     evaluation reward: 6.84\n",
            "episode: 1837   score: 5.0   memory length: 450362   epsilon: 0.30628126000962885    steps: 291    lr: 0.0001     evaluation reward: 6.81\n",
            "episode: 1838   score: 10.0   memory length: 450823   epsilon: 0.3053684800096231    steps: 461    lr: 0.0001     evaluation reward: 6.85\n",
            "episode: 1839   score: 16.0   memory length: 451435   epsilon: 0.3041567200096154    steps: 612    lr: 0.0001     evaluation reward: 6.94\n",
            "episode: 1840   score: 9.0   memory length: 451927   epsilon: 0.30318256000960925    steps: 492    lr: 0.0001     evaluation reward: 6.94\n",
            "episode: 1841   score: 13.0   memory length: 452498   epsilon: 0.3020519800096021    steps: 571    lr: 0.0001     evaluation reward: 7.01\n",
            "episode: 1842   score: 10.0   memory length: 453017   epsilon: 0.3010243600095956    steps: 519    lr: 0.0001     evaluation reward: 7.06\n",
            "episode: 1843   score: 4.0   memory length: 453260   epsilon: 0.30054322000959255    steps: 243    lr: 0.0001     evaluation reward: 7.05\n",
            "episode: 1844   score: 8.0   memory length: 453713   epsilon: 0.2996462800095869    steps: 453    lr: 0.0001     evaluation reward: 7.07\n",
            "episode: 1845   score: 7.0   memory length: 454100   epsilon: 0.298880020009582    steps: 387    lr: 0.0001     evaluation reward: 7.1\n",
            "episode: 1846   score: 11.0   memory length: 454610   epsilon: 0.29787022000957564    steps: 510    lr: 0.0001     evaluation reward: 7.12\n",
            "episode: 1847   score: 9.0   memory length: 455089   epsilon: 0.29692180000956964    steps: 479    lr: 0.0001     evaluation reward: 7.11\n",
            "episode: 1848   score: 5.0   memory length: 455398   epsilon: 0.29630998000956577    steps: 309    lr: 0.0001     evaluation reward: 7.09\n",
            "episode: 1849   score: 5.0   memory length: 455727   epsilon: 0.29565856000956164    steps: 329    lr: 0.0001     evaluation reward: 7.1\n",
            "episode: 1850   score: 9.0   memory length: 456179   epsilon: 0.294763600009556    steps: 452    lr: 0.0001     evaluation reward: 7.15\n",
            "episode: 1851   score: 6.0   memory length: 456553   epsilon: 0.2940230800095513    steps: 374    lr: 0.0001     evaluation reward: 7.2\n",
            "episode: 1852   score: 6.0   memory length: 456893   epsilon: 0.29334988000954704    steps: 340    lr: 0.0001     evaluation reward: 7.19\n",
            "episode: 1853   score: 15.0   memory length: 457464   epsilon: 0.2922193000095399    steps: 571    lr: 0.0001     evaluation reward: 7.29\n",
            "episode: 1854   score: 7.0   memory length: 457888   epsilon: 0.2913797800095346    steps: 424    lr: 0.0001     evaluation reward: 7.3\n",
            "episode: 1855   score: 5.0   memory length: 458219   epsilon: 0.2907244000095304    steps: 331    lr: 0.0001     evaluation reward: 7.28\n",
            "episode: 1856   score: 5.0   memory length: 458511   epsilon: 0.29014624000952677    steps: 292    lr: 0.0001     evaluation reward: 7.27\n",
            "episode: 1857   score: 14.0   memory length: 459118   epsilon: 0.28894438000951916    steps: 607    lr: 0.0001     evaluation reward: 7.35\n",
            "episode: 1858   score: 6.0   memory length: 459453   epsilon: 0.28828108000951497    steps: 335    lr: 0.0001     evaluation reward: 7.35\n",
            "episode: 1859   score: 6.0   memory length: 459782   epsilon: 0.28762966000951085    steps: 329    lr: 0.0001     evaluation reward: 7.34\n",
            "episode: 1860   score: 13.0   memory length: 460451   epsilon: 0.28630504000950247    steps: 669    lr: 0.0001     evaluation reward: 7.41\n",
            "episode: 1861   score: 10.0   memory length: 460942   epsilon: 0.2853328600094963    steps: 491    lr: 0.0001     evaluation reward: 7.45\n",
            "episode: 1862   score: 10.0   memory length: 461488   epsilon: 0.2842517800094895    steps: 546    lr: 0.0001     evaluation reward: 7.43\n",
            "episode: 1863   score: 8.0   memory length: 461931   epsilon: 0.2833746400094839    steps: 443    lr: 0.0001     evaluation reward: 7.43\n",
            "episode: 1864   score: 6.0   memory length: 462286   epsilon: 0.2826717400094795    steps: 355    lr: 0.0001     evaluation reward: 7.41\n",
            "episode: 1865   score: 8.0   memory length: 462716   epsilon: 0.2818203400094741    steps: 430    lr: 0.0001     evaluation reward: 7.41\n",
            "episode: 1866   score: 6.0   memory length: 463091   epsilon: 0.2810778400094694    steps: 375    lr: 0.0001     evaluation reward: 7.41\n",
            "episode: 1867   score: 5.0   memory length: 463387   epsilon: 0.2804917600094657    steps: 296    lr: 0.0001     evaluation reward: 7.38\n",
            "episode: 1868   score: 6.0   memory length: 463745   epsilon: 0.2797829200094612    steps: 358    lr: 0.0001     evaluation reward: 7.4\n",
            "episode: 1869   score: 11.0   memory length: 464269   epsilon: 0.27874540000945464    steps: 524    lr: 0.0001     evaluation reward: 7.45\n",
            "episode: 1870   score: 5.0   memory length: 464558   epsilon: 0.278173180009451    steps: 289    lr: 0.0001     evaluation reward: 7.43\n",
            "episode: 1871   score: 7.0   memory length: 464939   epsilon: 0.27741880000944624    steps: 381    lr: 0.0001     evaluation reward: 7.46\n",
            "episode: 1872   score: 7.0   memory length: 465341   epsilon: 0.2766228400094412    steps: 402    lr: 0.0001     evaluation reward: 7.46\n",
            "episode: 1873   score: 12.0   memory length: 465897   epsilon: 0.27552196000943424    steps: 556    lr: 0.0001     evaluation reward: 7.53\n",
            "episode: 1874   score: 6.0   memory length: 466219   epsilon: 0.2748844000094302    steps: 322    lr: 0.0001     evaluation reward: 7.56\n",
            "episode: 1875   score: 7.0   memory length: 466594   epsilon: 0.2741419000094255    steps: 375    lr: 0.0001     evaluation reward: 7.57\n",
            "episode: 1876   score: 8.0   memory length: 467031   epsilon: 0.27327664000942004    steps: 437    lr: 0.0001     evaluation reward: 7.57\n",
            "episode: 1877   score: 6.0   memory length: 467406   epsilon: 0.27253414000941534    steps: 375    lr: 0.0001     evaluation reward: 7.58\n",
            "episode: 1878   score: 5.0   memory length: 467718   epsilon: 0.27191638000941143    steps: 312    lr: 0.0001     evaluation reward: 7.54\n",
            "episode: 1879   score: 9.0   memory length: 468028   epsilon: 0.27130258000940755    steps: 310    lr: 0.0001     evaluation reward: 7.55\n",
            "episode: 1880   score: 9.0   memory length: 468392   epsilon: 0.270581860009403    steps: 364    lr: 0.0001     evaluation reward: 7.59\n",
            "episode: 1881   score: 10.0   memory length: 468854   epsilon: 0.2696671000093972    steps: 462    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 1882   score: 5.0   memory length: 469161   epsilon: 0.26905924000939335    steps: 307    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 1883   score: 5.0   memory length: 469490   epsilon: 0.26840782000938923    steps: 329    lr: 0.0001     evaluation reward: 7.6\n",
            "episode: 1884   score: 8.0   memory length: 469887   epsilon: 0.26762176000938426    steps: 397    lr: 0.0001     evaluation reward: 7.63\n",
            "episode: 1885   score: 6.0   memory length: 470246   epsilon: 0.26691094000937976    steps: 359    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 1886   score: 7.0   memory length: 470649   epsilon: 0.2661130000093747    steps: 403    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 1887   score: 7.0   memory length: 470987   epsilon: 0.2654437600093705    steps: 338    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 1888   score: 5.0   memory length: 471315   epsilon: 0.26479432000936637    steps: 328    lr: 0.0001     evaluation reward: 7.61\n",
            "episode: 1889   score: 12.0   memory length: 471760   epsilon: 0.2639132200093608    steps: 445    lr: 0.0001     evaluation reward: 7.69\n",
            "episode: 1890   score: 10.0   memory length: 472303   epsilon: 0.262838080009354    steps: 543    lr: 0.0001     evaluation reward: 7.73\n",
            "episode: 1891   score: 11.0   memory length: 472874   epsilon: 0.26170750000934684    steps: 571    lr: 0.0001     evaluation reward: 7.77\n",
            "episode: 1892   score: 8.0   memory length: 473275   epsilon: 0.2609135200093418    steps: 401    lr: 0.0001     evaluation reward: 7.76\n",
            "episode: 1893   score: 5.0   memory length: 473603   epsilon: 0.2602640800093377    steps: 328    lr: 0.0001     evaluation reward: 7.76\n",
            "episode: 1894   score: 7.0   memory length: 473957   epsilon: 0.25956316000933327    steps: 354    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 1895   score: 4.0   memory length: 474256   epsilon: 0.2589711400093295    steps: 299    lr: 0.0001     evaluation reward: 7.7\n",
            "episode: 1896   score: 4.0   memory length: 474518   epsilon: 0.25845238000932624    steps: 262    lr: 0.0001     evaluation reward: 7.68\n",
            "episode: 1897   score: 8.0   memory length: 474946   epsilon: 0.2576049400093209    steps: 428    lr: 0.0001     evaluation reward: 7.71\n",
            "episode: 1898   score: 8.0   memory length: 475365   epsilon: 0.25677532000931563    steps: 419    lr: 0.0001     evaluation reward: 7.75\n",
            "episode: 1899   score: 8.0   memory length: 475775   epsilon: 0.2559635200093105    steps: 410    lr: 0.0001     evaluation reward: 7.79\n",
            "episode: 1900   score: 13.0   memory length: 476274   epsilon: 0.25497550000930425    steps: 499    lr: 0.0001     evaluation reward: 7.85\n",
            "episode: 1901   score: 11.0   memory length: 476825   epsilon: 0.25388452000929734    steps: 551    lr: 0.0001     evaluation reward: 7.89\n",
            "episode: 1902   score: 5.0   memory length: 477149   epsilon: 0.2532430000092933    steps: 324    lr: 0.0001     evaluation reward: 7.91\n",
            "episode: 1903   score: 9.0   memory length: 477605   epsilon: 0.25234012000928757    steps: 456    lr: 0.0001     evaluation reward: 7.94\n",
            "episode: 1904   score: 8.0   memory length: 478013   epsilon: 0.25153228000928246    steps: 408    lr: 0.0001     evaluation reward: 7.98\n",
            "episode: 1905   score: 7.0   memory length: 478420   epsilon: 0.25072642000927736    steps: 407    lr: 0.0001     evaluation reward: 7.93\n",
            "episode: 1906   score: 11.0   memory length: 478934   epsilon: 0.24970870000927092    steps: 514    lr: 0.0001     evaluation reward: 8.0\n",
            "episode: 1907   score: 7.0   memory length: 479327   epsilon: 0.248930560009266    steps: 393    lr: 0.0001     evaluation reward: 8.03\n",
            "episode: 1908   score: 10.0   memory length: 479829   epsilon: 0.2479366000092597    steps: 502    lr: 0.0001     evaluation reward: 8.03\n",
            "episode: 1909   score: 9.0   memory length: 480302   epsilon: 0.24700006000925379    steps: 473    lr: 0.0001     evaluation reward: 8.06\n",
            "episode: 1910   score: 13.0   memory length: 480790   epsilon: 0.24603382000924767    steps: 488    lr: 0.0001     evaluation reward: 8.12\n",
            "episode: 1911   score: 9.0   memory length: 481238   epsilon: 0.24514678000924206    steps: 448    lr: 0.0001     evaluation reward: 8.11\n",
            "episode: 1912   score: 6.0   memory length: 481568   epsilon: 0.24449338000923793    steps: 330    lr: 0.0001     evaluation reward: 8.11\n",
            "episode: 1913   score: 3.0   memory length: 481797   epsilon: 0.24403996000923506    steps: 229    lr: 0.0001     evaluation reward: 8.02\n",
            "episode: 1914   score: 5.0   memory length: 482122   epsilon: 0.24339646000923099    steps: 325    lr: 0.0001     evaluation reward: 7.99\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-93c5cd84dad1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Start training after random sample generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Update the target network only for Double DQN only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-70fb2ca04b55>\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fe06b66ed4ea>\u001b[0m in \u001b[0;36msample_mini_batch\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHISTORY_SIZE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAHHCAYAAABdm0mZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFX0lEQVR4nO3deXgUVb7/8U8nIZ2QFUICCYQdQQRxQbjsKCAio+g4LshoQNQRcUQdHWHmp6ijBjfU8SoyMwp4XcAF0OuCgoqAAoosigsCssq+ZGFJyHJ+f+R2k046SSd0p7qr36/n6Yd09enqb6VC6pNzTlU5jDFGAAAANhNhdQEAAACBQMgBAAC2RMgBAAC2RMgBAAC2RMgBAAC2RMgBAAC2RMgBAAC2RMgBAAC2RMgBAAC2RMgBQsgDDzwgh8NRr5+5detWORwOzZw5s14/F6fO4XDogQcesLoMwDKEHCBAZs6cKYfDUeVjxYoVVpcYtirum6ioKDVv3lyjR4/Wb7/9ZnV5APwkyuoCALt76KGH1KZNm0rL27dvX+t1/b//9/80ceJEf5QFndw3BQUFWrFihWbOnKlly5Zp/fr1iomJsbo8AKeIkAME2LBhw9S9e3e/rCsqKkpRUfy39Zfy++bGG29UkyZN9Nhjj+m9997TVVddZXF1NTt69Kji4uKsLgMIWgxXARZzzXl58skn9fTTT6tVq1aKjY3VgAEDtH79eo+23ubkLFy4UH379lVycrLi4+PVsWNH/e1vf/Nos2/fPo0dO1ZNmzZVTEyMunXrplmzZlWqJScnR6NHj1ZSUpKSk5OVlZWlnJwcr3X//PPP+sMf/qDGjRsrJiZG3bt313vvvefRpqioSA8++KA6dOigmJgYpaSkqG/fvlq4cGGV349Vq1bJ4XB4re/jjz+Ww+HQ+++/L0nKz8/XHXfcodatW8vpdCotLU1DhgzR6tWrq1x/dfr16ydJ2rx5c622NScnR5GRkfrnP//pXnbgwAFFREQoJSVFxhj38nHjxqlZs2bu50uXLtWVV16pli1byul0KjMzU3feeaeOHz/uUcPo0aMVHx+vzZs36+KLL1ZCQoJGjRolSSosLNSdd96p1NRUJSQk6NJLL9XOnTvr9D0A7IQ/CYEAy83N1YEDBzyWORwOpaSkeCx75ZVXlJ+fr/Hjx6ugoEDPPvusLrjgAn3//fdq2rSp13X/8MMP+t3vfqczzzxTDz30kJxOpzZt2qQvv/zS3eb48eMaOHCgNm3apNtuu01t2rTRW2+9pdGjRysnJ0cTJkyQJBljNGLECC1btky33HKLTj/9dM2bN09ZWVleP7dPnz5q3ry5Jk6cqLi4OL355pu67LLL9M477+jyyy+XVBbKsrOzdeONN6pHjx7Ky8vTqlWrtHr1ag0ZMsTrNnXv3l1t27bVm2++Wemz58yZo0aNGmno0KGSpFtuuUVvv/22brvtNnXu3FkHDx7UsmXL9NNPP+mcc86pbrd4tXXrVklSo0aNarWtycnJ6tKli5YsWaLbb79dkrRs2TI5HA4dOnRIP/74o8444wxJZaHGFaYk6a233tKxY8c0btw4paSk6Ouvv9Zzzz2nnTt36q233vKor7i4WEOHDlXfvn315JNPqmHDhpLKeqFeffVVXXvtterdu7c+++wzDR8+vNbbD9iOARAQM2bMMJK8PpxOp7vdli1bjCQTGxtrdu7c6V6+cuVKI8nceeed7mWTJ0825f/bPv3000aS2b9/f5V1PPPMM0aSefXVV93LTpw4YXr16mXi4+NNXl6eMcaY+fPnG0nm8ccfd7crLi42/fr1M5LMjBkz3MsHDRpkunbtagoKCtzLSktLTe/evU2HDh3cy7p162aGDx/u67fMbdKkSaZBgwbm0KFD7mWFhYUmOTnZ3HDDDe5lSUlJZvz48bVev2vfLFq0yOzfv9/s2LHDvP322yY1NdU4nU6zY8cOd1tft3X8+PGmadOm7ud33XWX6d+/v0lLSzPTpk0zxhhz8OBB43A4zLPPPutud+zYsUr1ZWdnG4fDYbZt2+ZelpWVZSSZiRMnerRdu3atkWRuvfVWj+XXXnutkWQmT55cy+8OYB8MVwEB9vzzz2vhwoUej48++qhSu8suu0zNmzd3P+/Ro4d69uypDz/8sMp1JycnS5LeffddlZaWem3z4YcfqlmzZho5cqR7WYMGDXT77bfryJEj+uKLL9ztoqKiNG7cOHe7yMhI/fnPf/ZY36FDh/TZZ5/pqquuUn5+vg4cOKADBw7o4MGDGjp0qDZu3Og+Qyk5OVk//PCDNm7cWMN3ydPVV1+toqIizZ07173sk08+UU5Ojq6++mqP7V+5cqV27dpVq/W7DB48WKmpqcrMzNQf/vAHxcXF6b333lOLFi1qva39+vXT3r17tWHDBkllPTb9+/dXv379tHTpUkllvTvGGI+enNjYWPfXR48e1YEDB9S7d28ZY7RmzZpKNZffP5LcPx+uHiSXO+64o07fE8BOCDlAgPXo0UODBw/2eJx//vmV2nXo0KHSstNOO809hOLN1VdfrT59+ujGG29U06ZNdc011+jNN9/0CDzbtm1Thw4dFBHh+d/99NNPd7/u+jc9PV3x8fEe7Tp27OjxfNOmTTLG6L777lNqaqrHY/LkyZLK5gBJZWcv5eTk6LTTTlPXrl11zz336Lvvvqtye1y6deumTp06ac6cOe5lc+bMUZMmTXTBBRe4lz3++ONav369MjMz1aNHDz3wwAP69ddfa1y/iyuAvv3227r44ot14MABOZ3OOm2rK7gsXbpUR48e1Zo1a9SvXz/179/fHXKWLl2qxMREdevWzf0Z27dv1+jRo9W4cWPFx8crNTVVAwYMkFQ21FleVFSUO4C5bNu2TREREWrXrp3H8or7DQhHzMkBQlhsbKyWLFmizz//XB988IEWLFigOXPm6IILLtAnn3yiyMhIv3+mK0Ddfffd7rkxFblOj+/fv782b96sd999V5988on+85//6Omnn9aLL76oG2+8sdrPufrqq/XII4/owIEDSkhI0HvvvaeRI0d6nF121VVXqV+/fpo3b54++eQTPfHEE3rsscc0d+5cDRs2rMZt6dGjh/vsqssuu0x9+/bVtddeqw0bNig+Pr5W25qRkaE2bdpoyZIlat26tYwx6tWrl1JTUzVhwgRt27ZNS5cuVe/evd2Bs6SkREOGDNGhQ4d07733qlOnToqLi9Nvv/2m0aNHV+qdczqdlcIqgKoRcoAg4W1I55dfflHr1q2rfV9ERIQGDRqkQYMGaerUqXr00Uf197//XZ9//rkGDx6sVq1a6bvvvlNpaanHAfLnn3+WJLVq1cr976effqojR4549Oa4hl9c2rZtK6lsyGvw4ME1blfjxo01ZswYjRkzRkeOHFH//v31wAMP+BRyHnzwQb3zzjtq2rSp8vLydM0111Rql56erltvvVW33nqr9u3bp3POOUePPPKITyGnvMjISGVnZ+v888/Xf//3f2vixIm13tZ+/fppyZIlatOmjc466ywlJCSoW7duSkpK0oIFC7R69Wo9+OCD7vbff/+9fvnlF82aNUvXX3+9e3l1Z59V1KpVK5WWlmrz5s0evTcV9xsQjviTAAgS8+fP97ja7tdff62VK1dWe7A+dOhQpWVnnXWWpLLTiiXp4osv1p49ezyGfoqLi/Xcc88pPj7ePTRy8cUXq7i4WNOmTXO3Kykp0XPPPeex/rS0NA0cOFDTp0/X7t27K33+/v373V8fPHjQ47X4+Hi1b9/eXVt1Tj/9dHXt2lVz5szRnDlzlJ6erv79+3vUVnE4Jy0tTRkZGT6t35uBAweqR48eeuaZZ1RQUFCrbZXKQs7WrVs1Z84c9/BVRESEevfuralTp6qoqMhjPo6rp82UO8XcGKNnn33W55pdPx/lT1+XpGeeecbndQB2RU8OEGAfffSRu9ekvN69e7t7CqSyYY++fftq3LhxKiws1DPPPKOUlBT99a9/rXLdDz30kJYsWaLhw4erVatW2rdvn1544QW1aNFCffv2lSTdfPPNmj59ukaPHq1vv/1WrVu31ttvv60vv/xSzzzzjBISEiRJl1xyifr06aOJEydq69at6ty5s+bOnVspSEhlc1n69u2rrl276qabblLbtm21d+9eLV++XDt37tS6deskSZ07d9bAgQN17rnnqnHjxlq1apX7lG9fXH311br//vsVExOjsWPHevRE5efnq0WLFvrDH/6gbt26KT4+XosWLdI333yjp556yqf1e3PPPffoyiuv1MyZM3XLLbf4vK3SyXk5GzZs0KOPPupe3r9/f3300UdyOp0677zz3Ms7deqkdu3a6e6779Zvv/2mxMREvfPOOzp8+LDP9Z511lkaOXKkXnjhBeXm5qp379769NNPtWnTpjp/DwDbsPDMLsDWqjuFXOVOyXadQv7EE0+Yp556ymRmZhqn02n69etn1q1b57HOiqeQf/rpp2bEiBEmIyPDREdHm4yMDDNy5Ejzyy+/eLxv7969ZsyYMaZJkyYmOjradO3a1eOUcJeDBw+a6667ziQmJpqkpCRz3XXXmTVr1lQ6hdwYYzZv3myuv/5606xZM9OgQQPTvHlz87vf/c68/fbb7jYPP/yw6dGjh0lOTjaxsbGmU6dO5pFHHjEnTpzw6Xu4ceNG9/dr2bJlHq8VFhaae+65x3Tr1s0kJCSYuLg4061bN/PCCy/UuF7Xvvnmm28qvVZSUmLatWtn2rVrZ4qLi33eVpe0tDQjyezdu9e9bNmyZUaS6devX6X2P/74oxk8eLCJj483TZo0MTfddJNZt25dpe95VlaWiYuL87o9x48fN7fffrtJSUkxcXFx5pJLLjE7duzgFHKEPYcx5fpJAdS7rVu3qk2bNnriiSd09913W10OANgGc3IAAIAtEXIAAIAtEXIAAIAtMScHAADYEj05AADAlgg5AADAlkL6YoClpaXatWuXEhIS5HA4rC4HAAD4wBij/Px8ZWRkBPR+bCEdcnbt2qXMzEyrywAAAHWwY8cOtWjRImDrD+mQ47oc/Y4dO5SYmGhxNQAAwBd5eXnKzMx0H8cDJaRDjmuIKjExkZADAECICfRUEyYeAwAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAD7m50tKl0uHDVldyagg5AADAw1VXSf37S40bS7/+anU1dUfIAQAAHr744uTXM2daVsYpszTklJSU6L777lObNm0UGxurdu3a6R//+IeMMVaWBQBAWCsuPvl1TIx1dZyqKCs//LHHHtO0adM0a9YsnXHGGVq1apXGjBmjpKQk3X777VaWBgBA2CopOfn10aPW1XGqLA05X331lUaMGKHhw4dLklq3bq033nhDX3/9tZVlAQAQthYt8nx+8KA1dfiDpcNVvXv31qeffqpffvlFkrRu3TotW7ZMw4YN89q+sLBQeXl5Hg8AAOA/WVmez3fvtqYOf7C0J2fixInKy8tTp06dFBkZqZKSEj3yyCMaNWqU1/bZ2dl68MEH67lKAADCx65dVlfgP5b25Lz55pt67bXX9Prrr2v16tWaNWuWnnzySc2aNctr+0mTJik3N9f92LFjRz1XDABA+Bg8WBowwOoq6s7Snpx77rlHEydO1DXXXCNJ6tq1q7Zt26bs7GxlVewvk+R0OuV0Ouu7TAAAwsLcuZ7PFy60pg5/sbQn59ixY4qI8CwhMjJSpaWlFlUEAED4+tOfrK7Avyztybnkkkv0yCOPqGXLljrjjDO0Zs0aTZ06VTfccIOVZQEAEJYOHDj59RtvWFeHvziMhVfey8/P13333ad58+Zp3759ysjI0MiRI3X//fcrOjq6xvfn5eUpKSlJubm5SkxMrIeKAQCwL4fj5NeBTAf1dfy2NOScKkIOAAD+Y7eQw72rAACALRFyAACAh8hIqyvwD0snHgMAAOuVH6aSpBYtrKnD3+jJAQAAHubNs7oC/yDkAAAQxu69t/Kys8+u/zoCgZADAEAYe/xxqysIHEIOAACwJUIOAACwJUIOAABh6NChymdVSdIPP9R/LYHCKeQAAIShlJTKy0L3Hgje0ZMDAECYKSqyuoL6QcgBACDMeLsHdrt29V9HoDFcBQBAGPE2JGW3YSoXenIAAAgjrVtbXUH9IeQAABBGtm/3fP7ZZ9bUUR8YrgIAIEzZdZjKhZADAEAY8HZNHLtjuAoAANgSIQcAAJvLzLS6AmsQcgAAsLmdOysv27u3/uuob4QcAADCUFqa1RUEHiEHAADYEiEHAAAbq3ifqpIS+5867kLIAQDAxirepyoijI78YbSpAACEt8hIqyuoX4QcAADCxL//bXUF9YuQAwBAmBg92uoK6hchBwAAm5o50/N5uN3agZADAIBNjRljdQXWIuQAABAGXnzR6grqHyEHAAAbys/3fP6nP1lTh5UIOQAA2FBiotUVWI+QAwAAbMnSkNO6dWs5HI5Kj/Hjx1tZFgAAIe2mm6yuIDhEWfnh33zzjUpKStzP169fryFDhujKK6+0sCoAAELbf/7j+fzIEWvqsJqlISc1NdXj+ZQpU9SuXTsNGDDAoooAALCX336T4uKsrsIaloac8k6cOKFXX31Vd911lxxVXK2osLBQhYWF7ud5eXn1VR4AACEpI8PqCqwTNBOP58+fr5ycHI2u5prT2dnZSkpKcj8yMzPrr0AAABBSHMYYY3URkjR06FBFR0frf//3f6ts460nJzMzU7m5uUrkXDkAAFRa6nm38eA4ynvKy8tTUlJSwI/fQTFctW3bNi1atEhz586ttp3T6ZTT6aynqgAACD0tWlhdQfAIiuGqGTNmKC0tTcOHD7e6FAAAQtru3VZXEDwsDzmlpaWaMWOGsrKyFBUVFB1LAADYQjAOVdUny0POokWLtH37dt1www1WlwIAAGzE8q6TCy+8UEEy9xkAANiI5T05AAAAgUDIAQDABvbvl6q4lm7YIuQAAGADaWlWVxB8LJ+TAwCA3ZTvUbFq2inTXenJAQDAlo4ds7oC6xFyAADwo4r3jq6PeTKlpZ7Pr7tOatgw8J8b7Ag5AAD4SU6OlJRUeXmgg075e1VJ0iuvBPbzQgUhBwAAP2nUqOrXfvut7F9/z5VJSPDv+uyEiccAANQDbzfO9EfgOXLE83lR0amv0y7oyQEAwEa4DeRJhBwAAPyg4ryblJT6r+HKK+v/M4MZIQcAgFPkbWLxgQM1D0cVFkp79/pvns6bb/pnPXZByAEAwCIxMVKzZlJEhLR5s3TihDRyZNkp4VzM79QRcgAA8LPyAcWYk4/qtG8vOZ3S7Nllp4SPGxfYGsMBIQcAgFoqLCwbonI9AmH69Jrb0NtTPeZgAwBQC6cSaoyp+/u9vTeCropq8e0BACBIORzSvn1lX0dEVN9zxL2qKiPkAADgJ77MvXFd+dhXTZv61i42tnbrDQeEHAAA6lFGRuUgVPGmnhXVx00+7YiQAwCAj4qLq37t6NHarWvIkJNf1+X+UwSfmjHxGAAAHzVo4H15Xc5y+uQT7+sgvPgPPTkAAASRijfc9MWePf6vww4IOQAABJG4uJonMFfs7fF1cnK4IeQAAOCDitek8fVKxqfCtf5jx6Trrgvc59gVIQcAAB9YeXXh2FjplVes+/xQRcgBAKCWSkutrgC+IOQAAFBLwXQGFPevqhohBwCAENG2rdUVhBZCDgAAVXDdK+rwYasrKbN5s9UVhBZCDgAAXpQfkmrc2Lo6UHeEHAAAYEuEHAAAYEuEHAAAKgims6cqmjTp5NecWVU9y0POb7/9pj/+8Y9KSUlRbGysunbtqlWrVlldFgAAXp04Ye3nP/po4K+0bBeW3oX88OHD6tOnj84//3x99NFHSk1N1caNG9WoUSMrywIAoEpV3YkcwcfSkPPYY48pMzNTM2bMcC9r06aNhRUBAMJddUNV9J6EFkuHq9577z11795dV155pdLS0nT22Wfr3//+d5XtCwsLlZeX5/EAAADwxtKQ8+uvv2ratGnq0KGDPv74Y40bN0633367Zs2a5bV9dna2kpKS3I/MzMx6rhgAEE6Mkfh7OnQ5jLGu8y06Olrdu3fXV1995V52++2365tvvtHy5csrtS8sLFRhYaH7eV5enjIzM5Wbm6vExMR6qRkAYG/lh6sYngqMvLw8JSUlBfz4bWlPTnp6ujp37uyx7PTTT9f27du9tnc6nUpMTPR4AAAAeGNpyOnTp482bNjgseyXX35Rq1atLKoIAADYhaUh584779SKFSv06KOPatOmTXr99df1r3/9S+PHj7eyLAAAYAOWhpzzzjtP8+bN0xtvvKEuXbroH//4h5555hmNGjXKyrIAAIANWDrx+FTV18QlAED4YOJx4IXFxGMAAIJFYWFw37MKtUfIAQCEvdJSKSbG6irgb4QcAEDYi4ysvIyhqtBHyAEAALZEyAEAALZEyAEAhDVvk40ZqrIHQg4AIGw4HGUPV4gh4NgbIQcAEBbKB5qIKo5+BBx7IeQAAGzPW48N18SxP0IOAMDWCDPhi5ADAIAYqrIjQg4AwLboxQlvhBwAgC0RcBBldQEAAFiFISp7I+QAAEJC+Z4Zwgl8wXAVAMAWXBf6q+kCf0ePlj0nKNkfPTkAgLBAqAk/9OQAAEJOxR6bir03TDqGRMgBAISAqkILYQbVIeQAAEIaQQdVIeQAAABbIuQAAIJacfGpvZ8Jx+GLkAMACGoNGng+5/Rv+IqQAwAIGTWFG8IPyiPkAACCTlUX9isfYvbu9f7agQOBrQ2hg5ADAAgZ5UNPWpr3Nikp0qFDUmkpPTvhjiseAwCCyrFjtX9Pbq7n80aN/FMLQhshBwAQVOLifG9LTw2qw3AVAACwJUIOACAkbNlidQUINYQcAEBIaN3a6goQagg5AADAlgg5AICgcfSo9+V79tRvHbAHS0POAw88IIfD4fHo1KmTlSUBACwUH+/53HULh6ZNrakHoc3yU8jPOOMMLVq0yP08KsrykgAAQYDTw3GqLE8UUVFRatasmdVlAAAAm7F8Ts7GjRuVkZGhtm3batSoUdq+fXuVbQsLC5WXl+fxAADYQ8X7VAGnytKQ07NnT82cOVMLFizQtGnTtGXLFvXr10/5+fle22dnZyspKcn9yMzMrOeKAQBAqHAYEzyjnjk5OWrVqpWmTp2qsWPHVnq9sLBQhYWF7ud5eXnKzMxUbm6uEhMT67NUAICfVXfHcdhLXl6ekpKSAn78tnxOTnnJyck67bTTtGnTJq+vO51OOZ3Oeq4KAACEIsvn5JR35MgRbd68Wenp6VaXAgCoR0VFns/pxYE/WBpy7r77bn3xxRfaunWrvvrqK11++eWKjIzUyJEjrSwLAFDPoqOtrgB25Jfhqry8PH322Wfq2LGjTj/9dJ/ft3PnTo0cOVIHDx5Uamqq+vbtqxUrVig1NdUfZQEAgDBWp5Bz1VVXqX///rrtttt0/Phxde/eXVu3bpUxRrNnz9YVV1zh03pmz55dl48HANgIp44jUOo0XLVkyRL169dPkjRv3jwZY5STk6N//vOfevjhh/1aIAAgvDAfB/5Sp5CTm5urxo0bS5IWLFigK664Qg0bNtTw4cO1ceNGvxYIALCnvDxOG0dg1SnkZGZmavny5Tp69KgWLFigCy+8UJJ0+PBhxcTE+LVAAEDocjhOPsoHGIdDSkqyri6EhzrNybnjjjs0atQoxcfHq1WrVho4cKCksmGsrl27+rM+AIBNRETQU4P6VaeQc+utt6pHjx7asWOHhgwZooiIsg6htm3bMicHAFClij065RGA4G9BdVuH2qqvy0IDAOrG25lTxnhfXlJS1tsD+wu62zrcddddPq906tSpdSoGABCeQvfPbQQzn0POmjVrPJ6vXr1axcXF6tixoyTpl19+UWRkpM4991z/VggACElVBRfOqEJ98TnkfP755+6vp06dqoSEBM2aNUuNGjWSVHZm1ZgxY9zXzwEAhDeGnmC1Os3Jad68uT755BOdccYZHsvXr1+vCy+8ULt27fJbgdVhTg4ABC9fr2RMT074qa/jd51ydl5envbv319p+f79+5Wfn3/KRQEAwgMBB4FUp5Bz+eWXa8yYMZo7d6527typnTt36p133tHYsWP1+9//3t81AgAA1FqdrpPz4osv6u6779a1116roqKishVFRWns2LF64okn/FogAMCe6MVBoNV6Tk5JSYm+/PJLde3aVdHR0dq8ebMkqV27doqLiwtIkVVhTg4AWK/83JuKt24ov3z7dqlVK+9tEV7q6/hdp4nHMTEx+umnn9SmTZtA1OQzQg4AWM9byKnqNPGqAhHCS1BPPO7SpYt+/fVXf9cCAAgx3s6gKi6uur0xJx9AoNUp5Dz88MO6++679f7772v37t3Ky8vzeAAA7KugoOxR1WsNGnguI9DAKnUaroood4UnR7kYb4yRw+FQSUmJf6qrAcNVAFD/fL3+jQshBxUF3b2ryit/9WMAQHgwhqsYI7TUKeQMGDDA33UAAIKQq9eGgINQVKeQ43Ls2DFt375dJ06c8Fh+5plnnlJRAABrVRySqu0QlQtDVbBSnULO/v37NWbMGH300UdeX6+vOTkAAP/zVzAh4MBqdep8vOOOO5STk6OVK1cqNjZWCxYs0KxZs9ShQwe99957/q4RAFBPcnIYloJ91Kkn57PPPtO7776r7t27KyIiQq1atdKQIUOUmJio7OxsDR8+3N91AgACrK5DUkCwqlNeP3r0qNLS0iRJjRo1ct+RvGvXrlq9erX/qgMAWCo3t/IyhqEQKuoUcjp27KgNGzZIkrp166bp06frt99+04svvqj09HS/FggAsE5CQlmo2bpVKio6GXCKi08+N0bavdvSMgGv6jRcNWHCBO3+v5/oyZMn66KLLtJrr72m6OhozZw505/1AQACwDU0tXOn1Ly59zble2zK31hTkiIjPZ83a+a/2gB/qdMVjys6duyYfv75Z7Vs2VJNmjTxR10+4YrHAFB7FefeHDwoNWpUecJxbY8O5dd7/LgUE1O3+mB/QX2Dzoo352zYsKHOOeeceg04AAD/SEk59YBTEQEHwaBOw1Xt27dXixYtNGDAAA0cOFADBgxQ+/bt/V0bAMCPAn3VYiYkI9jU6cd9x44dys7OVmxsrB5//HGddtppatGihUaNGqX//Oc//q4RAOAHvgYcrucKu/DLnJyNGzfqkUce0WuvvabS0lLuQg4AQcjX6+DQI4NAC+q7kB87dkzLli3T4sWLtXjxYq1Zs0adOnXSbbfdpoEDB/q5RAAAgNqr03BVcnKyrrvuOhUUFGjixInatWuX1qxZo6efflojRoyoUyFTpkyRw+HQHXfcUaf3AwAAlFennpyLL75Yy5Yt0+zZs7Vnzx7t2bNHAwcO1GmnnVanIr755htNnz6du5cDQAAUFVW+ro1rSKriEFZeXv3UBNSHOvXkzJ8/XwcOHNCCBQvUq1cvffLJJ+rXr5+aN2+uUaNG1WpdR44c0ahRo/Tvf/9bjRo1qks5AIAqOBxSdHTlkONy+LDn84SEwNcE1JdTOpmwa9eu6tOnj3r16qXzzjtP+/bt05w5c2q1jvHjx2v48OEaPHjwqZQCAKiD5OSTt2ZgwjHspk7DVVOnTtXixYu1bNky5efnq1u3burfv79uvvlm9evXz+f1zJ49W6tXr9Y333zjU/vCwkIVFha6n+fRrwoAtUaYQbioU8h54403NGDAAHeoSUpKqvU6duzYoQkTJmjhwoWK8fHSmNnZ2XrwwQdr/VkAACD8+OU6OXUxf/58XX755YosN1BcUlIih8OhiIgIFRYWerwmee/JyczM5Do5AFAFb9fGoScHVgvq6+RI0tKlSzV9+nRt3rxZb7/9tpo3b67/+Z//UZs2bdS3b98a3z9o0CB9//33HsvGjBmjTp066d57760UcCTJ6XTK6XTWtWQAsJU9e6TU1KonFRNwEO7qNPH4nXfe0dChQxUbG6s1a9a4e1dyc3P16KOP+rSOhIQEdenSxeMRFxenlJQUdenSpS5lAUDYcDik9HQpKqrs6+3by/6t7qrGBByEmzqFnIcfflgvvvii/v3vf6tBgwbu5X369NHq1av9VhwAwDetWlX/OvejQjiq03DVhg0b1L9//0rLk5KSlJOTU+diFi9eXOf3AkC4qOkeVA6HdPy457JA3n0cCFZ1+rFv1qyZNm3aVGn5smXL1LZt21MuCgDgna832YyNDWwdQCioU8i56aabNGHCBK1cuVIOh0O7du3Sa6+9pr/85S8aN26cv2sEAACotToNV02cOFGlpaUaNGiQjh07pv79+8vpdOqee+7RjTfe6O8aASDs7dsnNW1a9/cC4ahOPTkOh0N///vfdejQIa1fv14rVqzQ/v37lZSUpDZt2vi7RgAIe94Cjq+3YkhN9X89QCioVcgpLCzUpEmT1L17d/Xp00cffvihOnfurB9++EEdO3bUs88+qzvvvDNQtQIAaomzqhDOajVcdf/992v69OkaPHiwvvrqK1155ZUaM2aMVqxYoaeeekpXXnml14v4AQDqrqbemvKvuyYmFxRIXDsV4a5WIeett97SK6+8oksvvVTr16/XmWeeqeLiYq1bt04OX6f8AwBqxdvp31UFHy74B5xUq+GqnTt36txzz5UkdenSRU6nU3feeScBBwAChFszAHVXq5BTUlKi6Oho9/OoqCjFx8f7vSgACDelpSdvy1BdiCHgAL6r1XCVMUajR4923ySzoKBAt9xyi+Li4jzazZ07138VAkAYKD+dMSLCe5hhEjFQO7UKOVlZWR7P//jHP/q1GABAmeJiqdytASVxawagtmoVcmbMmBGoOgAg7FQ3nbFiwAFQe/xdAAAAbImQAwAhoLTU6gqA0FOne1cBAE5Nba68wRlVQN0QcgCgHnA5MaD+MVwFAAHmS8ChtwbwP0IOAATQ4cO+t83JqbysoMBvpQBhh+EqAAigxo1rbuPqxUlKokcH8Cd6cgDAT7ZvP3lrBtcDgHUIOQDgJ61a1f493KoBCBxCDgD4gS9zZw4ckPbuPfn8+HFu1QAEEnNyAMAPYmNrbpOSUvYv826A+sHfEABQC665NoWFZc/37OEUcSBY0ZMDAHUQE2N1BQBqQk8OAHixf3/leTa+9Ni4JhIzoRiwHj05AFBBQYGUlnbyua9DTQUFZROJXe0ZogKsRU8OAFTgbRKxL3cBdzr9XwuAuiPkAEANDh6UIiOtrgJAbRFyAIS9EydODi15m3fTpEnlZUePej5naAoIPoQcALa3d2/Vt1lwOMqGmSIiancbhoYN/VcfgMBg4jEAW6sYXByO6nttfOHtbuEAgg89OQBs6ddfqw8xdQ04xpTdLdz1tesBIPhYGnKmTZumM888U4mJiUpMTFSvXr300UcfWVkSAJto167q1+oacHy5PxWA4GFpyGnRooWmTJmib7/9VqtWrdIFF1ygESNG6IcffrCyLABhKjfX83n5ycWlpZwiDoQahzHB1dHauHFjPfHEExo7dmyNbfPy8pSUlKTc3FwlJibWQ3UAQkFte2qC67cgYH/1dfwOmonHJSUleuutt3T06FH16tXL6nIAAECIszzkfP/99+rVq5cKCgoUHx+vefPmqXPnzl7bFhYWqtB161+VJUEAKK/ilYlLS0/27NR1Lg6A0GT52VUdO3bU2rVrtXLlSo0bN05ZWVn68ccfvbbNzs5WUlKS+5GZmVnP1QIIRuWDTcUrE5cPNqWllUNQUVHg6gJgraCbkzN48GC1a9dO06dPr/Sat56czMxM5uQAYaq0tObbLVT1G66khFs1AFYJuzk5LqWlpR5Bpjyn0yknpzcA+D81hZSSkrq/F0DoszTkTJo0ScOGDVPLli2Vn5+v119/XYsXL9bHH39sZVkAgpwvc2uCq48agBUsDTn79u3T9ddfr927dyspKUlnnnmmPv74Yw0ZMsTKsgAEMSYPA/CVpSHnpZdesvLjAQCAjVl+dhUA+BtDVQCkIJx4DAB1Uf56OAAg0ZMDwAaOHSPgAKiMnhwAIaNikKH3BkB16MkBELIIOACqQ8gBEJK4HQOAmhByAISEir02UQy2A6gBIQcAANgSIQdAyOE6OAB8QcgBEPQqDlURcgD4gpADIORwVhUAXzB1D0DQKR9iDh60rg4AoY2eHABBLSXF8zlDVQB8RcgBEDLo1QFQG4QcAEGluvk2jRvXXx0AQh8hB0BIYJgKQG0RcgAEPQIOgLog5ACwVPkA4+16OAQcAHXFKeQA6p2v17kxhmviAKg7Qg6AelVS4ls7enAAnCqGqwDUm8JC7h4OoP4QcgDUm5gY39oVFwe2DgDhgZADIOj4OqQFANUh5ACos5wc6cABqbRU2rWr+knCtZlAHB19yqUBABOPAdRNYaHUqFHl5Q6Hb5OGy585xVlUAAKBkAOgTnyZX1NVcHGFoPJhiLOpAPgbIQeA3xUVSceOWV0FgHBHyAFQazWd/VTdnBp6bADUFyYeA6i1Bg3q9r7Dh/1bBwBUh5AD4JQYU3Z2lS89NMnJAS8HANwIOQB89uWX3icT13RmVGEhw1QA6h9zcgD4xJdTvA8dkho3Pvk8P1+Kjw9cTQBQHUIOAL9p1IgeGwDBw9LhquzsbJ133nlKSEhQWlqaLrvsMm3YsMHKkgB4wYX6AIQiS0POF198ofHjx2vFihVauHChioqKdOGFF+ro0aNWlgWgnKp6Zoyh1wZAcHMYEzy/pvbv36+0tDR98cUX6t+/f43t8/LylJSUpNzcXCUmJtZDhUD4qdiLEzy/MQCEqvo6fgfV2VW5ubmSpMblZy4CCBoEHAChJGgmHpeWluqOO+5Qnz591KVLF69tCgsLVVhY6H6el5dXX+UBYaekRIoIqj+DAKB2guZX2Pjx47V+/XrNnj27yjbZ2dlKSkpyPzIzM+uxQiC8REURcgCEtqCYk3Pbbbfp3Xff1ZIlS9SmTZsq23nrycnMzGRODhAA3s6osv63BQA7qK85OZYOVxlj9Oc//1nz5s3T4sWLqw04kuR0OuV0OuupOiA8lZaWDVUBQKizNOSMHz9er7/+ut59910lJCRoz549kqSkpCTFxsZaWRoQtiIjvS8v14kKACHB0uEqRxVXGJsxY4ZGjx5d4/s5hRzwL2O8z8NhmAqAP4XNcBWA4EHAAWAnnDsBQBITjQHYDyEHgFe//WZ1BQBwaoLmYoAArFOxF+fwYSk52ZJSAMBv6MkBwpy3YSoCDgA7IOQAAABbIuQA8MBkYwB2QcgBAAC2RMgJcseOlc2ZcDj4Cxv+V3E+Dj9jAOyEkBPk4uJOfl3+Qm35+SfDT3Wn+rraVHFxaQAAbIuQE8Ty8iovc4WV8lfBbtGi7N/S0upDza+/SkVF/q8ToYleHAB2x3VyglhSkvflVV2ZtqobK7q0a+fZHuGLsAsgHBBygowrwBw9Wrv3ebvnUG2GqPirPnyUlkrR0Z7L2N8A7IjhqiBSPmiUn4sT6M/yFoZCaaJzXh5zj2qjph4/ALALenKChBUH55qCTESEdUHHmKrDl+Q5/6ii0lLvPVvwLlTCLADUFocCi/lygDHGs93x42UH8tocnKpqW1O4qq8eEtdnHDhQ9m9EROXPLl9DRIRUUOB9XZGRtR+qczhODhEG6qBf/nIA1SktDdycGYYlAYQTQo4Fyh+8XQdzX7jCTkzMyfdUdZAy5mQQcrXJzT31ugMtNdX3z46NrX5dpaXVv16xtyg+/mQvUMUwUlIi7d9f/fqksiBz/Hjl/eJweA5BVhV2iovLQlp0dNnrx4/X/JkAAO8IOX5UUFB1z8e2bbXrEXGFE197evbuldav93xPxc9KTKx+fb58XiCCTqDCU01DVt5erzhfxbXPoqKktLSq921eXllAiouTGjb0PbxWbNOggefzhg0D15NWXOz/dQJAMGFOjh9561mo7cHJ9Zd8baWllT18YUzZxQTLX2un4uvV1V3+tfz8sh4Qq3mr2eE4OXfH38qvu3XrmtvW9HpN3/Py66nrEFPF9TMBGYDd0ZMTQHU5uNbXgSchwXPeR0lJ3ddT3RWX65O3g38gJyDXZqjRn2r7ma55TgAQbgg5Fis/b6S+J4FGRZ0coqoYBipep6e62lq0KDuIuub8nDjhew0VD74rVpSFr+qGziouLyz0rc78/KqH8gKl4uf8/LNv7fypqnlOAGB3DFdZyDXcEYxnuDRsWPu6kpPL3uN0nlxW27O6evb0fG6MdPBg2ddRUSevAl2b2nwJEIcPlwU71y0yXJ9R/r2+DCnVpGPHk7Xv2CG1bOm9natNSUnZdtdVVfUG488cAPgbIaeeGFN25k3FM2zsxtu8GOnUDqopKbVrX1xcu2Bw/HjZGWvJyZXn8BQXl73umnfkS9Cpqk3F08IzM6t+v0tVw5c1fV8PHKAHBwAYrqql8tdUcX1d1fVaKnL1jvh61pSd1HSFZcl/35PIyNqtKybm5NfeJudWnFhdfn8XFHju0+o+t649MtWt1+HwPp+qqoATjj97AMIXIacWKl5TxSU21p69MhWVnz9U14nKVQnEgdeXddblontO58mwUH5oruJn79pVu1pq0668qKiqL5pYnre72gOAnRFyqlH+mje1DTHGSJs21f7KxMHMNX/INVG5uovteZtgW1Wvw+HD/quxopq+96cy36Um6ek195z446rVvr43IaHu7weAUMScnABq187qCgLLNWwXG1v57KyOHcvmIDVs6Lm8YqiojwBY/jOCscfNn9+D8meauZTvUQKAcELIqcKpHAwrnn5tZ64Q4+1AXdNtF/bt8389NSk/Kbim2z4EI1fPUFW9i+XnF7naA0C4YrgqAKqapxGOqjvIWnX2T/mgEIpcddd0L63t2wNfCwAEM3py/Iy/nH3DfZNOXZMm1Ye1qk5RB4BwQU/OKSLU1Mzb94j7JvkPgREAvCPknIK6nH6MmodZUDveAiPhGwAYrqqz8gcRDig143sUWK7bXyQkSNHRVlcDAMGBkAPYRG1vfwEAdsdwFQAAsCVLQ86SJUt0ySWXKCMjQw6HQ/Pnz7eynBoZY68rGAMAYGeWhpyjR4+qW7duev75560swyeuYBOq11YBACDcWDonZ9iwYRo2bJiVJXhVMcjQcwMAQOgJqYnHhYWFKix3c548bqsMAACqEFITj7Ozs5WUlOR+ZHJJVwAAUIWQCjmTJk1Sbm6u+7Fjxw6rSwIAAEEqpIarnE6nnNz9EgAA+CCkenIAAAB8ZWlPzpEjR7Rp0yb38y1btmjt2rVq3LixWrZsaWFlAAAg1FkaclatWqXzzz/f/fyuu+6SJGVlZWnmzJkWVQUAAOzA0pAzcOBAGS5CAwAAAoA5OQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOQAAwJYIOTXgJukAAIQmQg4AALAlQg4AALClKKsLCEYMUQEAEProyQEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALZEyAEAALYUZXUBp8IYI0nKy8uzuBIAAOAr13HbdRwPlJAOOfn5+ZKkzMxMiysBAAC1lZ+fr6SkpICt32ECHaMCqLS0VLt27VJCQoIcDodf152Xl6fMzEzt2LFDiYmJfl13sAmXbQ2X7ZTYVrtiW+0pHLd1+/btcjgcysjIUERE4GbOhHRPTkREhFq0aBHQz0hMTLT9D51LuGxruGynxLbaFdtqT+G0rUlJSfWyrUw8BgAAtkTIAQAAtkTIqYLT6dTkyZPldDqtLiXgwmVbw2U7JbbVrthWe2JbAyekJx4DAABUhZ4cAABgS4QcAABgS4QcAABgS4QcAABgS4QcL55//nm1bt1aMTEx6tmzp77++murS6qV7OxsnXfeeUpISFBaWpouu+wybdiwwaPNwIED5XA4PB633HKLR5vt27dr+PDhatiwodLS0nTPPfeouLi4PjelRg888ECl7ejUqZP79YKCAo0fP14pKSmKj4/XFVdcob1793qsIxS2U5Jat25daVsdDofGjx8vKbT36ZIlS3TJJZcoIyNDDodD8+fP93jdGKP7779f6enpio2N1eDBg7Vx40aPNocOHdKoUaOUmJio5ORkjR07VkeOHPFo891336lfv36KiYlRZmamHn/88UBvWiXVbWtRUZHuvfdede3aVXFxccrIyND111+vXbt2eazD28/ClClTPNoE+7ZK0ujRoyttx0UXXeTRxg77VZLX/7sOh0NPPPGEu00o7Fdfji/++r27ePFinXPOOXI6nWrfvr1mzpxZ+4INPMyePdtER0ebl19+2fzwww/mpptuMsnJyWbv3r1Wl+azoUOHmhkzZpj169ebtWvXmosvvti0bNnSHDlyxN1mwIAB5qabbjK7d+92P3Jzc92vFxcXmy5dupjBgwebNWvWmA8//NA0adLETJo0yYpNqtLkyZPNGWec4bEd+/fvd79+yy23mMzMTPPpp5+aVatWmf/6r/8yvXv3dr8eKttpjDH79u3z2M6FCxcaSebzzz83xoT2Pv3www/N3//+dzN37lwjycybN8/j9SlTppikpCQzf/58s27dOnPppZeaNm3amOPHj7vbXHTRRaZbt25mxYoVZunSpaZ9+/Zm5MiR7tdzc3NN06ZNzahRo8z69evNG2+8YWJjY8306dPrazONMdVva05Ojhk8eLCZM2eO+fnnn83y5ctNjx49zLnnnuuxjlatWpmHHnrIY1+X//8dCttqjDFZWVnmoosu8tiOQ4cOebSxw341xnhs4+7du83LL79sHA6H2bx5s7tNKOxXX44v/vi9++uvv5qGDRuau+66y/z444/mueeeM5GRkWbBggW1qpeQU0GPHj3M+PHj3c9LSkpMRkaGyc7OtrCqU7Nv3z4jyXzxxRfuZQMGDDATJkyo8j0ffvihiYiIMHv27HEvmzZtmklMTDSFhYWBLLdWJk+ebLp16+b1tZycHNOgQQPz1ltvuZf99NNPRpJZvny5MSZ0ttObCRMmmHbt2pnS0lJjjH32acUDRGlpqWnWrJl54okn3MtycnKM0+k0b7zxhjHGmB9//NFIMt988427zUcffWQcDof57bffjDHGvPDCC6ZRo0Ye23rvvfeajh07BniLqubtYFjR119/bSSZbdu2uZe1atXKPP3001W+J1S2NSsry4wYMaLK99h5v44YMcJccMEFHstCcb9WPL746/fuX//6V3PGGWd4fNbVV19thg4dWqv6GK4q58SJE/r22281ePBg97KIiAgNHjxYy5cvt7CyU5ObmytJaty4scfy1157TU2aNFGXLl00adIkHTt2zP3a8uXL1bVrVzVt2tS9bOjQocrLy9MPP/xQP4X7aOPGjcrIyFDbtm01atQobd++XZL07bffqqioyGN/durUSS1btnTvz1DazvJOnDihV199VTfccIPHzWntsk/L27Jli/bs2eOxH5OSktSzZ0+P/ZicnKzu3bu72wwePFgRERFauXKlu03//v0VHR3tbjN06FBt2LBBhw8frqetqb3c3Fw5HA4lJyd7LJ8yZYpSUlJ09tln64knnvDo6g+lbV28eLHS0tLUsWNHjRs3TgcPHnS/Ztf9unfvXn3wwQcaO3ZspddCbb9WPL746/fu8uXLPdbhalPbY3FI36DT3w4cOKCSkhKPb7wkNW3aVD///LNFVZ2a0tJS3XHHHerTp4+6dOniXn7ttdeqVatWysjI0Hfffad7771XGzZs0Ny5cyVJe/bs8fp9cL0WLHr27KmZM2eqY8eO2r17tx588EH169dP69ev1549exQdHV3p4NC0aVP3NoTKdlY0f/585eTkaPTo0e5ldtmnFblq81Z7+f2Ylpbm8XpUVJQaN27s0aZNmzaV1uF6rVGjRgGp/1QUFBTo3nvv1ciRIz1uZnj77bfrnHPOUePGjfXVV19p0qRJ2r17t6ZOnSopdLb1oosu0u9//3u1adNGmzdv1t/+9jcNGzZMy5cvV2RkpG3366xZs5SQkKDf//73HstDbb96O7746/duVW3y8vJ0/PhxxcbG+lQjIcfmxo8fr/Xr12vZsmUey2+++Wb31127dlV6eroGDRqkzZs3q127dvVdZp0NGzbM/fWZZ56pnj17qlWrVnrzzTd9/k8Qil566SUNGzZMGRkZ7mV22acoU1RUpKuuukrGGE2bNs3jtbvuusv99Zlnnqno6Gj96U9/UnZ2dkjdGuCaa65xf921a1edeeaZateunRYvXqxBgwZZWFlgvfzyyxo1apRiYmI8lofafq3q+BJMGK4qp0mTJoqMjKw0C3zv3r1q1qyZRVXV3W233ab3339fn3/+uVq0aFFt2549e0qSNm3aJElq1qyZ1++D67VglZycrNNOO02bNm1Ss2bNdOLECeXk5Hi0Kb8/Q3E7t23bpkWLFunGG2+stp1d9qmrtur+XzZr1kz79u3zeL24uFiHDh0KyX3tCjjbtm3TwoULPXpxvOnZs6eKi4u1detWSaG1reW1bdtWTZo08fiZtdN+laSlS5dqw4YNNf7/lYJ7v1Z1fPHX792q2iQmJtbqD1hCTjnR0dE699xz9emnn7qXlZaW6tNPP1WvXr0srKx2jDG67bbbNG/ePH322WeVuje9Wbt2rSQpPT1dktSrVy99//33Hr9gXL9sO3fuHJC6/eHIkSPavHmz0tPTde6556pBgwYe+3PDhg3avn27e3+G4nbOmDFDaWlpGj58eLXt7LJP27Rpo2bNmnnsx7y8PK1cudJjP+bk5Ojbb791t/nss89UWlrqDnu9evXSkiVLVFRU5G6zcOFCdezYMaiGNFwBZ+PGjVq0aJFSUlJqfM/atWsVERHhHtoJlW2taOfOnTp48KDHz6xd9qvLSy+9pHPPPVfdunWrsW0w7teaji/++r3bq1cvj3W42tT6WFz7udT2Nnv2bON0Os3MmTPNjz/+aG6++WaTnJzsMQs82I0bN84kJSWZxYsXe5yKeOzYMWOMMZs2bTIPPfSQWbVqldmyZYt59913Tdu2bU3//v3d63Cd4nfhhReatWvXmgULFpjU1NSgON24vL/85S9m8eLFZsuWLebLL780gwcPNk2aNDH79u0zxpSdytiyZUvz2WefmVWrVplevXqZXr16ud8fKtvpUlJSYlq2bGnuvfdej+Whvk/z8/PNmjVrzJo1a4wkM3XqVLNmzRr3GUVTpkwxycnJ5t133zXfffedGTFihNdTyM8++2yzcuVKs2zZMtOhQwePU41zcnJM06ZNzXXXXWfWr19vZs+ebRo2bFjvpxpXt60nTpwwl156qWnRooVZu3atx/9f11knX331lXn66afN2rVrzebNm82rr75qUlNTzfXXXx9S25qfn2/uvvtus3z5crNlyxazaNEic84555gOHTqYgoIC9zrssF9dcnNzTcOGDc20adMqvT9U9mtNxxdj/PN713UK+T333GN++ukn8/zzz3MKub8899xzpmXLliY6Otr06NHDrFixwuqSakWS18eMGTOMMcZs377d9O/f3zRu3Ng4nU7Tvn17c88993hcU8UYY7Zu3WqGDRtmYmNjTZMmTcxf/vIXU1RUZMEWVe3qq6826enpJjo62jRv3txcffXVZtOmTe7Xjx8/bm699VbTqFEj07BhQ3P55Zeb3bt3e6wjFLbT5eOPPzaSzIYNGzyWh/o+/fzzz73+zGZlZRljyk4jv++++0zTpk2N0+k0gwYNqvQ9OHjwoBk5cqSJj483iYmJZsyYMSY/P9+jzbp160zfvn2N0+k0zZs3N1OmTKmvTXSrblu3bNlS5f9f1/WQvv32W9OzZ0+TlJRkYmJizOmnn24effRRj2AQCtt67Ngxc+GFF5rU1FTToEED06pVK3PTTTdV+oPSDvvVZfr06SY2Ntbk5ORUen+o7Neaji/G+O/37ueff27OOussEx0dbdq2bevxGb5y/F/RAAAAtsKcHAAAYEuEHAAAYEuEHAAAYEuEHAAAYEuEHAAAYEuEHAAAYEuEHAAAYEuEHAD1YuvWrXI4HO7bTQTC6NGjddlllwVs/QBCCyEHgE9Gjx4th8NR6XHRRRf59P7MzEzt3r1bXbp0CXClAFAmyuoCAISOiy66SDNmzPBY5nQ6fXpvZGRkUN4VGoB90ZMDwGdOp1PNmjXzeLjufuxwODRt2jQNGzZMsbGxatu2rd5++233eysOVx0+fFijRo1SamqqYmNj1aFDB48A9f333+uCCy5QbGysUlJSdPPNN+vIkSPu10tKSnTXXXcpOTlZKSkp+utf/6qKd6kpLS1Vdna22rRpo9jYWHXr1s2jpppqABDaCDkA/Oa+++7TFVdcoXXr1mnUqFG65ppr9NNPP1XZ9scff9RHH32kn376SdOmTVOTJk0kSUePHtXQoUPVqFEjffPNN3rrrbe0aNEi3Xbbbe73P/XUU5o5c6ZefvllLVu2TIcOHdK8efM8PiM7O1uvvPKKXnzxRf3www+688479cc//lFffPFFjTUAsIFa39ITQFjKysoykZGRJi4uzuPxyCOPGGPK7k58yy23eLynZ8+eZty4ccYY477D9po1a4wxxlxyySVmzJgxXj/rX//6l2nUqJE5cuSIe9kHH3xgIiIi3HepTk9PN48//rj79aKiItOiRQszYsQIY4wxBQUFpmHDhuarr77yWPfYsWPNyJEja6wBQOhjTg4An51//vmaNm2ax7LGjRu7v+7Vq5fHa7169arybKpx48bpiiuu0OrVq3XhhRfqsssuU+/evSVJP/30k7p166a4uDh3+z59+qi0tFQbNmxQTEyMdu/erZ49e7pfj4qKUvfu3d1DVps2bdKxY8c0ZMgQj889ceKEzj777BprABD6CDkAfBYXF6f27dv7ZV3Dhg3Ttm3b9OGHH2rhwoUaNGiQxo8fryeffNIv63fN3/nggw/UvHlzj9dck6UDXQMAazEnB4DfrFixotLz008/vcr2qampysrK0quvvqpnnnlG//rXvyRJp59+utatW6ejR4+623755ZeKiIhQx44dlZSUpPT0dK1cudL9enFxsb799lv3886dO8vpdGr79u1q3769xyMzM7PGGgCEPnpyAPissLBQe/bs8VgWFRXlnqz71ltvqXv37urbt69ee+01ff3113rppZe8ruv+++/XueeeqzPOOEOFhYV6//333YFo1KhRmjx5srKysvTAAw9o//79+vOf/6zrrrtOTZs2lSRNmDBBU6ZMUYcOHdSpUydNnTpVOTk57vUnJCTo7rvv1p133qnS0lL17dtXubm5+vLLL5WYmKisrKxqawAQ+gg5AHy2YMECpaeneyzr2LGjfv75Z0nSgw8+qNmzZ+vWW29Venq63njjDXXu3NnruqKjozVp0iRt3bpVsbGx6tevn2bPni1JatiwoT7++GNNmDBB5513nho2bKgrrrhCU6dOdb//L3/5i3bv3q2srCxFRETohhtu0OWXX67c3Fx3m3/84x9KTU1Vdna2fv31VyUnJ+ucc87R3/72txprABD6HMZUuLAEANSBw+HQvHnzuK0CgKDBnBwAAGBLhBwAAGBLzMkB4BeMfAMINvTkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAWyLkAAAAW/r/Deh9hrx+glYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "rewards, episodes = [], []\n",
        "best_eval_reward = 0\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "    step = 0\n",
        "    d = False\n",
        "    state = env.reset()\n",
        "    next_state = state\n",
        "    life = number_lives\n",
        "\n",
        "    get_init_state(history, state)\n",
        "\n",
        "    while not done:\n",
        "        step += 1\n",
        "        frame += 1\n",
        "\n",
        "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
        "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "            action = 0\n",
        "        else:\n",
        "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "            \n",
        "        state = next_state\n",
        "        next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "        frame_next_state = get_frame(next_state)\n",
        "        history[4, :, :] = frame_next_state\n",
        "        terminal_state = check_live(life, info['lives'])\n",
        "\n",
        "        life = info['lives']\n",
        "        r = np.clip(reward, -1, 1) \n",
        "        r = reward\n",
        "\n",
        "        # Store the transition in memory \n",
        "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "        # Start training after random sample generation\n",
        "        if(frame >= train_frame):\n",
        "            agent.train_policy_net(frame)\n",
        "            # Update the target network only for Double DQN only\n",
        "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
        "                agent.update_target_net()\n",
        "        score += reward\n",
        "        history[:4, :, :] = history[1:, :, :]\n",
        "            \n",
        "        if done:\n",
        "            evaluation_reward.append(score)\n",
        "            rewards.append(np.mean(evaluation_reward))\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, rewards, 'b')\n",
        "            pylab.xlabel('Episodes')\n",
        "            pylab.ylabel('Rewards') \n",
        "            pylab.title('Episodes vs Reward')\n",
        "            pylab.savefig(\"/content/save_graph/breakout_dqn.png\") # save graph for training visualization\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
        "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
        "\n",
        "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
        "            ### Change this save condition to whatever you prefer ###\n",
        "            if (e%100) == 0:\n",
        "              torch.save(agent.policy_net, \"/content/save_model/breakout_dqn.pth\")\n",
        "\n",
        "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
        "                torch.save(agent.policy_net, \"/content/save_model/breakout_dqn.pth\")\n",
        "                best_eval_reward = np.mean(evaluation_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-3SL3fEX_bb"
      },
      "source": [
        "# Visualize Agent Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H5oix6WX_bb"
      },
      "source": [
        "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
        "\n",
        "Please save your model before running this portion of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kKMf6ow7X_bb"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.policy_net, \"/content/save_model/breakout_dqn.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZEKXXZFBX_bc"
      },
      "outputs": [],
      "source": [
        "# from gym.wrappers import Monitor\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Displaying the game live\n",
        "def show_state(env, step=0, info=\"\"):\n",
        "    plt.figure(3)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
        "    plt.axis('off')\n",
        "\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "    ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "# Recording the game and replaying the game afterwards\n",
        "def show_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "def wrap_env(env):\n",
        "\tenv=RecordVideo(env, './video', episode_trigger = lambda episode_number: True)\n",
        "\treturn env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PPNgef7zX_bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "442d30c3-207c-4978-a125-f385c2ad5b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment BreakoutDeterministic-v4 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/utils/passive_env_checker.py:297: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f9162ca5be0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "display = Display(visible=0, size=(300, 200))\n",
        "display.start()\n",
        "\n",
        "# Load agent\n",
        "# agent = Agent(action_size)\n",
        "\n",
        "agent.load_policy_net(\"/content/save_model/breakout_dqn.pth\")\n",
        "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
        "\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "env = wrap_env(env)\n",
        "\n",
        "done = False\n",
        "score = 0\n",
        "step = 0\n",
        "state = env.reset()\n",
        "next_state = state\n",
        "life = number_lives\n",
        "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
        "get_init_state(history, state)\n",
        "\n",
        "while not done:\n",
        "    \n",
        "    # Render breakout\n",
        "    env.render()\n",
        "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
        "\n",
        "    step += 1\n",
        "    frame += 1\n",
        "\n",
        "    # Perform a fire action if ball is no longer on screen\n",
        "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
        "        action = 0\n",
        "    else:\n",
        "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
        "    state = next_state\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action + 1)\n",
        "        \n",
        "    frame_next_state = get_frame(next_state)\n",
        "    history[4, :, :] = frame_next_state\n",
        "    terminal_state = check_live(life, info['lives'])\n",
        "        \n",
        "    life = info['lives']\n",
        "    r = np.clip(reward, -1, 1) \n",
        "    r = reward\n",
        "\n",
        "    # Store the transition in memory \n",
        "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
        "    # Start training after random sample generation\n",
        "    score += reward\n",
        "    \n",
        "    history[:4, :, :] = history[1:, :, :]\n",
        "env.close()\n",
        "show_video()\n",
        "display.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_dPAhGhX_bc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}